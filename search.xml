<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes挂载cephfs带来的mds卡顿问题及引入cephfs storageClass]]></title>
    <url>%2F2020%2F01%2F06%2Fkubernetes%E6%8C%82%E8%BD%BDcephfs%E5%B8%A6%E6%9D%A5%E7%9A%84mds%E5%8D%A1%E9%A1%BF%E9%97%AE%E9%A2%98%E5%8F%8A%E5%BC%95%E5%85%A5cephfs%20storageClass.html</url>
    <content type="text"><![CDATA[kubernetes挂载cephfs带来的mds卡顿问题及引入cephfs storageClass前言前面的文章中，有写过如何在kubernets中组合pv/pvc，使用cephfs进行数据的持久化存储： cephfs 在kubernetes中的使用 但是在经过一段时间的使用后，发现此方式意外地会造成ceph mds严重的性能问题，本文将介绍如何使用cephfs storageClass避免这个问题。 回顾回顾此前的pv/pvc结合使用的模式，这里直接贴yaml文件： 123456789101112131415161718192021222324252627282930313233343536373839# cat cephfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: cephfs-pv labels: pv: cephfs-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteMany cephfs: monitors: - 192.168.20.112:6789 - 192.168.20.113:6789 - 192.168.20.114:6789 # 注意这个路径，这个路径必须提前创建好，否则pv会绑定失败 path: /app user: admin secretRef: name: ceph-secret readOnly: false persistentVolumeReclaimPolicy: Delete---# cat cephfs-pvc.yaml kind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi selector: matchLabels: pv: cephfs-pv 注意看上方的注释，pv中指定的挂载路径，若不指定，将存放在cephfs的”/“根路径，当多个服务都使用cephfs时，很可能出现目录重名的问题，因此统一放在同级目录显然不行；但若手动指定不同的目录，则此路径必须是存在的，否则pv绑定会出错，所以使用挂载之前要去手动创建目录，这的确带来了不少麻烦。 为了实现创建pod时，自动创建子级的挂载目录，在此前时怎么办的呢？答案是使用subPath: 123456789101112131415161718192021222324252627apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: cephfs-test name: cephfs-test namespace: defaultspec: replicas: 1 selector: matchLabels: app: cephfs-test template: metadata: spec: containers: - image: php-fpm imagePullPolicy: Always name: cephfs-test volumeMounts: - mountPath: /var/www/ name: home subPath: cephfs-test/home/ volumes: - name: home persistentVolumeClaim: claimName: cephfs-pvc 指定subPath后，容器在运行之前会自动在挂载的路径上创建声明中所指定的subPath路径，如此，就解决了上面所说的问题。 部署运行起来后，查看此时的挂载： 容器层面： 12345oot@cephfs-test-765bb76d75-gsm79:/var/www# mount | grep "/var/www"192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789:/app on /var/www type ceph (rw,relatime,name=admin,secret=&lt;hidden&gt;,acl)root@cephfs-test-765bb76d75-gsm79:/var/www#root@cephfs-test-765bb76d75-gsm79:/var/www# lscheck_health.php cephfs视角: 12345# cephfs里的路径和文件[root@yksp020112 home]# pwd/mnt/cephfs/app/cephfs-test/home[root@yksp020112 home]# lscheck_health.php 宿主机层面： 12[root@yksp020107 ~]# mount | grep cephfs192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789:/app on /var/lib/kubelet/pods/8126f599-2f6e-11ea-af20-3440b5a2bb9c/volumes/kubernetes.io~cephfs/cephfs-test type ceph (rw,relatime,name=admin,secret=&lt;hidden&gt;,acl) 可以看到，确实在cephfs中创建了子目录，容器中所挂载路径下的文件，也如预期的存放在子目录下，实现了隔离。看似很正常？但注意看宿主机层面的挂载详情，整个cephfs的/app目录全部都挂载到了宿主机上，这就是隐患所在，随之而来的，就是文首所说的cephfs mds严重的性能问题 问题/解决思路在ceph服务端，来看看cephfs mds的会话信息，找到测试例子中容器所在宿主机与cephfs mds建立的会话详情，通过ip过滤一下： 12345678910111213141516[root@yskp020113 ~]# ceph daemon mds.yksp020113 session ls | grep -10 192.168.20.107 &#123; "id": 3355940, "num_leases": 0, "num_caps": 5, "state": "open", "replay_requests": 0, "completed_requests": 0, "reconnecting": false, "inst": "client.3355940 192.168.20.107:0\/3585027522", "client_metadata": &#123; "entity_id": "admin", "hostname": "yksp020107", "kernel_version": "3.10.0-693.21.1.el7.x86_64" &#125; &#125;, 注意看这里的一个选项：num_caps，目前数量只有5，非常小，但若以此方式运行一段时间后，数值会飙升到恐怖的百万级别，之前忘记截图保存了，比较遗憾。此数值较大后，会引起cephfs mds的性能问题。 先简单描述一下num_caps这个是做什么的： ceph mds元数据服务器，为了高速响应客户端对文件系统的读写操作，会将客户端操作过的文件描述符(句柄)，缓存在内存中，每一个cap，对应一个客户端的文件句柄缓存，num_caps即代表总数。当所有会话的总num_caps很大时(据个人观察，超过200W时)，mds响应时间会比较久，客户端这时的文件读写操作会感受到明显的卡顿，当num_caps到达千万级别时，mds可能会出现崩溃现象，此时备mds监听到主mds异常会，会开始主备切换，但mds主备切换过程涉及也异常的缓慢，观察mds切换日志会发现，备mds会在handle_mds_map state change up:rejoin这一步停留很久的时间才能进入active状态，原因也是因为这些为了恢复这庞大的客户端caps数据，这期间客户端读写操作完全阻塞，进程iowait，cpu load飞速飙升，影响巨大。总而言之，num_caps数值庞大会给mds带来巨大的压力，会造成挂载使用的客户端普遍读写卡顿现象很严重 关于num_caps造成mds问题的详细描述和分析解决过程，可参考此文章: https://github.com/lidaohang/ceph_study/blob/master/Ceph%20MDS%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90.md 这种挂载方式为什么会造成num_caps飙升的现象呢？ 如上面所描述，宿主机中的mount的是整个/app的根目录，而不是容器实际使用的/app/cephfs-test/home子目录，整个/app目录下有非常多的文件，猜测是客户端使用ceph-lib库，在某些情况下会对挂载目录做一些扫描操作，服务端mds则铁憨憨地将这些操作句柄全部缓存下来，假设/app下共100W文件，此时再有多台宿主机出现这种现象，所有session的总num_caps很轻松就上到了千万级别。由此，问题出现了。 解决思路既然问题原因找到了，那就好办了，不挂载整个根目录，挂载相应的子目录就行了，但pv里面指定的目录都是要提前创建好的，难道每创建pv之前都要手动去cephfs目录下创建相应的子目录吗，这一点都不自动化。 不要着急，k8s团队的扩展存储项目孵化了有很多种对接不同类型的存储的provisioner插件，其中也包括cephfs。 ##cephfs provisioner 项目地址： https://github.com/yinwenqin/external-storage/tree/master/ceph/cephfs 如果你还不了解storageClass/pv/pvc的关系，请参考这篇文章: https://blog.upweto.top/gitbooks/useKubernetes/part5/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8Ceph-RBD%E4%BD%BF%E7%94%A8.html 代码结构使用cephfs provisioner之前，先来看看它的代码结构。如果你不熟悉go语言，那么下面的这两个部分你可以直接跳过，直接使用仓库里的部署模板即可。 代码结构很简单，代码总共只有两个代码文件： cephfs-provisioner.go，主要作用是使用client-go，与k8s apiserver的连接，监听相应资源的增删改查。 cephfs_provisioner.py，主要作用是连接storageClass中指定的cephfs服务器后端，创建实际的存储空间并提供给pv使用。 修改源代码在部署cephfs provisioner之前，根据使用场景以及个人的偏好，对cephfs provisioner源码进行了微小地修改，然后重新编译打包成自制的镜像。这么修改目的是为了使pv/pvc/挂载路径 三者的名称一一对应，使得清晰一致，方便后续维护管理。 如果你也想像我这样修改，但又觉得配置环境改代码麻烦，可以直接pull我打包好的镜像： 1docker pull ywq935/my-cephfs-provisioner:latest 修改前，指定storageClass为cephfs并创建pvc后，自动创建并绑定的pv名字是这样子的： 123// ceph/cephfs/cephfs-provisioner.go:135// "kubernetes-dynamic-pvc-%s", uuid.NewUUID()kubernetes-dynamic-pvc-xxxxxxxxxxxx-xxxxxxxxxxxxxxx cephfs端挂载的根路径是： 12ceph/cephfs/cephfs-provisioner.go:305pvcRoot = "/volumes/kubernetes" 这自动创建的一大串随机字符结尾的pv名称，以及预设的cephfs挂载路径看起来很不喜欢，那么就来改改它吧 仅仅需要修改如下几个小地方即可: 1234567891011121314// ceph/cephfs/cephfs-provisioner.go:303// 注释的是原来的代码// pvcRoot = "/volumes/kubernetes" pvcRoot = "/app" // deterministicNames = false deterministicNames = true// ceph/cephfs/cephfs-provisioner.go:127// 加一行代码 if deterministicNames &#123; share = options.PVC.Name user = fmt.Sprintf("k8s.%s.%s", options.PVC.Namespace, options.PVC.Name) // 自定义修改pv名称 options.PVName = options.PVC.Name 修改后，创建的pv/pvc名称一致，完全对应，易于管理，挂载的cephfs根路径为自定义的/app路径。 编译打包完成上面修改后，要编译并打包成新的镜像并推向自己的私有仓库，那么你需要修改一下makefile文件，将REGISTRY替换成自己的私有仓库地址: 12# REGISTRY = quay.io/external_storage/ REGISTRY = your.personal.registry 然后在目录内执行（前提是你的GOPATH等go语言环境需要配置好）: 1make push 代码便会编译并推送打包成docker镜像到你的私人仓库，打包吃来完整的镜像路径是: 1your.personal.registry/cephfs-provisioner:latest 部署rbac授权的部署文件在这里已经全部包含了，把这几个yaml文件下载下来，kubectl apply -f ./ 一口气创建好就行，如果你修改了镜像，记得将deployment内的image替换成自己的： https://github.com/yinwenqin/external-storage/tree/master/ceph/cephfs/deploy/rbac 使用cephfs storageClass上面部署好cephfs provisioner后，需要创建一个sc来指向它： 1234567891011kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: cephfsprovisioner: ceph.com/cephfsreclaimPolicy: Retainparameters: monitors: 192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789 adminId: admin adminSecretName: cephfs-secret adminSecretNamespace: "default" 创建一个测试用的部署来使用它们： 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: cephfs-test name: cephfs-test namespace: defaultspec: replicas: 1 selector: matchLabels: app: cephfs-test template: metadata: creationTimestamp: null labels: app: cephfs-test spec: containers: - image: php-fpm imagePullPolicy: Always name: cephfs-test volumeMounts: - mountPath: /var/www/ name: home subPath: home/ volumes: - name: home persistentVolumeClaim: claimName: cephfs-test---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: cephfs-testspec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi storageClassName: cephfs 创建完成后，来看看现在的挂载详情： 宿主机层面： 12[root@yksp020203 ~]# mount | grep cephfs-test192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789:/app/default/cephfs-test on /var/lib/kubelet/pods/c310cf7f-2f96-11ea-b3e0-e4434b415914/volumes/kubernetes.io~cephfs/cephfs-test type ceph (rw,relatime,name=k8s.default.cephfs-test,secret=&lt;hidden&gt;,acl,wsize=33554432) 容器层面： 1234root@cephfs-test-56455f75c4-5kcqd:/var/www# mount | grep "/var/www"192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789:/app/default/cephfs-test on /var/www type ceph (rw,relatime,name=k8s.default.cephfs-test,secret=&lt;hidden&gt;,acl,wsize=33554432)root@cephfs-test-56455f75c4-5kcqd:/var/www# lscheck_health.php cephfs视角: 1234[root@yksp020112 home]# lscheck_health.php[root@yksp020112 home]# pwd/mnt/cephfs/app/default/cephfs-test/home 这次可以看到，不再是将整个根路径挂载在宿主机上了。 查看session信息： 1234567891011121314151617[root@yskp020113 ~]# ceph daemon mds.yksp020113 session ls | grep -10 cephfs-test &#123; "id": 3365682, "num_leases": 0, "num_caps": 4, "state": "open", "replay_requests": 0, "completed_requests": 0, "reconnecting": false, "inst": "client.3365682 192.168.20.203:0\/1842590235", "client_metadata": &#123; "entity_id": "k8s.default.cephfs-test", "hostname": "yksp020203", "kernel_version": "4.14.108", "root": "\/app\/default\/cephfs-test" &#125; &#125;, 对比上面一开始的挂载方式，可以再次确认，client挂载的路径已经从整个根路径变为了细分子目录，并且带来了一个额外的好处是可以观察到每一个使用挂载的应用的详情，不再是向此前一样整个宿主机共用一个连接session。 结果经过一段时间的运行观察，上面出现的cephfs mds client session 中的num_caps值没有再出现过很大的数值，读写卡顿的现象得到了很好的解决。 总结其实最本质的需求，无非就是希望能够在pod绑定pv时，自动创建隔离的子目录。之前指定pv/pvc的方式，单纯地使用subPath的实现，就满足这个需求，此前是为了避免增加复杂度，尽量避免引入更多的插件，因此没有使用cephfs provisioner，但未曾想到误打误撞地因为cephfs mds的工作机制，引发了这个问题，而今引入cephfs provisioner，由它来自动创建、挂载子目录，就可以避免这个问题，这一顿折腾，也算是对cephfs了解也更为深入了不少。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 部署redis cluster]]></title>
    <url>%2F2020%2F01%2F03%2Fkubernetes%20%E9%83%A8%E7%BD%B2redis%20cluster.html</url>
    <content type="text"><![CDATA[kubernetes 部署redis cluster前言Redis cluster的集群关系的维系，并非是依赖于ip的，而是依赖于cluster内部的唯一id, ip只在首次建立集群关系时连接彼此使用，不作为成员连接凭据，取而代之的是id。画外音：只要持有id，容器重启ip怎么变化都不会影响到维系redis cluster的成员关系。 那么id怎么保存的呢？redis cluster在建立起来后，每个主备节点都会保存一份cluster节点的元数据文件，因此，为了保证在kubernetes内pod重启后，集群相关的角色配置等不丢失，此文件必须持久化，因此，适合使用statefulSet来部署。 部署直接贴部署的yaml文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116apiVersion: v1data: redis.conf: |2 appendonly no save 900 1 save 300 10 save 60 300 maxmemory 4GB maxmemory-policy allkeys-lru cluster-enabled yes cluster-config-file /var/lib/redis/nodes.conf # cluster的node元数据文件 #cluster-node-timeout 5000 # 主从心跳检查的时间间隔 cluster-node-timeout 500 # 这里临时配置成500ms，为了下方的主备切换的测试 dir /var/lib/redis port 6379kind: ConfigMapmetadata: name: app-rds-cluster namespace: default---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: app-rds-clusterspec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi storageClassName: cephfs---# 用来提供redis cluster对外连接服务的apiVersion: v1kind: Servicemetadata: name: app-rds-cluster labels: app: redisspec: ports: - name: redis-port port: 6379 selector: app: app-rds-cluster appCluster: redis-cluster---apiVersion: apps/v1beta1kind: StatefulSetmetadata: name: app-rds-clusterspec: serviceName: "app-rds-cluster" replicas: 6 template: metadata: labels: app: app-rds-cluster appCluster: redis-cluster spec: terminationGracePeriodSeconds: 5 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - app-rds-cluster topologyKey: kubernetes.io/hostname containers: - name: redis env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name image: redis:v4.0.14 command: - "redis-server" #redis启动命令 args: - "/etc/redis/redis.conf" # 配置文件 # command: redis-server /etc/redis/redis.conf resources: #资源 requests: #请求的资源 cpu: "100m" #m代表千分之,相当于0.1 个cpu资源 memory: "100Mi" #内存100m大小 limits: cpu: "1" # 1代表1核 memory: "4096Mi" #内存100m大小 ports: - name: redis containerPort: 6379 protocol: "TCP" - name: cluster containerPort: 16379 protocol: "TCP" volumeMounts: - name: "redis-conf" # 挂载configmap生成的文件 mountPath: "/etc/redis" - name: pvc mountPath: "/var/lib/redis" subPathExpr: $(POD_NAME)/data/ nodeSelector: RDSDB: '' volumes: - name: "redis-conf" configMap: name: app-rds-cluster - name: pvc persistentVolumeClaim: claimName: app-rds-cluster 部署说明： redis配置文件使用configmap挂载 共配置6个节点，3主3从，这也是官方文档中说的推荐最小的集群规模了。 redis cluster node元数据文件很重要，下面会有详细说明。这个文件必须要做持久化，可使用pvc/hostPath等方式。 使用redis-trib工具建立cluster，建立好cluster后，后续cluster关系维护不再需要它。 网上有不少其他文章中都部署了2个service，包含一个headless service，建cluster时用作服务发现；和一个正常的service，给客户端连接使用。但这个headless service其实根本没有必要，建cluster的时候手动取一次pod ip就好，何必多建一个无用的service 假设你的持久化没有问题，那么，你现在应该有6个运行正常的pod了： 123456789101112[root@008019 redis-cluster]# kubectl get pods -o wide --all-namespaces | grep app-rds-clusterdefault app-rds-cluster-0 1/1 Running 0 5m1s 172.36.4.43 008031 &lt;none&gt; &lt;none&gt;default app-rds-cluster-1 1/1 Running 0 4m58s 172.36.1.30 008020 &lt;none&gt; &lt;none&gt;default app-rds-cluster-2 1/1 Running 0 4m56s 172.36.6.21 020203 &lt;none&gt; &lt;none&gt;default app-rds-cluster-3 1/1 Running 0 4m53s 172.36.5.25 008032 &lt;none&gt; &lt;none&gt;default app-rds-cluster-4 1/1 Running 0 4m51s 172.36.0.198 008019 &lt;none&gt; &lt;none&gt;default app-rds-cluster-5 1/1 Running 0 4m48s 172.36.3.134 020204 &lt;none&gt; &lt;none&gt;# 获取到这些pod 的ip，下面会用到[root@008019 redis-cluster]# echo `kubectl get pods -o wide --all-namespaces | grep app-rds-cluster | awk '&#123;print $7":6379"&#125;'`172.36.4.43:6379 172.36.1.30:6379 172.36.6.21:6379 172.36.5.25:6379 172.36.0.198:6379 172.36.3.134:6379 建立集群因此建立集群的操作只需要执行一次，后续集群关系redis节点之间会基于节点元数据文件自动维护，所以，专门部署一个环境，用来初始化配置集群，部署文件如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: redis-cluster-manager name: redis-cluster-manager namespace: defaultspec: replicas: 1 selector: matchLabels: app: redis-cluster-manager template: metadata: labels: app: redis-cluster-manager spec: containers: - command: - tail args: - -f - /dev/null env: - name: DB_NAME value: redis-cluster-manager image: centos:centos7 imagePullPolicy: Always name: redis-cluster-manager resources: limits: cpu: 500m memory: 300Mi requests: cpu: 100m memory: 100Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 1234[root@008019 redis-cluster]# kubectl get pods -o wide --all-namespaces | grep redis-cluster-managerdefault redis-cluster-manager-5468b99f7f-lxpw7 1/1 Running 0 87m 172.36.4.42 008031 &lt;none&gt; &lt;none&gt;[root@008019 redis-cluster]# kubectl exec -it redis-cluster-manager-5468b99f7f-lxpw7 bash[root@redis-cluster-manager-5468b99f7f-lxpw7 /]# 安装集群配置工具: 12345678910111213cat &gt;&gt; /etc/yum.repos.d/epel.repo&lt;&lt;'EOF'[epel]name=Extra Packages for Enterprise Linux 7 - $basearchbaseurl=https://mirrors.tuna.tsinghua.edu.cn/epel/7/$basearch#mirrorlist=https://mirrors.fedoraproject.org/metalink?repo=epel-7&amp;arch=$basearchfailovermethod=priorityenabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7EOFyum -y install redis-trib.noarch 初始化集群: 12345678910# 拿到上面获取到的pod ip,执行命令，交互步骤输入yes,不出意外，集群初始化成功# 解释一下，这里的6个节点，--replicas 1，前3个会是master，后3个会是前3个的slaveredis-trib create --replicas 1 \172.36.4.43:6379 172.36.1.30:6379 172.36.6.21:6379 172.36.5.25:6379 172.36.0.198:6379 172.36.3.134:6379...[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 连接选择其中一个 123456789101112131415161718192021222324252627282930313233343536[root@008019 redis-cluster]# kubectl exec -it redis-cluster-manager-5468b99f7f-lxpw7 bash[root@redis-cluster-manager-5468b99f7f-lxpw7 /]#[root@redis-cluster-manager-5468b99f7f-lxpw7 /]# exitexit[root@008019 redis-cluster]# kubectl exec -it app-rds-cluster-0 bash# redis-cli连接cluster记得加-c参数root@app-rds-cluster-0:/var/lib/redis# redis-cli -c# 设一个key127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.36.6.21:6379(nil)172.36.6.21:6379&gt; set a 1OK172.36.6.21:6379&gt; get a"1"# 查看角色172.36.6.21:6379&gt; role1) "master"2) (integer) 19543) 1) 1) "172.36.3.134" 2) "6379" 3) "1954"172.36.6.21:6379&gt; quit# 查看集群节点元数据文件，第一列是id号，后面则是ip与角色信息， myself指的是自身root@app-rds-cluster-0:/var/lib/redis# cat nodes.confbdcda6ce963add5bc9706e912b6fcf4b355e2add 172.36.3.134:6379@16379 slave d24da6b03aa3b614a1429847b3a47836f89dbc07 0 1578042761171 6 connectedf85ce96851823832a9bda4233cc6e3066e97c050 172.36.5.25:6379@16379 slave 561f70b2c94c6e46f7b9588705f2cb48861b1e89 0 1578042760571 4 connected561f70b2c94c6e46f7b9588705f2cb48861b1e89 172.36.4.43:6379@16379 myself,master - 0 1578042760000 1 connected 0-5460e5c64aa60dcc73746e196b6c1630019ec0c10ad5 172.36.0.198:6379@16379 slave 7e2a3fa95ef9402bfba7b2e08a27812272640179 0 1578042760000 5 connected7e2a3fa95ef9402bfba7b2e08a27812272640179 172.36.1.30:6379@16379 master - 0 1578042760000 2 connected 5461-10922d24da6b03aa3b614a1429847b3a47836f89dbc07 172.36.6.21:6379@16379 master - 0 1578042759569 3 connected 10923-16383vars currentEpoch 6 lastVoteEpoch 0root@app-rds-cluster-0:/var/lib/redis#root@app-rds-cluster-0:/var/lib/redis# 使用service 固定ip连接： 12345678[root@008019 ~]# kubectl get service --all-namespaces | grep app-rds-clusterdefault app-rds-cluster ClusterIP 10.123.80.163 &lt;none&gt; 6379/TCP 48m[root@008019 ~]# kubectl exec -it redis-cluster-manager-5468b99f7f-lxpw7 bash[root@redis-cluster-manager-5468b99f7f-lxpw7 /]# redis-cli -h 10.123.80.163 -c10.123.80.163:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.36.6.22:6379"1" 主备切换作为一个高可用集群，主备切换是基本能力，看一看切换过程发生了什么: 1234567891011121314151617181920212223242526# 重启一个master[root@008019 ~]# kubectl delete pod app-rds-cluster-0pod "app-rds-cluster-0" deleted[root@008019 ~]#[root@008019 ~]# kubectl get pods -o wide --all-namespaces | grep app-rds-cldefault app-rds-cluster-0 1/1 Running 0 20s 172.36.4.44 008031 &lt;none&gt; &lt;none&gt;default app-rds-cluster-1 1/1 Running 0 31m 172.36.1.30 008020 &lt;none&gt; &lt;none&gt;default app-rds-cluster-2 1/1 Running 0 31m 172.36.6.21 020203 &lt;none&gt; &lt;none&gt;default app-rds-cluster-3 1/1 Running 0 31m 172.36.5.25 008032 &lt;none&gt; &lt;none&gt;default app-rds-cluster-4 1/1 Running 0 31m 172.36.0.198 008019 &lt;none&gt; &lt;none&gt;default app-rds-cluster-5 1/1 Running 0 31m 172.36.3.134 020204 &lt;none&gt; &lt;none&gt;# 对比上面可以发现，redis-0的ip从172.36.4.43变成了172.36.4.44，那么连接看看数据和角色# 可以看到，此时master已经变成了slave, 但nodes.conf内的ip没有随着容器ip变化而修改，只是修改了从属关系的记录。当然，数据也没有丢失。root@app-rds-cluster-0:/var/lib/redis# cat nodes.conf7e2a3fa95ef9402bfba7b2e08a27812272640179 172.36.1.31:6379@16379 master - 1578044978612 1578044978607 2 connected 5461-10922e5c64aa60dcc73746e196b6c1630019ec0c10ad5 172.36.0.199:6379@16379 slave 7e2a3fa95ef9402bfba7b2e08a27812272640179 0 1578044978614 5 connected561f70b2c94c6e46f7b9588705f2cb48861b1e89 172.36.4.43:6379@16379 myself,slave f85ce96851823832a9bda4233cc6e3066e97c050 0 1578044978608 9 connectedbdcda6ce963add5bc9706e912b6fcf4b355e2add 172.36.3.135:6379@16379 slave d24da6b03aa3b614a1429847b3a47836f89dbc07 1578044978612 1578044978608 10 connectedf85ce96851823832a9bda4233cc6e3066e97c050 172.36.5.26:6379@16379 master - 0 1578044978614 11 connected 0-5460d24da6b03aa3b614a1429847b3a47836f89dbc07 172.36.6.22:6379@16379 master - 1578044978612 1578044978608 10 connected 10923-16383vars currentEpoch 11 lastVoteEpoch 10root@app-rds-cluster-0:/var/lib/redis# redis-cli -c127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.36.6.22:6379"1" 有兴趣可以同时删除多个节点试试，6节点，3个为主，按照raft的算法，外加主挂了之后还有备接替上，因此，除非2主+2备或更多节点同时挂掉，不然不会影响redis的服务。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Etcd和kubernetes master的灾备与恢复]]></title>
    <url>%2F2020%2F01%2F01%2FEtcd%20%E5%92%8C%20kubernetes%20master%E7%9A%84%E7%81%BE%E5%A4%87%E4%B8%8E%E6%81%A2%E5%A4%8D.html</url>
    <content type="text"><![CDATA[Etcd和kubernetes master的灾备与恢复背景说明问题：假设某台带有etcd的k8s master节点完全故障，彻底无法恢复 方案：新启动一台主机，配置为故障master主机相同的ip和主机名，并尝试原地恢复，顶替原故障master节点 Etcd恢复参考官方文档：https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md#restoring-a-cluster 注意：从官方文档可得知，备份快照时会改变cluster id(改变cluster id不影响客户端，客户端是使用ip来连接访问的)，因此，需要etcd cluster的每个成员都参与备份与恢复。 检查1234567891011121314151617~# export ENDPOINTS="https://10.90.1.238:2379,https://10.90.1.239:2379,https://10.90.1.240:2379"~# etcdctl --endpoints=$ENDPOINTS --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --cacert=/etc/etcd/ssl/ca.pem member list316dbd21b17d1b4f, started, NODE238, https://10.90.1.238:2380, https://10.90.1.238:2379, false8a206cf1ed53b6f4, started, NODE240, https://10.90.1.240:2380, https://10.90.1.240:2379, falsee444265665d3bd32, started, NODE239, https://10.90.1.239:2380, https://10.90.1.239:2379, false~# etcdctl endpoint health --endpoints=$ENDPOINTS --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pemhttps://10.90.1.239:2379 is healthy: successfully committed proposal: took = 16.882627mshttps://10.90.1.240:2379 is healthy: successfully committed proposal: took = 17.046735mshttps://10.90.1.238:2379 is healthy: successfully committed proposal: took = 19.395533ms~ # kubectl get nodesNAME STATUS ROLES AGE VERSIONubuntu238 Ready master 123d v1.15.3ubuntu239 Ready master 123d v1.15.3ubuntu240 Ready master 123d v1.15.3 确认正常无误 开始备份关停客户端123# 3台分别执行,二进制安装的关停相应组件服务service kubelet stopservice docker stop 备份快照在每一台etcd节点上分别执行： 123456789101112# 修改为对应节点的ipexport EP="https://10.90.1.238:2379"export ETCDCTL_API=3mkdir -p /var/lib/etcd_baketcdctl --endpoints=$EP --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --cacert=/etc/etcd/ssl/ca.pem snapshot save /var/lib/etcd_bak/`hostname`-etcd_`date +%Y%m%d%H%M`.dbsystemctl stop etcdmv /var/lib/etcd /var/lib/etcd_old 拷贝文件： 123scp -r /etc/etcd/ssl/* NEW_HOSTscp -r /etc/systemd/system/etcd.service NEW_HOSTscp /var/lib/etcd_bak/ubuntu238-etcd_202001011000.db NEW_HOST 将备份出的快照文件、ssl认证文件、systemd启动脚本，拷贝到新主机上，同时，在新主机上安装好docker、kubelet、kubectl、etcd服务，安装步骤这里省略，由于这几个组件都是go语言的二进制程序，因此可以直接将原主机的执行文件scp到新主机上相同路径中，不必花费精力去找相应版本的rpm包。 恢复备份在新加入的一台替换主机，和保持不变的另外2台主机上执行： 必须每台成员主机全部执行，因为etcd协商的cluster id等一些元数据发生了改变 1234567891011121314151617181920# 这3个环境变量相应修改export NODE_NAME=NODE238export NODE_IP=10.90.1.238export SNAPSHOT=ubuntu238-etcd_202001011000.dbexport ETCDCTL_API=3export ETCD_NODES="NODE238"=https://10.90.1.238:2380,"NODE239"=https://10.90.1.239:2380,"NODE240"=https://10.90.1.240:2380export ENDPOINTS="https://10.90.1.238:2379,https://10.90.1.239:2379,https://10.90.1.240:2379"cd /var/lib/etcd_bak/etcdctl snapshot restore $SNAPSHOT \--name=$NODE_NAME \--cert=/etc/etcd/ssl/etcd.pem \--key=/etc/etcd/ssl/etcd-key.pem \--cacert=/etc/etcd/ssl/etcd-ca.pem \--initial-advertise-peer-urls=https://$NODE_IP:2380 \--initial-cluster-token=etcd-cluster-0 \--initial-cluster=$ETCD_NODES \--data-dir=/var/lib/etcd/ 完成以上步骤后，3台etcd主机上一起启动etcd并检查状态： 12345~ # service etcd start~ # etcdctl endpoint health --endpoints=$ENDPOINTS --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pemhttps://10.90.1.240:2379 is healthy: successfully committed proposal: took = 16.112222mshttps://10.90.1.238:2379 is healthy: successfully committed proposal: took = 16.126138mshttps://10.90.1.239:2379 is healthy: successfully committed proposal: took = 17.345037ms etcd恢复成功！ K8s master恢复如果是原主机不动，则无需操作，如果是替换为新主机，则需要将如下文件拷贝至新主机上(适用于kubeadm安装的k8s,二进制需要对应修改)： /etc/kubernetes/目录下的所有文件(证书，manifest文件) 用户(root)主目录下 /root/.kube/config文件(kubectl连接认证) /var/lib/kubelet/目录下所有文件(plugins、容器连接认证) /etc/systemd/system/kubelet.service.d/10-kubeadm.conf(kubelet systemd 启动文件) 复制完成后，启动服务: 12service docker startservice kubelet start 检查： 12345kubectl get nodesNAME STATUS ROLES AGE VERSIONubuntu238 Ready master 123d v1.15.3ubuntu239 Ready master 123d v1.15.3ubuntu240 Ready master 123d v1.15.3 不出意外的话，一分钟左右master就恢复了。 总结master恢复还是很简单的，没什么好多说的，etcd的恢复比较麻烦一些，需要全部成员重新initial一次，一定要按步骤来，仔细一些不要错。 提示：操作需要中断客户端写入，即停止k8s服务，风险高，生产慎用！]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Controller-P3-Controller启动流程.md]]></title>
    <url>%2F2019%2F12%2F22%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Controller-P3-Controller%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B.html</url>
    <content type="text"><![CDATA[P3-Controller启动流程 APIGroup : api所属组别的信息，其中包含的groupVersion字段，即我们每个yaml文件中apiVersion所指定的 APIResource：所有使用到的resouce资源类型，包含pod/deployment/svc等等 APIGroup结构体： 1234567type APIGroup struct &#123; TypeMeta `json:",inline"` Name string `json:"name" protobuf:"bytes,1,opt,name=name"` Versions []GroupVersionForDiscovery `json:"versions" protobuf:"bytes,2,rep,name=versions"` PreferredVersion GroupVersionForDiscovery `json:"preferredVersion,omitempty" protobuf:"bytes,3,opt,name=preferredVersion"` ServerAddressByClientCIDRs []ServerAddressByClientCIDR `json:"serverAddressByClientCIDRs,omitempty" protobuf:"bytes,4,rep,name=serverAddressByClientCIDRs"`&#125; APIGroup实例： 通过如下方式快速访问的apiserver的rest api： 1234567891011121314151617181920212223242526272829303132333435363738394041# 节点上运行kubectl proxy代理,免去认证步骤，这不太安全，只建议测试使用，使用完毕后及时关闭 ~# kubectl proxy --port=8001 &amp; # 查看所有的APIGroup，groups 数组内的每一个成员都是一个APIGroup实例~#curl 127.0.0.1:8001/apis/&#123; "kind": "APIGroupList", "apiVersion": "v1", "groups": [ &#123; "name": "apiregistration.k8s.io", "versions": [ &#123; "groupVersion": "apiregistration.k8s.io/v1", "version": "v1" &#125;, &#123; "groupVersion": "apiregistration.k8s.io/v1beta1", "version": "v1beta1" &#125; ], "preferredVersion": &#123; "groupVersion": "apiregistration.k8s.io/v1", "version": "v1" &#125; &#125;, &#123; "name": "extensions", "versions": [ &#123; "groupVersion": "extensions/v1beta1", "version": "v1beta1" &#125; ], "preferredVersion": &#123; "groupVersion": "extensions/v1beta1", "version": "v1beta1" &#125; &#125;, ...&#125; APIResource结构体 12345type APIResourceList struct &#123; TypeMeta `json:",inline"` GroupVersion string `json:"groupVersion" protobuf:"bytes,1,opt,name=groupVersion"` APIResources []APIResource `json:"resources" protobuf:"bytes,2,rep,name=resources"`&#125; APIResource实例 Resource分为多个groupVersion 12~# curl 127.0.0.1:8001/api/v1 #默认资源~# curl 127.0.0.1:8001/apis/apps/v1/ # 扩展资源 具体参考官方api说明： https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/ 下面是一个APIResourceList实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768~# curl 127.0.0.1:8001/api/v1&#123; "kind": "APIResourceList", "groupVersion": "v1", "resources": [ &#123; "name": "pods", "singularName": "", "namespaced": true, "kind": "Pod", "verbs": [ "create", "delete", "deletecollection", "get", "list", "patch", "update", "watch" ], "shortNames": [ "po" ], "categories": [ "all" ] &#125;, &#123; "name": "configmaps", "singularName": "", "namespaced": true, "kind": "ConfigMap", "verbs": [ "create", "delete", "deletecollection", "get", "list", "patch", "update", "watch" ], "shortNames": [ "cm" ] &#125;, &#123; "name": "endpoints", "singularName": "", "namespaced": true, "kind": "Endpoints", "verbs": [ "create", "delete", "deletecollection", "get", "list", "patch", "update", "watch" ], "shortNames": [ "ep" ] &#125;, ... ]&#125;]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Controller-P2-Controller与client-go.md]]></title>
    <url>%2F2019%2F12%2F18%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Controller-P2-Controller%E4%B8%8Eclient-go.html</url>
    <content type="text"><![CDATA[P2-Controller与client-go前言Controller作为k8s的资源控制组件，必定要实时地监控对比资源的目标状态、当前状态，这其中会与apiserver产生大量的交互。在k8s中，k8s各个组件都会与apiServer交互，因此k8s代码中提取了一个client-go包，路径位于项目vendor/k8s.io/client-go，非常多的组件向apiServer的curd操作都在client-go包中封装，client-go是k8s项目的核心包之一。那么它跟controller有何关联呢？往下看 ApiServer的连接方式1.短连接 获取kubernetes某种资源的方式有多种，常见的如kubectl、调用apiserver restful api的接口，restful api接口详情查看官方手册。同时也可通过kubectl命令来查看操作对应的api，例如： 1kubectl get pod POD_NAME -v=9 输出信息中会包含此操作对应的api url 但这种全量型的操作方式，在大集群规模下，开销还是比较大的，因此，k8s还提供长连接的watch接口。 2.长连接 watch接口是对list接口的一种改进，在调用watch接口后，首先会一次性返回list的数据，同时会保持会话连接，后续的接口对象的curd，都会产生事件由apiserver将变更数据推送给调用端，调用端接收数据后，再更新初始接收到的list的全量数据以及其他操作。 3.client-go watch依然比较麻烦，毕竟list获取的数据以及后续watch到的数据，需要调用端在内部处理和更新缓存。索性，官方提供了一个client-go客户端工具包封装，里面提供多种apiserver相关操作，做到开箱即用，k8s各组件也都使用了它。 client-go工作模式在client-go中，informer是对watch操作的一次再封装， Informer是一个带有本地缓存、索引功能的client端(list/watch)，在绝大多数场景下，客户端与apiserver的交互都是读多写少的，因此，做好本地缓存(Store)和索引(Index)可以大幅减少开销提升性能。同时，informer可以注册相应的EventHandler事件触发器的，在执行资源更新后触发其他连锁操作。 工作流程图 流程图组件解释图中有上下分层，上层逻辑由client-go内部封装，下层的逻辑由controller内部完成，对照上图分层说明： 上层： Reflector：反射器，调用 Kubernetes 的 List/Watch API，实现对 apiserver 指定类型对象的监控(List/Watch)，将获取的数据，反序列化成对象实例，存入delta 缓存队列中； DeltaIFIFO Queue：一个增量fifo缓存队列，接收来自 Reflector 传递的反序列化对象； Informer：是这个流程中的关键重要的桥梁，主要有两个工作：1.从DeltaIFIFO Queue中获取对象，更新LocalStore的cache数据 2.触发后续的eventHandler，生成工作队列对象，加入WorkQueue，供后面的controller读取进行实际的控制处理工作。（值得注意的是，每一种资源都对应一个informer，多种资源对应多个informer，但每个informer都会与apiserver建立起一个watch长连接，通常controller都会使用SharedInformerFactory这个单例工厂模式，来使所有的informer的创建全部都经过这个单例工厂，从而保证每种资源对应的informer都是唯一且可复用的，以降低开销。） LocalStore：informer 的 cache缓存，这里缓存的是从 apiserver 中获取得到的对象(其中有一部分可能还在DeltaFIFO 中没来得及放入缓存里来)，此时client再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 读操作的方法访问； 下层： ResourceEventHandler：由controller注册，由informer来触发，当resource object符合过滤规则时，触发ResourceEventHandler将其丢入WorkQueue内。 WorkQueue：informer更新本地的缓存后，会根据注册的相关EventHandler，生成事件放入WorkQueue内，Controller 接收 WorkQueue 中的事件，然后相应执行controller的业务逻辑，一般来说，就是保证资源对象的目标状态与实际状态达成一致的逻辑。 如果你想自定义controller，关于infomer的实践建议，参考这里： 如何用 client-go 拓展 Kubernetes 的 API 总结本篇先简单介绍一下client-go包和controller组件的结合工作流程，为后面的几篇(deploymentCrontroller、statefulSetController)代码分析作铺垫，后面的文章会结合controller实例代码再仔细讲解细节。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Controller-P1-多实例leader选举.md]]></title>
    <url>%2F2019%2F12%2F09%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Controller-P1-%E5%A4%9A%E5%AE%9E%E4%BE%8Bleader%E9%80%89%E4%B8%BE.html</url>
    <content type="text"><![CDATA[P1-多实例leader选举.md前言Kubernetes多master场景下，核心组件都是以一主多从的模式来运行的，在前面scheduler部分的文章中，并没有分析其主从选举及工作的流程，那么在本篇中，以controller为例，单独作一篇分析组件之间主从工作模式。 入口如scheduler一样，controller的cmd启动也是借助的cobra，对cobra不了解可以回到前面的文章中查看，这里不再赘述，直接顺着入口找到启动函数： 总入口 ==&gt; cmd/kube-controller-manager/controller-manager.go:38 command := app.NewControllerManagerCommand() 默认初始化的controller配置： ==&gt; cmd/kube-controller-manager/app/controllermanager.go:103 c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) ==&gt; cmd/kube-controller-manager/app/controllermanager.go:319 ret := sets.StringKeySet(NewControllerInitializers(IncludeCloudLoops)) ==&gt; cmd/kube-controller-manager/app/controllermanager.go:343 123456789101112131415161718192021222324252627282930313233343536373839404142func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123; controllers := map[string]InitFunc&#123;&#125; controllers["endpoint"] = startEndpointController controllers["replicationcontroller"] = startReplicationController controllers["podgc"] = startPodGCController controllers["resourcequota"] = startResourceQuotaController controllers["namespace"] = startNamespaceController controllers["serviceaccount"] = startServiceAccountController controllers["garbagecollector"] = startGarbageCollectorController controllers["daemonset"] = startDaemonSetController controllers["job"] = startJobController controllers["deployment"] = startDeploymentController controllers["replicaset"] = startReplicaSetController controllers["horizontalpodautoscaling"] = startHPAController controllers["disruption"] = startDisruptionController controllers["statefulset"] = startStatefulSetController controllers["cronjob"] = startCronJobController controllers["csrsigning"] = startCSRSigningController controllers["csrapproving"] = startCSRApprovingController controllers["csrcleaner"] = startCSRCleanerController controllers["ttl"] = startTTLController controllers["bootstrapsigner"] = startBootstrapSignerController controllers["tokencleaner"] = startTokenCleanerController controllers["nodeipam"] = startNodeIpamController controllers["nodelifecycle"] = startNodeLifecycleController if loopMode == IncludeCloudLoops &#123; controllers["service"] = startServiceController controllers["route"] = startRouteController controllers["cloud-node-lifecycle"] = startCloudNodeLifecycleController // TODO: volume controller into the IncludeCloudLoops only set. &#125; controllers["persistentvolume-binder"] = startPersistentVolumeBinderController controllers["attachdetach"] = startAttachDetachController controllers["persistentvolume-expander"] = startVolumeExpandController controllers["clusterrole-aggregation"] = startClusterRoleAggregrationController controllers["pvc-protection"] = startPVCProtectionController controllers["pv-protection"] = startPVProtectionController controllers["ttl-after-finished"] = startTTLAfterFinishedController controllers["root-ca-cert-publisher"] = startRootCACertPublisher return controllers&#125; 这里是所有在初始化配置阶段，启动的各个资源的controller，这里会在后面的章节中展开讲述其中的两三个。 回到run函数继续往下： ==&gt; cmd/kube-controller-manager/app/controllermanager.go:109 Run(c.Complete(), wait.NeverStop) ==&gt; cmd/kube-controller-manager/app/controllermanager.go:153 func Run(c *config.CompletedConfig, stopCh &lt;-chan struct{}) error {} 入口函数就在这里，代码块中已分段注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123; ... // 篇幅有限，省略部分代码 // Setup any healthz checks we will want to use. // 配置leader健康检查的接口，默认检查间隔20s var checks []healthz.HealthzChecker var electionChecker *leaderelection.HealthzAdaptor if c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; electionChecker = leaderelection.NewLeaderHealthzAdaptor(time.Second * 20) checks = append(checks, electionChecker) &#125; // 启动kube-controller的http/https服务 // Start the controller manager HTTP server // unsecuredMux is the handler for these controller *after* authn/authz filters have been applied var unsecuredMux *mux.PathRecorderMux if c.SecureServing != nil &#123; // 展开这里的代码，可以看到会注册两个url，分别是/metrics 和/healthz，分别给prometheus监控和leader健康检查使用 unsecuredMux = genericcontrollermanager.NewBaseHandler(&amp;c.ComponentConfig.Generic.Debugging, checks...) handler := genericcontrollermanager.BuildHandlerChain(unsecuredMux, &amp;c.Authorization, &amp;c.Authentication) // TODO: handle stoppedCh returned by c.SecureServing.Serve if _, err := c.SecureServing.Serve(handler, 0, stopCh); err != nil &#123; return err &#125; &#125; if c.InsecureServing != nil &#123; unsecuredMux = genericcontrollermanager.NewBaseHandler(&amp;c.ComponentConfig.Generic.Debugging, checks...) insecureSuperuserAuthn := server.AuthenticationInfo&#123;Authenticator: &amp;server.InsecureSuperuser&#123;&#125;&#125; handler := genericcontrollermanager.BuildHandlerChain(unsecuredMux, nil, &amp;insecureSuperuserAuthn) if err := c.InsecureServing.Serve(handler, 0, stopCh); err != nil &#123; return err &#125; &#125; // 启动controller工作的run函数，特别标注，会作为回调函数在leader选举成功后执行 run := func(ctx context.Context) &#123; rootClientBuilder := controller.SimpleControllerClientBuilder&#123; ClientConfig: c.Kubeconfig, &#125; var clientBuilder controller.ControllerClientBuilder if c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials &#123; if len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 &#123; // It'c possible another controller process is creating the tokens for us. // If one isn't, we'll timeout and exit when our client builder is unable to create the tokens. klog.Warningf("--use-service-account-credentials was specified without providing a --service-account-private-key-file") &#125; clientBuilder = controller.SAControllerClientBuilder&#123; ClientConfig: restclient.AnonymousClientConfig(c.Kubeconfig), CoreClient: c.Client.CoreV1(), AuthenticationClient: c.Client.AuthenticationV1(), Namespace: "kube-system", &#125; &#125; else &#123; clientBuilder = rootClientBuilder &#125; controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil &#123; klog.Fatalf("error building controller context: %v", err) &#125; saTokenControllerInitFunc := serviceAccountTokenControllerStarter&#123;rootClientBuilder: rootClientBuilder&#125;.startServiceAccountTokenController if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123; klog.Fatalf("error starting controllers: %v", err) &#125; controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) select &#123;&#125; &#125; if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; run(context.TODO()) panic("unreachable") &#125; id, err := os.Hostname() if err != nil &#123; return err &#125; // add a uniquifier so that two processes on the same host don't accidentally both become active id = id + "_" + string(uuid.NewUUID()) rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock, "kube-system", "kube-controller-manager", c.LeaderElectionClient.CoreV1(), c.LeaderElectionClient.CoordinationV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: c.EventRecorder, &#125;) if err != nil &#123; klog.Fatalf("error creating lock: %v", err) &#125; // 主从选举从这里开始 leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123; Lock: rl, LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration, RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration, RetryPeriod: c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration, Callbacks: leaderelection.LeaderCallbacks&#123; // 回调函数，选举成功后，主工作节点开始运行上方的工作run函数 OnStartedLeading: run, OnStoppedLeading: func() &#123; klog.Fatalf("leaderelection lost") &#125;, &#125;, WatchDog: electionChecker, Name: "kube-controller-manager", &#125;) panic("unreachable")&#125; 从这里可以看到，选举成为主领导节点后，才会进入工作流程，先跳过具体的工作流程，来看看leaderelection的选举过程 选举选举入口==&gt; cmd/kube-controller-manager/app/controllermanager.go:252 leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{} 123456789101112func RunOrDie(ctx context.Context, lec LeaderElectionConfig) &#123; le, err := NewLeaderElector(lec) if err != nil &#123; panic(err) &#125; // 加载检查leader健康状态的http接口 if lec.WatchDog != nil &#123; lec.WatchDog.SetLeaderElection(le) &#125; // 开始进入选举 le.Run(ctx)&#125; ==&gt; vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:196 le.Run(ctx) 1234567891011121314151617// Run starts the leader election loopfunc (le *LeaderElector) Run(ctx context.Context) &#123; defer func() &#123; runtime.HandleCrash() le.config.Callbacks.OnStoppedLeading() &#125;() // 1.acquire是竞选函数，如果选举执行失败直接返回 if !le.acquire(ctx) &#123; return // ctx signalled done &#125; ctx, cancel := context.WithCancel(ctx) defer cancel() // 2.竞选成功则另起一个线程，执行上面特别标注的run工作函数，即controller的工作循环 go le.config.Callbacks.OnStartedLeading(ctx) // 3.刷新leader状态函数 le.renew(ctx)&#125; 这个函数里包含多个defer和return，这里额外备注一下defer和return的执行先后顺序： 1231.多个defer是以栈结构保存的，后入先出，下文的defer先执行2.return在defer之后执行3.触发return条件后，return上下文的所有defer中，下文的defer不会被执行 这个函数这里，大概可以看出选举执行的逻辑： 1.选举成功者，开始执行run()函数，即controller的工作函数。同时提供leader状态健康检查的api 2.选举失败者，会结束选举程序。但watchDog会持续运行，监测leader的健康状态 3.选举成功者，在之后会持续刷新自己的leader状态信息 竞选函数：vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:212 123456789101112131415161718192021222324252627// acquire loops calling tryAcquireOrRenew and returns true immediately when tryAcquireOrRenew succeeds.// Returns false if ctx signals done.// 选举者开始循环执行申请，若申请leader成功则返回true，若申请leader失败则进入循环状态，每间隔一段时间再申请一次func (le *LeaderElector) acquire(ctx context.Context) bool &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() klog.Infof("attempting to acquire leader lease %v...", desc) // 进入循环申请leader状态，JitterUntil是一个定时循环功能的函数 wait.JitterUntil(func() &#123; // 申请或刷新leader函数 succeeded = le.tryAcquireOrRenew() le.maybeReportTransition() if !succeeded &#123; klog.V(4).Infof("failed to acquire lease %v", desc) return &#125; le.config.Lock.RecordEvent("became leader") le.metrics.leaderOn(le.config.Name) klog.Infof("successfully acquired lease %v", desc) // 选举成功后，执行cancel()从定时循环函数中跳出来，返回成功结果 cancel() &#125;, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded&#125; 定时执行函数来看下定时循环函数JitterUntil的代码： vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:130 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123; var t *time.Timer var sawTimeout bool for &#123; select &#123; case &lt;-stopCh: return default: &#125; jitteredPeriod := period if jitterFactor &gt; 0.0 &#123; jitteredPeriod = Jitter(period, jitterFactor) &#125; // sliding代表是否将f()的执行时间计算在间隔之内 // 若执行间隔将f()的执行时间包含在内，则在f()开始之前就启动计时器 if !sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; func() &#123; defer runtime.HandleCrash() f() &#125;() // 若执行间隔不将f()的执行时间包含在内，则在f()执行完成之后再启动计时器 if sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; // 在这里，select的case没有优先级之分，因此，可能跳过stop判断，所以，在for loop的前面，也加入了一次stop判断，防止重复执行。 select &#123; case &lt;-stopCh: return // 到达 case &lt;-t.C: sawTimeout = true &#125; &#125;&#125;// resetOrReuseTimer avoids allocating a new timer if one is already in use.// Not safe for multiple threads.func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123; if t == nil &#123; return time.NewTimer(d) &#125; // timer首次启动时，先将t.C channel内的值都取出来，避免channel消费方hang住 if !t.Stop() &amp;&amp; !sawTimeout &#123; &lt;-t.C &#125; // 定时器重置 t.Reset(d) return t&#125; k8s定时任务用的是非常原生的time.timer()来实现的，t.C本质上还是一个channel struct {}，消费方运用select来触发到达指定计时间隔后，消费消息，进入下一次循环。 这里关于select结合channel的用法说明进行以下备注： 1234567在select中，代码逻辑执行步骤如下：1.检查每个case代码块2.如果存在一个case代码块下有数据产生，执行对应case下的内容3.如果多个case代码块下有数据产生，随机选取一个case并执行对应内容，无优先级之分4.如果有default代码块，在没有任何case产生数据时，执行default代码块对应内容5.如果default之后的代码为空，此时也没有任何case产生数据，则跳出select继续执行下文6.如果任何一个case代码块都没有数据产生或代码上下文，同时也没有default，则select阻塞等待 关于go time.Timer，这里有一篇文章讲得很好： https://tonybai.com/2016/12/21/how-to-use-timer-reset-in-golang-correctly/ 申请/刷新leader函数vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:293 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// tryAcquireOrRenew tries to acquire a leader lease if it is not already acquired,// else it tries to renew the lease if it has already been acquired. Returns true// on success else returns false.// 在初次选举、后续间隔刷新状态 这两处地方都会调用这个函数// 如果参选者不是leader则尝试选举，如果已经是leader，则尝试续约租期，最后刷新信息func (le *LeaderElector) tryAcquireOrRenew() bool &#123; now := metav1.Now() leaderElectionRecord := rl.LeaderElectionRecord&#123; HolderIdentity: le.config.Lock.Identity(), LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), RenewTime: now, AcquireTime: now, &#125; // 1. obtain or create the ElectionRecord // 第1步：获取当前的leader的竞选记录，如果当前还没有leader记录，则创建 // 首先获取当前的leader记录 oldLeaderElectionRecord, err := le.config.Lock.Get() if err != nil &#123; if !errors.IsNotFound(err) &#123; klog.Errorf("error retrieving resource lock %v: %v", le.config.Lock.Describe(), err) return false &#125; if err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123; klog.Errorf("error initially creating leader election record: %v", err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true &#125; // 第2步，对比观察记录里的leader与当前实际的leader // 2. Record obtained, check the Identity &amp; Time if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123; // 如果参选者的上一次观察记录中的leader，不是当前leader，则修改记录，以当前leader为准 le.observedRecord = *oldLeaderElectionRecord le.observedTime = le.clock.Now() &#125; if len(oldLeaderElectionRecord.HolderIdentity) &gt; 0 &amp;&amp; // 如果参选者不是当前的leader，且当前leader的任期尚未结束，则返回false，参选者选举失败 le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp; !le.IsLeader() &#123; klog.V(4).Infof("lock is held by %v and has not yet expired", oldLeaderElectionRecord.HolderIdentity) return false &#125; // 3. We're going to try to update. The leaderElectionRecord is set to it's default // here. Let's correct it before updating. if le.IsLeader() &#123; // 如果参选者就是当前的leader本身，则修改记录里的当选时间变为它此前的当选时间，而不是本次时间，变更次数维持不变 leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions &#125; else &#123; // 如果参选者不是leader(则说明当前leader在任期已经结束，但并未续约)，则当前参选者变更为新的leader leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 &#125; // update the lock itself // 更新leader信息，更新leader锁，返回true选举过程顺利完成 if err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123; klog.Errorf("Failed to update lock: %v", err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true&#125; 这一段代码中有多个leader记录信息相关的变量，很容易混淆，为了便于理解这里抽出来说明下： 1234LeaderElector # 参选者，每一个controller进程都会参与leader选举oldLeaderElectionRecord # 本次选举开始前，leader锁中记载的当前leaderleaderElectionRecord # 本次选举的leader记录，最终会更新进入新的leader锁中observedRecord # 每个参选者都会定期观察当前的leader信息，记录在自身的这个字段中 先来看第1步中是怎么获取当前leader记录的： vendor/k8s.io/client-go/tools/leaderelection/resourcelock/leaselock.go:39 1234567891011// Get returns the election record from a Lease specfunc (ll *LeaseLock) Get() (*LeaderElectionRecord, error) &#123; var err error // 1.取得lease对象 ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Get(ll.LeaseMeta.Name, metav1.GetOptions&#123;&#125;) if err != nil &#123; return nil, err &#125; // 2.将lease.spec转为LeaderElectionRecord记录并返回 return LeaseSpecToLeaderElectionRecord(&amp;ll.lease.Spec), nil&#125; 取得lease对象的方法在这里： vendor/k8s.io/client-go/kubernetes/typed/coordination/v1/lease.go:66 func (c *leases) Get(name string, options metav1.GetOptions) (result *v1.Lease, err error) {} 转换并返回的LeaderElectionRecord结构体是这样的： 1234567LeaderElectionRecord&#123; HolderIdentity: holderIdentity, // leader持有标识 LeaseDurationSeconds: leaseDurationSeconds, // 选举间隔 AcquireTime: metav1.Time&#123;spec.AcquireTime.Time&#125;, // 选举成为leader的时间 RenewTime: metav1.Time&#123;spec.RenewTime.Time&#125;, // 续任时间 LeaderTransitions: leaseTransitions, // leader位置的转接次数&#125; 对返回的LeaderElectionRecord进行对比，如果是自身，则续约，如果不是自身，则看leader是否过期，对leader lock信息相应处理。 刷新选举状态函数vendor/k8s.io/client-go/tools/leaderelection/leaderelection.go:234 123456789101112131415161718192021222324252627282930313233343536373839func (le *LeaderElector) renew(ctx context.Context) &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() wait.Until(func() &#123; timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline) defer timeoutCancel() // 间隔刷新leader状态，成功则续约，不成功则释放 err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) &#123; done := make(chan bool, 1) go func() &#123; defer close(done) done &lt;- le.tryAcquireOrRenew() &#125;() select &#123; case &lt;-timeoutCtx.Done(): return false, fmt.Errorf("failed to tryAcquireOrRenew %s", timeoutCtx.Err()) case result := &lt;-done: return result, nil &#125; &#125;, timeoutCtx.Done()) le.maybeReportTransition() desc := le.config.Lock.Describe() if err == nil &#123; klog.V(5).Infof("successfully renewed lease %v", desc) return &#125; le.config.Lock.RecordEvent("stopped leading") le.metrics.leaderOff(le.config.Name) klog.Infof("failed to renew lease %v: %v", desc, err) cancel() &#125;, le.config.RetryPeriod, ctx.Done()) // if we hold the lease, give it up if le.config.ReleaseOnCancel &#123; le.release() &#125;&#125; tryAcquireOrRenew()和循环间隔执行函数同上面所讲基本一致，这里就不再说明了。 总结组件选举大致可以概括为以下流程： 初始时，各实例均为LeaderElector，最先开始选举的，成为leader，成为工作实例。同时它会维护一份信息(leader lock)供各个LeaderElector探测，包括状态信息、健康监控接口等。 其余LeaderElector，进入热备状态，监控leader的运行状态，异常时会再次参与选举 leader在运行中会间隔持续刷新自身的leader状态。 不止于controller，其余的几个组件，主从之间的工作关系也应当是如此。 感谢阅读，欢迎指正]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Controller-总览篇]]></title>
    <url>%2F2019%2F12%2F05%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Controller-%E6%80%BB%E8%A7%88%E7%AF%87.html</url>
    <content type="text"><![CDATA[Controller源码分段阅读导航 多实例leader选举 [Informer工作流程] [Deployment Controller] [StafulSet Controlle] 待补充 概述kube-controller的作用引述 首先依照惯例，贴两篇官方对 于controller的设计逻辑和运行机制的说明文档： https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md https://github.com/kubernetes/community/blob/master/contributors/design-proposals/apps/controller_history.md Controller是做什么用的？ Controller通过watch apiServer，循环地观察监控着某些特定的资源对象，获取它们当前的状态，对它们进行对比、修正、收敛，来使这些对象的状态不断靠近、直至达成在它们的声明语义中所期望的目标状态，这即是controller的作用。 伪代码 一个简单的controller工作模式的伪代码示例： 12345678for &#123; // 期望状态 desired := getDesiredState() // 当前状态 current := getCurrentState() // 使当前状态达到期望状态 makeChanges(desired, current)&#125; kube-controller的组成kube-controller是一个控制组件，根据我们的使用经验，有多种经常使用的资源，都不是实际地直接进行任务计算的资源类型，而在申明之后由k8s自动发现并保证以达成申明语义状态的逻辑资源，例如deployment、statefulSet、pvc、endpoint等，这些资源都分别由对应的controller子组件，那么这样的controller子组件有多少呢？如下图： 可见controller的组件数量是非常之多的，因此在本部分中计划只抽选其中的deploymentController和statefulSetController这两种常见的对pod管理类型资源对应的controller来进行源码分析。 代码结构启动入口 kubernetes/src/k8s.io/kubernetes/cmd/kube-controller-manager/controller-manager.go 功能模块 kubernetes/src/k8s.io/kubernetes/pkg/controller]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes各组件参数配置优化建议]]></title>
    <url>%2F2019%2F11%2F17%2FKubernetes%E5%90%84%E7%BB%84%E4%BB%B6%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE.html</url>
    <content type="text"><![CDATA[Kubernetes各组件参数配置优化建议kubernetes虽然默认配置下已经足够可用满足常见的中小规模场景，但是若是将各组件参数、内核参数进行适当的调整，以达到更贴合使用场景的参数值，对集群运行的稳定性、故障切换能力等方面会有不小的提升。下面介绍一下各组件生产运行常做的一些参数调整。 Kubelet参数配置kubelet在各个组件之中，作为唯一的分布在每个节点上的daemon控制程序，应该也是需要调整参数最多的组件了，从此前的flag形式的传参，到自1.11版本起的改用专属配置文件，充分说明了其配置调整是很常用的场景，下面举几个常见的配置项，如node状态汇报频率、pod最大数量、以及生产运行非常重要的pod驱逐、资源保留等。 旧版本kubelet配置在1.11版本之前，kubelet通过命令flag的方式指定和调整参数，例如： --node-status-update-frequency=5s # 向apiserver刷新node状态的时间间隔，默认10s --serialize-image-pulls=false # 是否串行拉取镜像，默认为true，false改为并行，提升拉取镜像速度 --max-pods=200 # 单node运行的最大pod数量，默认110，注意调整不要超过pod CIDR的单个子网可用地址数量。 --eviction-hard=memory.available&lt;1024Mi,nodefs.available&lt;10%# 节点资源小于指定值时开启硬驱逐上吗的pod 资源保留配置 为防止pod使用过多的资源，从而挤占和影响了k8s控制组件(例如kubelet)、系统管理进程的正常运行，指定以下配置为这两者保留一部分的资源。与pod QOS 一样，资源保留的方式同样也是使用cgroup来做资源隔离保证。 --enforce-node-allocatable=pods,kube-reserved,system-reserved # 开启针对这几者的cgroup资源管理。 --kube-reserved-cgroup=/system.slice/kubelet.service # 指定k8s组件保留资源对应的cgroup路径 --system-reserved-cgroup=/system.slice # 指定系统组件保留资源对应的cgroup路径 `–kube-reserved=cpu=1,memory=2Gi # 指定k8s组件保留1g/1c的资源 `–system-reserved=cpu=1,memory=1Gi # 为系统运行保留1g/1c的资源 以上参数配置在kubelet systemd启动服务内，例如kubeadm安装的集群，kubelet启动服务一般在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf路径内，配置环境变量的形式来修改启动配置，例如： 12345# 增加一个变量Environment="RESOUCE_RESERVE_CONFIG_ARGS=--kube-reserved-cgroup=/system.slice/kubelet.service --system-reserved-cgroup=/system.slice --kube-reserved=cpu=1,memory=2Gi,ephemeral-storage=1Gi --system-reserved=cpu=1,memory=1Gi,ephemeral-storage=1Gi# 启动命令末尾加入参数变量ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $RESOUCE_RESERVE_CONFIG_ARGS 新版本kubelet配置1.11及以后的版本，开始使用配置文件的形式指定kubelet配置，不再建议使用flag参数的形式。配置文件的路径在启动服务的文件里可以找到，如果使用的是kubeadm安装的，路径一般是/var/lib/kubelet/config.yaml各版本可能有些许不同。参数其实还是与上方一致，只不过abc-def式写法变成了go更推荐的驼峰式写法，这里不再对参数作解释，直接贴出配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687address: 0.0.0.0apiVersion: kubelet.config.k8s.io/v1beta1authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crtauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30scgroupDriver: cgroupfscgroupsPerQOS: trueclusterDNS:- 10.112.0.10clusterDomain: cluster.localconfigMapAndSecretChangeDetectionStrategy: WatchcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuCFSQuotaPeriod: 100mscpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: true# 这里添加kube-reserved, system-reserved仅作参考，官方建议不要开启enforceNodeAllocatable:- pods- kube-reserved- system-reservedeventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% # 加到1G memory.available: 1024Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 3m0sfailSwapOn: truefileCheckFrequency: 20shairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kind: KubeletConfigurationkubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: truemaxOpenFiles: 1000000# 调整为200maxPods: 200nodeLeaseDurationSeconds: 40nodeStatusReportFrequency: 1m0snodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1port: 10250registryBurst: 10registryPullQPS: 5resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0sserializeImagePulls: falsestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0skubeReservedCgroup: /system.slice/kubelet.servicekubeReserved: cpu: "1" memory: 2GisystemReservedCgroup: /system.slice# 官方不建议开启systemReserved，因为可能会带来某些不可控的系统关键进程的异常，这里列出仅作参考systemReserved: cpu: "1" memory: 2Gi 看到网上有很多内容高度一致的文章将systemReserved和kubeReserved两者都同时开启了，其实官方不建议开启systemReserved，因为这可能会导致系统关键进程遭遇不可控的风险，说明如下： Be extra careful while enforcing system-reserved 注：非常重要！kubelet并不会自动创建cgroup，因此，需要在启动服务前为其指定创建相应cgroup的命令,建议写入启动服务文件内，无论是传统指定flag方式，还是配置文件方式，都需要在启动服务文件内添加创建cgroup的命令，否则kubelet服务会启动失败 这里贴一个新版本的启动服务文件内容： 12345678910111213141516171819202122~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# Note: This dropin only works with kubeadm and kubelet v1.11+[Service]Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamicallyEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.EnvironmentFile=-/etc/default/kubelet# 下面命令提前创建cgroupExecStartPre=/bin/mkdir -p /sys/fs/cgroup/pids/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpu/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuacct/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/cpuset/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/memory/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/devices/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/blkio/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/hugetlb/system.slice/kubelet.serviceExecStartPre=/bin/mkdir -p /sys/fs/cgroup/systemd/system.slice/kubelet.serviceExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 修改完以上配置后，执行systemctl daemon-reload &amp;&amp; systemctl restart kubelet重启kubelet，若无意外，kubelet重启正常后，可检验cgroup是否如期创建指定的内存限制，如下刚好是2g，说明cgroup指定已成功： 12cat /sys/fs/cgroup/memory/system.slice/kubelet.service/memory.limit_in_bytes2147483648 当然，以上的kubelet配置修改需要重启kubelet，如果你需要直接在线更新kubelet配置的话，可以参考官方的的步骤进行修改配置： Reconfiguring the Kubelet on a Live Node in your Cluster Controller参数配置controller常调整的参数是针对其驱逐功能的，即在node进入离线状态后，将node置为ConditionUnknown状态，并在持续一段时间后开始依一定的次序驱逐问题node上面pod到其他node上，以尽快恢复到声明数量的ready副本数。 --node-cidr-mask-size # node上的pod cidr掩码位数，默认为24位，即最多253个可用地址，视地址空间和pod数量调整。 --node-monitor-period # 检查当前node健康状态的周期间隔，默认5s --node-monitor-grace-period # 当前node超过了这个指定周期后，即视node为不健康，进入ConditionUnknown状态，默认40s --pod-eviction-timeout # 当node进入notReady状态后，经过这个指定时间后，会开始驱逐node上的pod，默认5m --large-cluster-size-threshold # 判断集群是否为大集群，默认为 50，即 50 个节点以上的集群为大集群。 --unhealthy-zone-threshold：# 故障节点数比例，默认为 55% --node-eviction-rate # 开始对node进行驱逐操作的频率，默认0.1个/s，即每10s最多驱逐某一个node上的pod，避免当master出现问题时，会有批量的node出现异常，这时候如果一次性驱逐过多的node，对master造成额外的压力 --secondary-node-eviction-rate： # 当集群规模大于large-cluster-size-threshold个node时，视为大集群， 集群中只要有55%的node不健康， 就会认为master出现了故障， 会将驱逐速率从0.1降为0.001； 如果集群规模小于large-cluster-size-threshold个node， 集群中出现55%的node不健康，就会停止驱逐。 配置建议： 中大规模集群(500+ node) 大规模集群一般节点数较多，优势是可用性、抗风险能力强，劣势是组件之间的交互频繁，会给apiServer和etcd带来较大的压力，因此配置建议： --node-monitor-period 适当增加 --large-cluster-size-threshold 适当增加 --unhealthy-zone-threshold 适当减小 --pod-eviction-timeout 视服务可用性保障级别适当调整 --node-eviction-rate 视服务可用性保障级别适当增加 --secondary-node-eviction-rate 视服务可用性保障级别适当增加 小规模集群(100- node) 小规模集群一般节点数不多，因此副本数量一般也较小，对节点中断的容忍度和容忍时间较低，因此配置建议： --node-monitor-period 不变或可适当减小 --pod-eviction-timeout大幅减小，如1m-2m，单节点故障尽快转移上面的pod --node-eviction-rate 建议减小，避免问题pod转移导致集群快速雪崩，预留防范处理时间 如果使用kubeadm部署的，修改yaml文件即可自动触发 apiserver/scheduler/controller这几个static pod的重启, 3者的yaml文件都位于：/etc/kubernetes/manifests/下，修改保存pod即会自动重启加载新配置。 另注：默认驱逐pod的优先级 根据pod的QOS保证级别，优先驱逐调度保证保障质量高级别的pod最先恢复正常： BestEffort &gt; Burstable &gt; Guaranteed ApiServer参数配置 --max-mutating-requests-inflight # 单位时间内的最大修改型请求数量，默认200 --max-requests-inflight # 单位时间内的最大非修改型(读)请求数量，默认400 --watch-cache-sizes # 各类resource的watch cache，默认100，资源数量较多时需要增加 --feature-gates=VolumeSubpathEnvExpansion=true # 开启alpha版的功能，默认alpha版功能是关闭的，可以通过这个参数指定来开启某些alpha状态的功能，关于各功能所处的alpha、beta、GA状态，请查询官方文档： https://kubernetes-io-vnext-staging.netlify.com/docs/reference/command-line-tools-reference/kube-apiserver/ Scheduler参数配置scheduler的配置项比较少，因为调度规则已经是很明确了，不过可以自定义预选和优选策略。 --kube-api-qps # 请求apiserver的最大qps，默认50 --policy-config-file # json文件，不指定时使用默认的调度预选和优选策略，可以自定义指定，样例参考：scheduler-policy-config.json 调度策略分为预选和优选两个步骤，建议对其过程或源码有一定了解后，再做策略调整，调度过程可以参考我的源码笔记： 《Kubernetes Source Code Note》 调度策略的调整要与自身的环境结合，默认的优选策略中，每一项计算纬度的权重默认几乎都是1，在我的环境中，比较重视资源平均利用率以及服务副本的高可用性，因此，将SelectorSpreadPriority和BalancedResourceAllocation调高。下面是我的json文件： scheduler-policy-config.json 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; "apiVersion": "v1", "kind": "Policy", "predicates": [ &#123;"name": "NoVolumeZoneConflict"&#125;, &#123;"name": "MaxEBSVolumeCount"&#125;, &#123;"name": "MaxGCEPDVolumeCount"&#125;, &#123;"name": "MaxAzureDiskVolumeCount"&#125;, &#123;"name": "MatchInterPodAffinity"&#125;, &#123;"name": "NoDiskConflict"&#125;, &#123;"name": "GeneralPredicates"&#125;, &#123;"name": "PodToleratesNodeTaints"&#125;, // 下面三项在v1.12及之后的版本下需要删除掉，这是一个bug，删掉不影响正常工作。 // 参考：https://github.com/kubernetes/kops/issues/7740 &#123;"name": "CheckNodeMemoryPressure"&#125;, &#123;"name": "CheckNodeDiskPressure"&#125;, &#123;"name": "CheckNodeCondition"&#125;, &#123; "argument": &#123; "serviceAffinity": &#123; "labels": [ "region" ] &#125; &#125;, "name": "Region" &#125; ], "priorities": [ &#123; "name": "SelectorSpreadPriority", "weight": 3 &#125;, &#123; "name": "BalancedResourceAllocation", "weight": 3 &#125;, &#123; "name": "InterPodAffinityPriority", "weight": 1 &#125;, &#123; "name": "LeastRequestedPriority", "weight": 1 &#125;, &#123; "name": "NodePreferAvoidPodsPriority", "weight": 10000 &#125;, &#123; "name": "NodeAffinityPriority", "weight": 1 &#125;, &#123; "name": "TaintTolerationPriority", "weight": 1 &#125;, &#123; "argument": &#123; "serviceAntiAffinity": &#123; "label": "zone" &#125; &#125;, "name": "Zone", "weight": 2 &#125; ]&#125; 注意 指定policy配置启动scheduler后，会完全覆盖默认注册的所有调度策略，因此，即使是无需改动的策略纬度，必须完全显示的在policy json中写出来，如果不写出，scheduler重启后该项纬度不再使用。 配置完成后，修改scheduler的启动参数，指定此json文件，同时，最好将此文件使用configMap形式或其他形式持久化挂载。 内核参数配置12345678910111213141516171819202122232425262728fs.file-max=1000000# max-file 表示系统级别的能够打开的文件句柄的数量# 配置arp cache 大小net.ipv4.neigh.default.gc_thresh1=1024# 存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。net.ipv4.neigh.default.gc_thresh2=4096# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。net.ipv4.neigh.default.gc_thresh3=8192# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。以上三个参数，当内核维护的arp表过于庞大时候，可以考虑优化net.netfilter.nf_conntrack_max=10485760# 允许的最大跟踪连接条目，是在内核内存中netfilter可以同时处理的“任务”（连接跟踪条目）net.netfilter.nf_conntrack_tcp_timeout_established=300net.netfilter.nf_conntrack_buckets=655360# 哈希表大小（只读）（64位系统、8G内存默认 65536，16G翻倍，如此类推）net.core.netdev_max_backlog=10000# 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。关于conntrack的详细说明：https://testerhome.com/topics/7509fs.inotify.max_user_instances=524288# 默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限fs.inotify.max_user_watches=524288# 默认值: 8192 指定了每个inotify instance相关联的watches的上限]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Pod 无法终结问题]]></title>
    <url>%2F2019%2F09%2F27%2FKubernetes%20Pod%20%E6%97%A0%E6%B3%95%E7%BB%88%E7%BB%93%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[Kubernetes Pod 无法终结问题问题现象某pod运行出现异常，被终结后一直无法正常退出，在kubernetes端的现象： 12root@h009028:~# kubectl get pods -o wide --all-namespaces | grep bsys | grep Terdefault bsysprod-5bf5f44b84-2f5qw 1/1 Terminating 0 6d 172.26.0.50 h009028 pod一直处于Terminating状态 在docker端的现象: 12root@h009028:~# docker ps | grep bsys6b7697bc161e nginx@sha256:c792a8c50279261c7c72f144f86e91c76366a95877a7ed02927ee5b15f3c67a8 "/bin/bash -c 'wge..." 6 days ago Up 6 days k8s_bsysprod_bsysprod-5bf5f44b84-2f5qw_default_80a684b9-db5e-11e9-b952-141877468256_0 排查1.尝试docker exec 进入容器内或执行命令，均失败: 12root@h009028:~# docker exec -it 6b7697bc161e bashrpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:240: creating new parent process caused "container_linux.go:1254: running lstat on namespace path \"/proc/120770/ns/ipc\" caused \"lstat /proc/120770/ns/ipc: no such file or directory\"" 原因是容器的初始pid已经被回收： 12root@h009028:~# ps -ef | grep 120770root 45063 124565 0 10:02 pts/5 00:00:00 grep --color=auto 120770 2.查看docker daemon日志: 123456789101112root@h009028:~# journalctl -fu docker.serviceSep 27 09:50:44 h009028 dockerd[2231]: time="2019-09-27T09:50:44.274858423+08:00" level=error msg="Error running exec in container: rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:240: creating new parent process caused \"container_linux.go:1254: running lstat on namespace path \\\"/proc/120770/ns/ipc\\\" caused \\\"lstat /proc/120770/ns/ipc: no such file or directory\\\"\"\n"Sep 27 09:51:09 h009028 dockerd[2231]: time="2019-09-27T09:51:09.694681419+08:00" level=error msg="Handler for POST /v1.27/containers/2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909/stop returned error: Container 2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909 is already stopped"Sep 27 09:51:40 h009028 dockerd[2231]: time="2019-09-27T09:51:40.557659277+08:00" level=info msg="Container 6b7697bc161e482a761be87cb39f5d178ea6825d8a40710aec10a3a0e7988875 failed to exit within 30 seconds of signal 15 - using the force"Sep 27 09:51:50 h009028 dockerd[2231]: time="2019-09-27T09:51:50.558415715+08:00" level=info msg="Container 6b7697bc161e failed to exit within 10 seconds of kill - trying direct SIGKILL"## 这个post请求是用来停止pod中附属的pause容器的，pause容器正常退出没有问题Sep 27 09:53:10 h009028 dockerd[2231]: time="2019-09-27T09:53:10.559098158+08:00" level=error msg="Handler for POST /v1.27/containers/2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909/stop returned error: Container 2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909 is already stopped"Sep 27 09:53:41 h009028 dockerd[2231]: time="2019-09-27T09:53:41.659795797+08:00" level=info msg="Container 6b7697bc161e482a761be87cb39f5d178ea6825d8a40710aec10a3a0e7988875 failed to exit within 30 seconds of signal 15 - using the force"Sep 27 09:53:51 h009028 dockerd[2231]: time="2019-09-27T09:53:51.660637583+08:00" level=info msg="Container 6b7697bc161e failed to exit within 10 seconds of kill - trying direct SIGKILL"Sep 27 09:55:11 h009028 dockerd[2231]: time="2019-09-27T09:55:11.497119594+08:00" level=error msg="Handler for POST /v1.27/containers/2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909/stop returned error: Container 2ef9c159685fea3ae04ca6f7213c232d1e3b20c88de70a11be16361745cd5909 is already stopped"Sep 27 09:55:42 h009028 dockerd[2231]: time="2019-09-27T09:55:42.415539477+08:00" level=info msg="Container 6b7697bc161e482a761be87cb39f5d178ea6825d8a40710aec10a3a0e7988875 failed to exit within 30 seconds of signal 15 - using the force"Sep 27 09:55:52 h009028 dockerd[2231]: time="2019-09-27T09:55:52.416319734+08:00" level=info msg="Container 6b7697bc161e failed to exit within 10 seconds of kill - trying direct SIGKILL" 日志分析可以得知： 默认向容器发送的sig 15(SIGTREM)并没有使容器正常退出，10S后，执行SIGKILL使其强制退出,但实际的结果却是，SIGKILL也没能奏效，容器依然没有退出。 3.查看容器内现有未终结的进程 1234567root@h009028:~# docker top 6b7697bc161eUID PID PPID C STIME TTY TIME CMDwww-data 81091 121143 11 08:49 ? 00:08:39 php-fpm: pool wwwwww-data 81097 121143 13 08:49 ? 00:09:42 php-fpm: pool wwwwww-data 81098 121143 16 08:49 ? 00:12:14 php-fpm: pool wwwwww-data 81099 121143 0 08:49 ? 00:00:02 php-fpm: pool wwwwww-data 81100 121143 14 08:49 ? 00:10:29 php-fpm: pool www 发现容器内部应该作为PID 1的初始进程早已终结，但却有一些子进程还存在，应该就是这些进程导致容器无法彻底退出,查看它们的pid为121143的父进程的信息: 123456789root@h009028:~# ps -ef | grep 121143root 5930 124565 0 09:49 pts/5 00:00:00 grep --color=auto 121143www-data 81091 121143 10 08:49 ? 00:06:08 php-fpm: pool wwwwww-data 81097 121143 12 08:49 ? 00:07:34 php-fpm: pool wwwwww-data 81098 121143 16 08:49 ? 00:09:51 php-fpm: pool wwwwww-data 81099 121143 0 08:49 ? 00:00:02 php-fpm: pool wwwwww-data 81100 121143 13 08:49 ? 00:07:48 php-fpm: pool wwwwww-data 81181 121143 14 08:49 ? 00:08:43 php-fpm: pool wwwroot 121143 121094 0 Sep20 ? 00:00:01 [bash] 123456789101112131415161718192021222324252627282930313233343536root@h009028:~# cat /proc/121143/statusName: bashState: S (sleeping)Tgid: 121143Ngid: 0Pid: 121143PPid: 121094TracerPid: 0Uid: 0 0 0 0Gid: 0 0 0 0FDSize: 0Groups:NStgid: 121143 1NSpid: 121143 1NSpgid: 121143 1NSsid: 121143 1Threads: 1SigQ: 7/515154SigPnd: 0000000000000000ShdPnd: 0000000000014100SigBlk: 0000000000010000SigIgn: 0000000000010004SigCgt: 0000000000000002CapInh: 00000000a80425fbCapPrm: 00000000a80425fbCapEff: 00000000a80425fbCapBnd: 00000000a80425fbCapAmb: 0000000000000000Seccomp: 0Cpus_allowed: 0000,00000000,00000000,00000000,ffffffffCpus_allowed_list: 0-31Mems_allowed: 00000000,00000003Mems_allowed_list: 0-1voluntary_ctxt_switches: 19402nonvoluntary_ctxt_switches: 45 此进程处于sleep状态，kill掉它和它的子进程： 1ps -ef | grep 121143 | awk '&#123;print $2&#125;' | xargs kill -s 9 再来查看一下容器状态： 12root@h009028:~# docker ps -a | grep bsysroot@h009028:~# 容器已退出 原因容器内所有的其他进程都是pid 1号进程的子进程，正常情况下，pid收到sig 15退出后，会发送信号处理好依附自身的子进程，但偶然情况下还是会出现一些问题：父进程退出前，未来得及处理完容器内的子进程，导致它们变成了孤儿进程,而容器本身的pid 1号进程此时已经退出，docker daemon向其发送sig kill，此信号只针对pid 1的主进程，对这些孤儿进程不生效，从而导致了容器始终无法终结。处理了这些孤儿进程后，容器就可以正常退出了。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang并发模型]]></title>
    <url>%2F2019%2F09%2F25%2FGolang%E5%B9%B6%E5%8F%91%E6%A8%A1%E5%9E%8B.html</url>
    <content type="text"><![CDATA[Golang并发模型前言Go实现了两种并发形式。第一种是大家普遍认知的：多线程共享内存，这是最常见的各语言中的多线程并发模型，go本身也支持。另外一种是Go语言特有的，也是Go语言推荐的CSP（communicating sequential processes）并发模型。Go语言提倡以通信的方式来共享内存，这句话相信看过一些go相关文档的同学一定不陌生:Do not communicate by sharing memory; instead, share memory by communicating. 那么本篇就来看一看go所推崇的并发模型 线程模型在讲并发模型之前，首先来看看线程的模型。 我们知道，linux线程的工作区域被划分为用户空间和内核空间，用户空间是程序内部活动所处在的空间，当涉及到资源调用时(CPU/IO/MEM/NET等资源)，需要向内核发起内核syscall，由内核来完成对程序的资源分配。因此，程序的线程(用户线程)，最终都是落到内核线程上来被执行的，内核态的线程，简称KSE，一般一个内核线程对应一颗cpu核心。用户线程和KSE线程的关联映射模型分为以下几种： 用户级线程模型 如图所示，进程内的多个用户态的线程对应着一个内核线程，程序线程的创建、终止、切换或者同步等线程工作必须自身来完成。 内核级线程模型 这种模型直接调用操作系统的内核级线程，所有线程的创建、终止、切换、同步等操作，都由内核来管理。 两级线程模型 这种模型是介于用户级线程模型和内核级线程模型之间的一种线程模型。这种模型的实现非常复杂，和内核级线程模型类似，一个进程中可以对应多个内核级线程，但是进程中的线程不和内核线程一一对应；这种线程模型会先创建多个内核级线程，然后用自身的用户级线程去对应创建的多个内核级线程，自身的用户级线程需要本身程序去调度，内核级的线程交给操作系统内核去调度。 Go语言的goroutine线程模型就是一种特殊的两级线程模型,即我们常说的MPG模型。 传统并发模型普通的线程并发模型，就是像Java、C++、或者Python，线程间通信都是通过共享内存的方式来进行的。非常典型的方式就是，在访问共享数据（例如数组、Map、或者某个结构体或对象）的时候，通过锁来访问这些数据结构，然后基于里面的数据块，并发执行相关操作。因此，在很多时候，衍生出一种方便操作的数据结构，叫做“线程安全的数据结构”。例如Java提供的包”java.util.concurrent”中的数据结构。Go的sync包也提供了对传统的线程并发模型的支持，但Go本身更推崇CSP的并发模型，通过goroutine和channel`来组合实现： goroutine 是Go语言中并发的执行单位。和传统概念上的”线程“类似，可以理解为go语言的微线程 channel是Go语言中各个并发执行单位(goroutine)之间的通信机制。 通俗的讲，就是各个goroutine之间通信的”管道“，类似于Linux中的pipe管道 关于go的传统并发模型、channel相关说明，这里不作赘述，直接进入go CSP(MPG)并发模型 MPG模型M指的是Machine，一个M直接关联了一个内核线程,可以直接理解为M就是内核线程。P指的是processor，代表了M所需的上下文环境，也是处理用户级代码逻辑的处理器。G指的是Goroutine，即Go实现的轻量级线程。 MPG关系图: 以上这个图讲的是两个线程(内核线程)的情况。一个M会对应一个内核线程，一个M也会连接一个上下文P，一个上下文P相当于一个“处理器”，一个P连接一个或者多个Goroutine。简单来说，G是go实现的微线程，G要想进入到M中执行，需要通过P来协调。P的数量是在启动时被设置为环境变量GOMAXPROCS的值，或者通过运行时调用函数runtime.GOMAXPROCS()进行设置。Processor数量固定意味着任意时刻只有固定数量的线程在运行go代码。Goroutine中就是我们要执行并发的代码。如图所示，P正在执行的Goroutine为蓝色的；处于待执行状态的Goroutine为灰色的，灰色的Goroutine形成了一个队列runqueues。 这个模型多引入了一个P的角色，作为中间人来协调goroutine和内核线程的调度分配，下面着重理解一下P这个角色的作用。 P的作用1.保存上下文P起着保存Goroutine上下文的作用，例如线程在执行syscall时，线程G和实际执行的线程M会进入阻塞状态，则此时P需要保存好上下文状态，然后被其他的M内核线程所接管，以便开始调度P管理的其他G线程。 如图所示： G0–&gt;M0在执行syscall后阻塞了，则P会被M1接管，以便调度runqueues内的其他G线程。 2.均衡分配G线程当某个P所接管的G队列为空时，代表当前队列里已经没有需要调度的线程了，但别的P管理的队列中或许还有任务在排队，因此，Go的做法是，空闲的P会从繁忙的P的队列中抽取一半的G到自身的队列中来，以均衡分配工作线程。 总结MPG模型中，P这个角色容易使人迷惑，理解它的作用有助于理解MPG模型的整体工作模式。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-P5-Pod优先级抢占调度.md]]></title>
    <url>%2F2019%2F09%2F02%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-P5-Pod%E4%BC%98%E5%85%88%E7%BA%A7%E6%8A%A2%E5%8D%A0%E8%B0%83%E5%BA%A6.html</url>
    <content type="text"><![CDATA[P5-Pod优先级抢占调度1. 前言前面的两篇文章中，已经讲过了调度pod的算法(predicate/priority)，在kubernetes v1.8版本之后可以指定pod优先级(v1alpha1)，若资源不足导致高优先级pod匹配失败，高优先级pod会转而将部分低优先级pod驱逐，以抢占低优先级pod的资源尽力保障自身能够调度成功，那么本篇就从代码的层面展开看一看pod抢占调度的逻辑。 2. 抢占调度入口在P1-入口篇中我们找到了调度算法计算的入口，随后展开了调度算法的两篇解读，本篇我们再次回到此入口的位置，接着往下看: pkg/scheduler/scheduler.go:457 123456789101112131415161718192021222324252627func (sched *Scheduler) scheduleOne() &#123; ... // 省略 // 调度算法计算入口 scheduleResult, err := sched.schedule(pod) if err != nil &#123; // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok &#123; if !util.PodPriorityEnabled() || sched.config.DisablePreemption &#123; klog.V(3).Infof("Pod priority feature is not enabled or preemption is disabled by scheduler configuration." + " No preemption is performed.") &#125; else &#123; preemptionStartTime := time.Now() sched.preempt(pod, fitError) // 抢占调度逻辑入口 metrics.PreemptionAttempts.Inc() ... // 省略 metrics.PodScheduleFailures.Inc() &#125; else &#123; klog.Errorf("error selecting node for pod: %v", err) metrics.PodScheduleErrors.Inc() &#125; return &#125; 注释中可看出，若在筛选算法中并未找到fitNode且返回了fitError，那么就会进入基于pod优先级的资源抢占的逻辑，入口是sched.preempt(pod, fitError)函数。在展开抢占逻辑之前，我们先来看一看pod优先级是怎么一回事吧。 2.1. Pod优先级的定义字面意义上来理解，pod优先级可以在调度的时候为高优先级的pod提供资源空间保障，若出现资源紧张的情况，则在其他约束规则允许的情况下，高优先级pod会抢占低优先级pod的资源。此功能在1.11版本以后默认开启，默认情况下pod的优先级是0，优先级值high is better，具体说明来看看官方文档的解释吧: Pod Priority and Preemption 下面列举一个pod优先级使用的实例： 12345678910111213141516171819202122# Example PriorityClassapiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: high-priorityvalue: 1000000globalDefault: falsedescription: "This priority class should be used for XYZ service pods only."# Example Pod specapiVersion: v1kind: Podmetadata: name: nginx labels: env: testspec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority 了解了定义及如何使用，那我们来看看代码层面是如何实现的吧！ 3. 抢占调度算法从上面的入口跳转: pkg/scheduler/scheduler.go:469 –&gt; pkg/scheduler/scheduler.go:290 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) &#123; preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil &#123; klog.Errorf("Error getting the updated preemptor pod object: %v", err) return "", err &#125; // 通过默认注册的抢占算法，计算得出最终被执行抢占调度的node、node上需要驱逐的pod等信息 node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) if err != nil &#123; klog.Errorf("Error preempting victims to make room for %v/%v.", preemptor.Namespace, preemptor.Name) return "", err &#125; var nodeName = "" if node != nil &#123; nodeName = node.Name // Update the scheduling queue with the nominated pod information. Without // this, there would be a race condition between the next scheduling cycle // and the time the scheduler receives a Pod Update for the nominated pod. // 给调度队列内的preemptor pod加上提名node信息，避免下一个调度周期出现冲突 sched.config.SchedulingQueue.UpdateNominatedPodForNode(preemptor, nodeName) // Make a call to update nominated node name of the pod on the API server. // 给待调度pod指定NominatedNodeName，pod.Status.NominatedNodeName = nodeName err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil &#123; klog.Errorf("Error in preemption process. Cannot update pod %v/%v annotations: %v", preemptor.Namespace, preemptor.Name, err) sched.config.SchedulingQueue.DeleteNominatedPodIfExists(preemptor) return "", err &#125; for _, victim := range victims &#123; // 对node上需要驱逐的pod执行删除操作 if err := sched.config.PodPreemptor.DeletePod(victim); err != nil &#123; klog.Errorf("Error preempting pod %v/%v: %v", victim.Namespace, victim.Name, err) return "", err &#125; sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, "Preempted", "by %v/%v on node %v", preemptor.Namespace, preemptor.Name, nodeName) &#125; metrics.PreemptionVictims.Set(float64(len(victims))) &#125; // Clearing nominated pods should happen outside of "if node != nil". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. // 当找不到合适的抢占node时，可能是因为preemptor pod已经有了提名的node，但它又执行了一遍抢占逻辑，说明它 // 在上一次调度周期中没有调度成功，因此，删除调度队列中比当前preemptor pod优先级更低的pod所指定的提名 // node信息(pod.Status.NominatedNodeName) for _, p := range nominatedPodsToClear &#123; rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil &#123; klog.Errorf("Cannot remove nominated node annotation of pod: %v", rErr) // We do not return as this error is not critical. &#125; &#125; return nodeName, err&#125; 如优先级筛选算法一样，调度算法最终也是要挑选出一个供以实际运行抢占调度逻辑的node，那么一起来看看这个计算算法是怎么样的。如schedule()方法一样，preempt()的默认方法也在generic_scheduler.go这个文件中： pkg/scheduler/core/generic_scheduler.go:288 将函数内拆成几个重要的部分，其余部分省略，逐个说明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) &#123; // ... 省略 // 每次开始抢占调度之前，检查一下当前pod是否已经有了提名抢占调度的节点，且该节点上当前不包含正在终结中的pod，若pod已有提名调度节点，且该节点上已经有pod正在终结中，则视为已经在执行抢占的动作了，所以不再往下重复执行。可以自行进去查看，比较简单，不拿出来讲了。 if !podEligibleToPreemptOthers(pod, g.nodeInfoSnapshot.NodeInfoMap) &#123; klog.V(5).Infof("Pod %v/%v is not eligible for more preemption.", pod.Namespace, pod.Name) return nil, nil, nil, nil &#125; // ... 省略 // potentialNodes，找出潜在的可能进行抢占调度的节点，下方详解 potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) // ... 省略 // pdb,pod Disruption Budget,是用来保障可用副本的一种功能，下方详解 pdbs, err := g.pdbLister.List(labels.Everything()) if err != nil &#123; return nil, nil, nil, err &#125; // 最重要的抢占算法入口，下方详解 nodeToVictims, err := selectNodesForPreemption(pod, g.nodeInfoSnapshot.NodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil &#123; return nil, nil, nil, err &#125; // ... 省略 // candidateNode，从所有提名的node中挑选一个真正执行抢占步骤 candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil &#123; return nil, nil, nil, nil &#125; // 返回3个值，分别是选中的node、node上将要驱逐的pod、调度队列中比当前pod优先级更低的pod nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.nodeInfoSnapshot.NodeInfoMap[candidateNode.Name]; ok &#123; return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, nil &#125; return nil, nil, nil, fmt.Errorf( "preemption failed: the target node %s has been deleted from scheduler cache", candidateNode.Name)&#125; 3.1. potentialNodes第一步，先找出所有潜在的可能会参与抢占调度的node,何为潜在可能呢？意思是node调度此pod调度失败的原因并非”硬伤”类原因。所谓硬伤原因，指的是即使驱逐调几个pod，也无法改变此node无法运行这个pod的事实。这些硬伤包括哪些？来看看代码： pkg/scheduler/core/generic_scheduler.go:306 -&gt; pkg/scheduler/core/generic_scheduler.go:1082 12345678910111213141516171819202122232425262728293031323334353637383940414243func nodesWherePreemptionMightHelp(nodes []*v1.Node, failedPredicatesMap FailedPredicateMap) []*v1.Node &#123; potentialNodes := []*v1.Node&#123;&#125; for _, node := range nodes &#123; unresolvableReasonExist := false failedPredicates, _ := failedPredicatesMap[node.Name] // If we assume that scheduler looks at all nodes and populates the failedPredicateMap // (which is the case today), the !found case should never happen, but we'd prefer // to rely less on such assumptions in the code when checking does not impose // significant overhead. // Also, we currently assume all failures returned by extender as resolvable. for _, failedPredicate := range failedPredicates &#123; switch failedPredicate &#123; case // 下面所有的failedPredicates，都视为"硬伤"，因此若相应的节点上若出现下面的失败原因之一，则视为该node不可参与抢占调度。 predicates.ErrNodeSelectorNotMatch, predicates.ErrPodAffinityRulesNotMatch, predicates.ErrPodNotMatchHostName, predicates.ErrTaintsTolerationsNotMatch, predicates.ErrNodeLabelPresenceViolated, // Node conditions won't change when scheduler simulates removal of preemption victims. // So, it is pointless to try nodes that have not been able to host the pod due to node // conditions. These include ErrNodeNotReady, ErrNodeUnderPIDPressure, ErrNodeUnderMemoryPressure, .... predicates.ErrNodeNotReady, predicates.ErrNodeNetworkUnavailable, predicates.ErrNodeUnderDiskPressure, predicates.ErrNodeUnderPIDPressure, predicates.ErrNodeUnderMemoryPressure, predicates.ErrNodeUnschedulable, predicates.ErrNodeUnknownCondition, predicates.ErrVolumeZoneConflict, predicates.ErrVolumeNodeConflict, predicates.ErrVolumeBindConflict: unresolvableReasonExist = true break &#125; &#125; if !unresolvableReasonExist &#123; klog.V(3).Infof("Node %v is a potential node for preemption.", node.Name) potentialNodes = append(potentialNodes, node) &#125; &#125; return potentialNodes&#125; 3.2. Pod Disruption Budget(pdb)这种资源类型本人没有实际应用过，查阅了一下官方的手册，实际上它也是kubernetes设计的一种抽象资源，主要用作面对主动中断时，保障副本可用数量的一种功能，与deployment的maxUnavailable不一样，maxUnavailable是在滚动更新(非主动中断)时用来保障，pdb通常是面对主动中断的场景，例如删除pod,drain node等主动操作，更多详细说明参考官方的手册: Specifying a Disruption Budget for your Application 资源实例: 123456789apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: minAvailable: 2 selector: matchLabels: app: zookeeper 1234567891011121314151617181920$ kubectl get poddisruptionbudgetsNAME MIN-AVAILABLE ALLOWED-DISRUPTIONS AGEzk-pdb 2 1 7s$ kubectl get poddisruptionbudgets zk-pdb -o yamlapiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: creationTimestamp: 2017-08-28T02:38:26Z generation: 1 name: zk-pdb...status: currentHealthy: 3 desiredHealthy: 3 disruptedPods: null disruptionsAllowed: 1 expectedPods: 3 observedGeneration: 1 为什么这个资源相关的逻辑会出现在抢占调度里面呢？因为设计者将pod抢占造成的低优先级pod驱逐动作视为主动中断，有了这一层理解，我们接着往下。 3.3. nodeToVictimsselectNodesForPreemption()函数很重要，这个函数将会返回所有可行的node驱逐方案 pkg/scheduler/core/generic_scheduler.go:316 selectNodesForPreemption –&gt; pkg/scheduler/core/generic_scheduler.go:916 123456789101112131415161718192021222324252627282930313233343536func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, potentialNodes []*v1.Node, fitPredicates map[string]predicates.FitPredicate, metadataProducer predicates.PredicateMetadataProducer, queue internalqueue.SchedulingQueue, pdbs []*policy.PodDisruptionBudget,) (map[*v1.Node]*schedulerapi.Victims, error) &#123; // 返回的结构体，类型是map，key是*v1.Node，value是一个结构体，包含两个元素:node上待驱逐的pod信息和将会违反PDB规则的次数 nodeToVictims := map[*v1.Node]*schedulerapi.Victims&#123;&#125; var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) checkNode := func(i int) &#123; nodeName := potentialNodes[i].Name var metaCopy predicates.PredicateMetadata if meta != nil &#123; metaCopy = meta.ShallowCopy() &#125; // selectVictimsOnNode()是核心计算的函数 pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], fitPredicates, queue, pdbs) if fits &#123; resultLock.Lock() victims := schedulerapi.Victims&#123; Pods: pods, NumPDBViolations: numPDBViolations, &#125; nodeToVictims[potentialNodes[i]] = &amp;victims resultLock.Unlock() &#125; &#125; // 熟悉的并发控制模型 workqueue.ParallelizeUntil(context.TODO(), 16, len(potentialNodes), checkNode) return nodeToVictims, nil&#125; 上面已在代码中对重要部分进行注释，不难发现，重要的计算函数是selectVictimsOnNode()函数，每个node所需要驱逐的pod，以及违反PDB规则次数信息，都由此函数来计算返回，最终组成nodeToVictims这个map，返回给上层调用函数。所以，接着来看selectVictimsOnNode()函数是怎么运行的。 selectVictimsOnNode 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879func selectVictimsOnNode( pod *v1.Pod, meta predicates.PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo, fitPredicates map[string]predicates.FitPredicate, queue internalqueue.SchedulingQueue, pdbs []*policy.PodDisruptionBudget,) ([]*v1.Pod, int, bool) &#123; if nodeInfo == nil &#123; return nil, 0, false &#125; // 潜在的受害者(pod)，按优先级排序的有序list，高优先级的排序靠前，低优先级的排序靠后 potentialVictims := util.SortableList&#123;CompFunc: util.HigherPriorityPod&#125; // 在实际调度之前，所有的资源的考量计算都只能是预估，因此不能实际实施到node上，所以，基于node的元数据进行一个复制，将node信息的复制样本来参与计算，最终计算得到正确的结果后才会考虑实际往node上实施。 nodeInfoCopy := nodeInfo.Clone() // 基于node复制样本，假设减去一个pod之后，复制样本重新计算得到的数据。例如node a上运行着有若干pod，假设减去了其上的pod1，pod1 request的内存是4Gi，那么可假设node可分配的内存就多了4Gi removePod := func(rp *v1.Pod) &#123; nodeInfoCopy.RemovePod(rp) if meta != nil &#123; meta.RemovePod(rp) &#125; &#125; // 基于node复制样本，假设加上一个pod之后，复制样本重新计算得到的数据。 addPod := func(ap *v1.Pod) &#123; nodeInfoCopy.AddPod(ap) if meta != nil &#123; meta.AddPod(ap, nodeInfoCopy) &#125; &#125; // 首先，枚举出node上所有的低于待调度pod优先级的pod，并将它们加入潜在受害者potentialVictims，计算假设剔出它们后，node上现有的资源信息 podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() &#123; if util.GetPodPriority(p) &lt; podPriority &#123; potentialVictims.Items = append(potentialVictims.Items, p) removePod(p) &#125; &#125; potentialVictims.Sort() // 第二步，判断待调度pod是否fit此node，主要是亲和性方面的考量，这个podFitsOnNode函数前面筛选算法已经讲过了，这里不再复述，这个函数通过后，会把待调度pod的request资源加入nodeInfoCopy内。 if fits, _, err := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, queue, false); !fits &#123; if err != nil &#123; klog.Warningf("Encountered error while selecting victims on node %v: %v", nodeInfo.Node().Name, err) &#125; return nil, 0, false &#125; var victims []*v1.Pod numViolatingVictim := 0 // 第三步，将前面枚举出的低优先级的pod有序list，拆分为两个有序list，一个是违反了PDB规则的(pdb.Status.PodDisruptionsAllowed &lt;= 0,这个值等于0则代表理论上要求不能出现中断的pod副本)，一个是不违反PDB规则的。 violatingVictims, nonViolatingVictims := filterPodsWithPDBViolation(potentialVictims.Items, pdbs) // 第四步，前面枚举假设把所有的低优先级pod都剔除了，但实际上可能不用剔除这么多，因此，保证了待调度pod计算进来之后，这里再用贪心法将低优先级的pod按优先级排序尽可能多地加入回来，最终无法调度的pod，才归为实际驱逐的pod。显而易见的是，优先保障有PDB约束的pod。 reprievePod := func(p *v1.Pod) bool &#123; addPod(p) fits, _, _ := podFitsOnNode(pod, meta, nodeInfoCopy, fitPredicates, queue, false) if !fits &#123; removePod(p) victims = append(victims, p) klog.V(5).Infof("Pod %v/%v is a potential preemption victim on node %v.", p.Namespace, p.Name, nodeInfo.Node().Name) &#125; return fits &#125; for _, p := range violatingVictims &#123; if !reprievePod(p) &#123; numViolatingVictim++ &#125; &#125; // Now we try to reprieve non-violating victims. for _, p := range nonViolatingVictims &#123; reprievePod(p) &#125; // 第五步，返回最终node的运算结果，分别是驱逐的pod list，以及驱逐的数量 return victims, numViolatingVictim, true&#125; 这个函数分5步，先是枚举出所有的低优先级pod，再贪心保障尽量多的pod能正常运行，从而计算出最终需要被驱逐的pod及相关信息，详见代码内注释。 3.4. candidateNode上面函数返回每一个可抢占的node各自的抢占方案后，这里就需要筛选其中一个node来实际执行抢占调度操作。 pkg/scheduler/core/generic_scheduler.go:330 pickOneNodeForPreemption() –&gt; pkg/scheduler/core/generic_scheduler.go:809 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node &#123; if len(nodesToVictims) == 0 &#123; return nil &#125; minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 for node, victims := range nodesToVictims &#123; if len(victims.Pods) == 0 &#123; // 可能在调度的过程中，有极小的概率某个node上有pod终结了，使node上不再有需要驱逐的pod，那么pod可直接调度到该node上 return node &#125; // 按违反PDB约束的次数排序，越少的node优先级越高，若最大优先级的node只有一个，则直接返回违反次数最小的node，若有多个，则进入下一步筛选 numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods &lt; minNumPDBViolatingPods &#123; minNumPDBViolatingPods = numPDBViolatingPods minNodes1 = nil lenNodes1 = 0 &#125; if numPDBViolatingPods == minNumPDBViolatingPods &#123; minNodes1 = append(minNodes1, node) lenNodes1++ &#125; &#125; if lenNodes1 == 1 &#123; return minNodes1[0] &#125; // 按node上需驱逐的第一个pod(即需驱逐的优先级最高的pod)的优先级大小排序，pod[0]优先级越小，则所属的node优先级越高，若最大优先级的node只有一个，则直接返回此node，若有多个，则进入下一步筛选 minHighestPriority := int32(math.MaxInt32) var minNodes2 = make([]*v1.Node, lenNodes1) lenNodes2 := 0 for i := 0; i &lt; lenNodes1; i++ &#123; node := minNodes1[i] victims := nodesToVictims[node] // highestPodPriority is the highest priority among the victims on this node. highestPodPriority := util.GetPodPriority(victims.Pods[0]) if highestPodPriority &lt; minHighestPriority &#123; minHighestPriority = highestPodPriority lenNodes2 = 0 &#125; if highestPodPriority == minHighestPriority &#123; minNodes2[lenNodes2] = node lenNodes2++ &#125; &#125; if lenNodes2 == 1 &#123; return minNodes2[0] &#125; // 按node上需驱逐的所有的pod的优先级总和计算，总和越小，node优先级越高，若最大优先级的node只有一个，则直接返回此node，若有多个，则进入下一步筛选 minSumPriorities := int64(math.MaxInt64) lenNodes1 = 0 for i := 0; i &lt; lenNodes2; i++ &#123; var sumPriorities int64 node := minNodes2[i] for _, pod := range nodesToVictims[node].Pods &#123; // We add MaxInt32+1 to all priorities to make all of them &gt;= 0. This is // needed so that a node with a few pods with negative priority is not // picked over a node with a smaller number of pods with the same negative // priority (and similar scenarios). sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) &#125; if sumPriorities &lt; minSumPriorities &#123; minSumPriorities = sumPriorities lenNodes1 = 0 &#125; if sumPriorities == minSumPriorities &#123; minNodes1[lenNodes1] = node lenNodes1++ &#125; &#125; if lenNodes1 == 1 &#123; return minNodes1[0] &#125; // 按node上需驱逐的所有的pod数量计算，数量越少，node优先级越高，若最大优先级的node只有一个，则直接返回此node，若有多个，则进入下一步筛选 minNumPods := math.MaxInt32 lenNodes2 = 0 for i := 0; i &lt; lenNodes1; i++ &#123; node := minNodes1[i] numPods := len(nodesToVictims[node].Pods) if numPods &lt; minNumPods &#123; minNumPods = numPods lenNodes2 = 0 &#125; if numPods == minNumPods &#123; minNodes2[lenNodes2] = node lenNodes2++ &#125; &#125; // 若经过上面四个步骤的筛选，筛选出的node还是不止一个，那么就挑选其中的第一个作为最后选中被执行抢占调度的node if lenNodes2 &gt; 0 &#123; return minNodes2[0] &#125; klog.Errorf("Error in logic of node scoring for preemption. We should never reach here!") return nil&#125; 上面代码结合注释，可以归纳出，这个函数中做了非常细致地检查，最高分如下4个步骤来对node进行优先级排序，筛选出一个最终合适的node来被执行抢占调度pod的操作： 1.按违反PDB约束的次数排序 2.按node上需驱逐的第一个pod(即需驱逐的优先级最高的pod)的优先级大小排序 3.按node上需驱逐的所有的pod的优先级总和计算排序 4.按node上需驱逐的所有的pod数量计算排序 5.若经过上面四个步骤的筛选，筛选出的node还是不止一个，那么就挑选其中的第一个作为最后选中node 4. 总结抢占调度的逻辑可以说是非常细致和精彩，例如 1.从资源计算的角度： 基于nodeInfo快照的计算，所有计算在最终确定实施之前都是预计算 先枚举出所有低优先级的pod，保障待调度pod能充分获取资源 在待调度pod能运行后，再尽力保障最多的低优先级pod能同时运行 2.从node选取的角度： 分4个步骤筛选以选出驱逐造成影响最小一个node 本章完，感谢阅读！]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-P4-Node优先级算法]]></title>
    <url>%2F2019%2F08%2F22%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-P4-Node%E4%BC%98%E5%85%88%E7%BA%A7%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[P4-Node优先级算法前言在上一篇文档中，我们过了一遍node筛选算法： p3-Node筛选算法 按调度规则设计，对筛选出的node，选择优先级最高的作为最终的fit node。那么本篇承接上一篇，进入下一步，看一看node优先级排序的过程。 Tips: 本篇篇幅较长，因调度优选算法较为复杂，但请耐心结合本篇阅读源码，多看几次，一定会有收获。 正文1. 优先级函数1.1 优先级函数入口同上一篇，回到pkg/scheduler/core/generic_scheduler.go中的Schedule()函数，pkg/scheduler/core/generic_scheduler.go:184: 截图中有几处标注，metric相关的几行，是收集metric信息，用以提供给prometheus使用的，kubernetes的几个核心组件都有这个功能，以后如果读prometheus的源码，这个单独拎出来再讲。直接进入优先级函数PrioritizeNodes()内部pkg/scheduler/core/generic_scheduler.go:215 1.2 优先级函数概括说明pkg/scheduler/core/generic_scheduler.go:645 PrioritizeNodes()，代码块较长，就不贴了. 在此函数上方的注释可以得知，这个函数的工作逻辑： 1.列出所有的优先级计算维度的方法，每个维度的方法返回该维度的得分，每个维度都有内部定义的weight权重，以及得分score，score取值范围在[0-10之间]，该维度的最终得分为 (score * weight)，得分越高越好 2.列出所有参与运算的node 3.循环对每一个node分别进行1中所有维度方法项计算，最后将该node的所有计算维度得分汇总 这里有一个重要的结构体始终贯穿整个函数栈，特别指出: 1234567 // HostPriority represents the priority of scheduling to a particular host, higher priority is better.type HostPriority struct &#123; // Name of the host Host string // Score associated with the host Score int&#125; 两个重要变量 123456789101112131415161718// pkg/scheduler/core/generic_scheduler.go:678// 注意，这里的results是个双层array的结构，统计的是各维度各node的分别得分，即[][]HostPriority类型，用伪代码抽象一下:/*result = [// 维度1,各node的得分[&#123;node-a: 1&#125;,&#123;node-b: 2&#125;,&#123;node-c: 3&#125;...],// 维度2,各node的得分[&#123;node-a: 3&#125;,&#123;node-b: 1&#125;,&#123;node-c: 2&#125;...],...]*/ results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) // pkg/scheduler/core/generic_scheduler.go:738 // 这里的result是[]HostPriority类型，即汇总所有维度之后每个node的最终得分 result := make(schedulerapi.HostPriorityList, 0, len(nodes)) 1.3 优先级函数分段说明1.3.1 Function(DEPRECATED)pkg/scheduler/core/generic_scheduler.go:682 12345678910111213141516171819 // DEPRECATED: we can remove this when all priorityConfigs implement the// Map-Reduce pattern.for i := range priorityConfigs &#123; if priorityConfigs[i].Function != nil &#123; wg.Add(1) go func(index int) &#123; defer wg.Done() var err error results[index], err = priorityConfigs[index].Function(pod, nodeNameToInfo, nodes) if err != nil &#123; appendError(err) &#125; &#125;(i) &#125; else &#123; results[i] = make(schedulerapi.HostPriorityList, len(nodes)) &#125;&#125; 注释中说明这种直接计算方法(priorityConfigs[i].Function)是传统模式，已经DEPRECATED掉了，当前版本实际上只有一个维度(pod亲和性)采取了这种方法，取而代之的是Map-Reduce模式的计算方法,参见后方。Function运算的方式，随后会以pod亲和性这个维度的实例代码来说明。 1.3.2 Map-Reduce Functionpkg/scheduler/core/generic_scheduler.go:698 1234567891011121314151617181920212223242526272829303132333435workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) &#123; nodeInfo := nodeNameToInfo[nodes[index].Name] for i := range priorityConfigs &#123; if priorityConfigs[i].Function != nil &#123; continue &#125; var err error results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil &#123; appendError(err) results[i][index].Host = nodes[index].Name &#125; &#125;&#125;)for i := range priorityConfigs &#123; if priorityConfigs[i].Reduce == nil &#123; continue &#125; wg.Add(1) go func(index int) &#123; defer wg.Done() if err := priorityConfigs[index].Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil &#123; appendError(err) &#125; if klog.V(10) &#123; for _, hostPriority := range results[index] &#123; klog.Infof("%v -&gt; %v: %v, Score: (%d)", util.GetPodFullName(pod), hostPriority.Host, priorityConfigs[index].Name, hostPriority.Score) &#125; &#125; &#125;(i)&#125;// Wait for all computations to be finished.wg.Wait() 这里可以看出，若该维度未直接指定priorityConfigs[i].Function，则采取Map-Reduce模式. 12引申：Map-Reduce是大数据里的思想，简单来说Map函数是对一组元素集上的每一个元素进行高度并行的运算，得到与元素集对应(mapping关系)的结果集，Reduce函数则对结果集进行归纳运算而后返回需要的结果。 这里再次出现了上一篇中特别提到的workqueue.ParallelizeUntil()并行运算控制方法，同样以node为粒度，运行Map函数；而下方并行度不高的Reduce函数，则使用的sync模块才实现并发控制。符合Map-Reduce的思想。 没接触过Map-Reduce，但先不要被吓住，这里只是利用了这个思想，数据量并没有复杂到要拆分给多台机器分布式运算的级别。随后举一个使用Map-Reduce计算方法的维度的实例代码来说明。 2. 优先级计算维度2.1 默认注册的计算维度通过上面的内容，对优先级算法有了一个模糊的认知：统计节点的各计算维度得分的总和，分数越高优先级越高。那么默认的优先级计算维度分别有哪些呢？在前面的scheduler-框架篇中有讲过，调度算法全部位于pkg/scheduler/algorithm目录中，而pkg/scheduler/algorithmprovider内提供以工厂模式创建调度算法相关元素的方法，所以，我们直接来到pkg/scheduler/algorithmprovider/defaults/register_priorities.go文件内，所有默认的优先级计算维度的算法都在这里注册，篇幅有限，随便列举其中几个: 12345678factory.RegisterPriorityFunction2(priorities.EqualPriority, core.EqualPriorityMap, nil, 1)// Optional, cluster-autoscaler friendly priority function - give used nodes higher priority.factory.RegisterPriorityFunction2(priorities.MostRequestedPriority, priorities.MostRequestedPriorityMap, nil, 1)factory.RegisterPriorityFunction2( priorities.RequestedToCapacityRatioPriority, priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) 如果仔细看代码里的注释可以发现，个别factory函数虽然已经将计算维度注册，但实际上默认并没有启用它，例如ServiceSpreadingPriority这一项中的注释表明，它已经相当大程度被SelectorSpreadPriority取代了，保留它是为了兼容此前的版本。那么默认使用的计算维度有哪些呢？ 2.2 默认使用的计算维度默认使用的计算维度，在这个地方声明: pkg/scheduler/algorithmprovider/defaults/defaults.go:108 123456789101112func defaultPriorities() sets.String &#123; return sets.NewString( priorities.SelectorSpreadPriority, priorities.InterPodAffinityPriority, priorities.LeastRequestedPriority, priorities.BalancedResourceAllocation, priorities.NodePreferAvoidPodsPriority, priorities.NodeAffinityPriority, priorities.TaintTolerationPriority, priorities.ImageLocalityPriority, )&#125; 2.3 新旧两种计算方式在注册的每一个计算维度，都有专属的维度描述关键字，即factory方法的第一个参数(str类型)。不难发现，这里的每一个关键字，pkg/scheduler/algorithm/priorities目录内都有与其对应的文件,图中圈出了几个例子(灵魂画笔请原谅): 显而易见，维度计算的内容就在这些文件中，可以自行通过编辑器的跳转功能逐级查看进行验证. 通过这是factory方法可以看出，所有维度，默认的注册权重都是1，除了NodePreferAvoidPodsPriority这一项之外，它的weight值是10000，这一项是为了避免pod调度到node上，我们找到文件查看该方法的注释: pkg/scheduler/algorithm/priorities/node_prefer_avoid_pods.go:31 12345// CalculateNodePreferAvoidPodsPriorityMap priorities nodes according to the node annotation// "scheduler.alpha.kubernetes.io/preferAvoidPods".func CalculateNodePreferAvoidPodsPriorityMap(pod *v1.Pod, meta interface&#123;&#125;, nodeInfo *schedulernodeinfo.NodeInfo) (schedulerapi.HostPriority, error) &#123;... // 省略&#125; 得知node可以通过annotation添加scheduler.alpha.kubernetes.io/preferAvoidPods指定来避免指定的pod调度到本身之上，因此此项优先级超高覆盖过其他的各计算维度。 如果ctrl + F 过滤一下map关键字，你会发现，仅有InterPodAffinityPriority这一项是没有map关键字的： 1234567891011// pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.)// as some other pods, or, conversely, should not be placed in the same topological domain as some other pods.factory.RegisterPriorityConfigFactory( priorities.InterPodAffinityPriority, factory.PriorityConfigFactory&#123; Function: func(args factory.PluginFactoryArgs) priorities.PriorityFunction &#123; return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) &#125;, Weight: 1, &#125;,) 这也印证了前面说的当前仅剩pod亲和性这一个维度在使用传统的Function,虽然已经被DEPRECATED掉了，传统的Function是直接计算出结果，Map-Reduce是将这个过程解耦拆成了两个步骤，且我们可以看到所有的factory函数，很多形参reduceFunction接收到的实参实际是是nil: 这就说明这些维度的计算工作在map函数里面已经执行完成了，不需要再执行reduce函数了。因此，传统的Function的计算过程同样值得参考，那么首先就来看看InterPodAffinityPriority维度是怎么计算的吧! 3. 传统计算Function3.1 InterPodAffinityPriority看代码之前，先来看一个标准的PodAffinity配置示例： PodAffinity示例： 12345678910111213141516171819202122232425262728293031323334apiVersion: v1kind: Podmetadata: name: pod-a namespace: defaultspec: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: weight: 100 labelSelector: matchExpressions: - key: like operator: In values: - pod-b # 拓扑层级，大多数是node层级，但其实还有zone/region等层级 topologyKey: kubernetes.io/hostname podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: unlike operator: In values: - pod-c topologyKey: kubernetes.io/hostname containers: - name: test image: gcr.io/google_containers/pause:2.0 yaml中的申明意图是: pod-a亲近pod-b，疏远pod-c，所以在这项计算维度里，如果node上运行着pod-b ,则该node加分，如果该node上运行着pod-c，则node减分。 来看代码，仔细读代码，你会发现示例中的几个层级的key: PreferredDuringSchedulingIgnoredDuringExecution,podAffinityTerm,labelSelector,topologyKey在代码中都会出现： pkg/scheduler/algorithm/priorities/interpod_affinity.go:119: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697func (ipa *InterPodAffinity) CalculateInterPodAffinityPriority(pod *v1.Pod, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, nodes []*v1.Node) (schedulerapi.HostPriorityList, error) &#123; affinity := pod.Spec.Affinity // 判断待调度pod是否存在亲和性约束 hasAffinityConstraints := affinity != nil &amp;&amp; affinity.PodAffinity != nil // 判断待调度是否pod存在反亲和性约束 hasAntiAffinityConstraints := affinity != nil &amp;&amp; affinity.PodAntiAffinity != nil ... // 省略 // 根据node上正在运行的pod来计算node得分的函数，分为两个层面计算，两个层面都可以加减分: // 1.待调度pod与现存pod的亲和性(软亲和性，因为待调度pod还未实际运行起来) // 2.现存pod与待调度pod的亲和性(硬亲和性，因为待调度pod正在运行) // 加减分操作由processTerm()方法进行计分，这个下面再讲 // 这里是pod级别，被下方node级别的processNode调用 processPod := func(existingPod *v1.Pod) error &#123; existingPodNode, err := ipa.info.GetNodeInfo(existingPod.Spec.NodeName) if err != nil &#123; if apierrors.IsNotFound(err) &#123; klog.Errorf("Node not found, %v", existingPod.Spec.NodeName) return nil &#125; return err &#125; existingPodAffinity := existingPod.Spec.Affinity // 判断node上正在运行的pod是否与待调度的pod存在亲和性约束 existingHasAffinityConstraints := existingPodAffinity != nil &amp;&amp; existingPodAffinity.PodAffinity != nil // 判断node上正在运行的pod是否与待调度的pod存在反亲和性约束 existingHasAntiAffinityConstraints := existingPodAffinity != nil &amp;&amp; existingPodAffinity.PodAntiAffinity != nil if hasAffinityConstraints &#123; terms := affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, 1) &#125; if hasAntiAffinityConstraints &#123; terms := affinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, pod, existingPod, existingPodNode, -1) &#125; if existingHasAffinityConstraints &#123; if ipa.hardPodAffinityWeight &gt; 0 &#123; terms := existingPodAffinity.PodAffinity.RequiredDuringSchedulingIgnoredDuringExecution for _, term := range terms &#123; pm.processTerm(&amp;term, existingPod, pod, existingPodNode, float64(ipa.hardPodAffinityWeight)) &#125; &#125; terms := existingPodAffinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, 1) &#125; if existingHasAntiAffinityConstraints &#123; terms := existingPodAffinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution pm.processTerms(terms, existingPod, pod, existingPodNode, -1) &#125; return nil &#125; // 这里是node级别的，调用上方的processPod,被下方的并发控制函数调用，内部逻辑分支有两支: // 1.pod指定了亲和性约束，那么node上每个现存的pod都要与待调度pod进行硬、软亲和性计算 // 2.pod未指定亲和性约束，那么仅需要对node上现存的已指定亲和性约束的pod，与待调度pod进行硬亲和性计算 processNode := func(i int) &#123; nodeInfo := nodeNameToInfo[allNodeNames[i]] if nodeInfo.Node() != nil &#123; if hasAffinityConstraints || hasAntiAffinityConstraints &#123; for _, existingPod := range nodeInfo.Pods() &#123; if err := processPod(existingPod); err != nil &#123; pm.setError(err) &#125; &#125; &#125; else &#123; for _, existingPod := range nodeInfo.PodsWithAffinity() &#123; if err := processPod(existingPod); err != nil &#123; pm.setError(err) &#125; &#125; &#125; &#125; &#125; // node级别并发 workqueue.ParallelizeUntil(context.TODO(), 16, len(allNodeNames), processNode) ... // 省略 // 计算此Pod亲和性维度的各node的得分 result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for _, node := range nodes &#123; fScore := float64(0) if (maxCount - minCount) &gt; 0 &#123; // 分母是maxCount - minCount,不直接使用maxCount做分母是因为maxCount可能为0，通过整除运算，控制node的最高得分为MaxPriority(默认10),最低位0 fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) &#125; result = append(result, schedulerapi.HostPriority&#123;Host: node.Name, Score: int(fScore)&#125;) if klog.V(10) &#123; klog.Infof("%v -&gt; %v: InterPodAffinityPriority, Score: (%d)", pod.Name, node.Name, int(fScore)) &#125; &#125; return result, nil&#125; 上面代码中的注释已经将CalculateInterPodAffinityPriority这个函数的工作模式介绍的比较清晰了，那么再看一看计分函数processTerm()： pkg/scheduler/algorithm/priorities/interpod_affinity.go:107 –&gt; pkg/scheduler/algorithm/priorities/interpod_affinity.go:86 123456789101112131415161718192021222324func (p *podAffinityPriorityMap) processTerm(term *v1.PodAffinityTerm, podDefiningAffinityTerm, podToCheck *v1.Pod, fixedNode *v1.Node, weight float64) &#123; namespaces := priorityutil.GetNamespacesFromPodAffinityTerm(podDefiningAffinityTerm, term) selector, err := metav1.LabelSelectorAsSelector(term.LabelSelector) if err != nil &#123; p.setError(err) return &#125; // 待调度pod和被检查pod存在亲和性则匹配,匹配且node与指定的term处于同一拓扑层级，则node加分 match := priorityutil.PodMatchesTermsNamespaceAndSelector(podToCheck, namespaces, selector) if match &#123; func() &#123; p.Lock() defer p.Unlock() for _, node := range p.nodes &#123; // TopologyKey是拓扑逻辑层级，上面例子中的是kubernetes.io/hostname，kuernetes内建了几个层级 // 如failure-domain.beta.kubernetes.io/zone，kubernetes.io/hostname等，参考: // https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity if priorityutil.NodesHaveSameTopologyKey(node, fixedNode, term.TopologyKey) &#123; p.counts[node.Name] += weight &#125; &#125; &#125;() &#125;&#125; podAffinityPriority这个维度的算法到此就明了了 4. Map-Reduce计算方法在pkg/scheduler/algorithmprovider/defaults/register_priorities.go:26中的init()函数内，找出所有在注册且默认被使用的，同时包含map方法和reduce方法的factory函数，一共有3个，我们挑其中之一为例作启发，其余的就不写在文章里了，可以自行阅读: 1234567891011121314151617 // pkg/scheduler/algorithmprovider/defaults/register_priorities.go:58// spreads pods by minimizing the number of pods (belonging to the same service or replication controller) on the same node.factory.RegisterPriorityConfigFactory( priorities.SelectorSpreadPriority, factory.PriorityConfigFactory&#123; MapReduceFunction: func(args factory.PluginFactoryArgs) (priorities.PriorityMapFunction, priorities.PriorityReduceFunction) &#123; return priorities.NewSelectorSpreadPriority(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) &#125;, Weight: 1, &#125;,)// pkg/scheduler/algorithmprovider/defaults/register_priorities.go:90 factory.RegisterPriorityFunction2(priorities.NodeAffinityPriority, priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1) // pkg/scheduler/algorithmprovider/defaults/register_priorities.go:93 factory.RegisterPriorityFunction2(priorities.TaintTolerationPriority, priorities.ComputeTaintTolerationPriorityMap, priorities.ComputeTaintTolerationPriorityReduce, 1) 那就以第一个ServiceSpreadingPriority维度为例吧，名字直译为: 选择器均分优先级，注释中可以得知，这一项是为了保障属于同一个Service或replication controller的的pod，尽量分散开在不同的node里，保障高可用。 NewSelectorSpreadPriority()方法用来注册此维度的Map和Reduce函数，来看看其内容： pkg/scheduler/algorithmprovider/defaults/register_priorities.go:62 NewSelectorSpreadPriority()—-&gt; pkg/scheduler/algorithm/priorities/selector_spreading.go:45 12345678910111213func NewSelectorSpreadPriority( serviceLister algorithm.ServiceLister, controllerLister algorithm.ControllerLister, replicaSetLister algorithm.ReplicaSetLister, statefulSetLister algorithm.StatefulSetLister) (PriorityMapFunction, PriorityReduceFunction) &#123; selectorSpread := &amp;SelectorSpread&#123; serviceLister: serviceLister, controllerLister: controllerLister, replicaSetLister: replicaSetLister, statefulSetLister: statefulSetLister, &#125; return selectorSpread.CalculateSpreadPriorityMap, selectorSpread.CalculateSpreadPriorityReduce&#125; 注意这4个参数:serviceLister/replicaSetLister/statefulSetLister/controllerLister,与pod相关的四个上层抽象概念Service/RC/RS/StatefulSet都列出来了，返回的map函数是CalculateSpreadPriorityMap,reduce函数是CalculateSpreadPriorityReduce,分别看一看他们吧 4.1 Map函数pkg/scheduler/algorithm/priorities/selector_spreading.go:66 12345678910111213141516171819202122232425262728func (s *SelectorSpread) CalculateSpreadPriorityMap(pod *v1.Pod, meta interface&#123;&#125;, nodeInfo *schedulernodeinfo.NodeInfo) (schedulerapi.HostPriority, error) &#123; var selectors []labels.Selector node := nodeInfo.Node() if node == nil &#123; return schedulerapi.HostPriority&#123;&#125;, fmt.Errorf("node not found") &#125; priorityMeta, ok := meta.(*priorityMetadata) if ok &#123; selectors = priorityMeta.podSelectors &#125; else &#123; selectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) &#125; if len(selectors) == 0 &#123; return schedulerapi.HostPriority&#123; Host: node.Name, Score: int(0), &#125;, nil &#125; count := countMatchingPods(pod.Namespace, selectors, nodeInfo) return schedulerapi.HostPriority&#123; Host: node.Name, Score: count, &#125;, nil&#125; 继续看countMatchingPods函数: pkg/scheduler/algorithm/priorities/selector_spreading.go:187: 1234567891011121314151617181920212223func countMatchingPods(namespace string, selectors []labels.Selector, nodeInfo *schedulernodeinfo.NodeInfo) int &#123; if nodeInfo.Pods() == nil || len(nodeInfo.Pods()) == 0 || len(selectors) == 0 &#123; return 0 &#125; count := 0 for _, pod := range nodeInfo.Pods() &#123; // Ignore pods being deleted for spreading purposes // Similar to how it is done for SelectorSpreadPriority if namespace == pod.Namespace &amp;&amp; pod.DeletionTimestamp == nil &#123; matches := true for _, selector := range selectors &#123; if !selector.Matches(labels.Set(pod.Labels)) &#123; matches = false break &#125; &#125; if matches &#123; count++ &#125; &#125; &#125; return count&#125; 这里的计算方式概括一下: 已知Service/RC/RS/StatefulSet这四种对pod进行管理的抽象高层级资源(后面统称高层级资源)，选择器都是通过label来匹配pod的，因此，这里将待调度pod的高层级资源的selector选择器依次列出，与node上现运行的pod中的每一个进行依次比较，每出现一次待调度pod的selector，命中了某个现运行pod的标签的情况，则视为匹配成功，加1分，未命中则不加分(这里的分数越高代表匹配到的现运行pod数量越多，则最终优先级得分应该越低，待会儿在reduce函数里我们可以印证)。 举个例子: 假设待调度的为pod-a-1，node-a,node-b上现都运行有若干个pod node-a其中有1个pod-a-2与pod-a-1属于同一个Service，那么，node-a的count计数为1； node-b中没有pod被pod-a-1的selector命中，则node-b的count计数为0 计数越多，则对应的最终优先级得分应该越低，因此node-b的得分会比node-a高 map函数到这里就结束了，但这个计数显然还不能作为节点在此维度的最终得分，因此，下面还有reduce函数 4.1 Reduce函数基于前面map函数得出的各node的匹配次数count计数，来展开reduce函数运算: pkg/scheduler/algorithm/priorities/selector_spreading.go:99 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func (s *SelectorSpread) CalculateSpreadPriorityReduce(pod *v1.Pod, meta interface&#123;&#125;, nodeNameToInfo map[string]*schedulernodeinfo.NodeInfo, result schedulerapi.HostPriorityList) error &#123; countsByZone := make(map[string]int, 10) maxCountByZone := int(0) maxCountByNodeName := int(0) for i := range result &#123; if result[i].Score &gt; maxCountByNodeName &#123; maxCountByNodeName = result[i].Score &#125; zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == "" &#123; continue &#125; countsByZone[zoneID] += result[i].Score &#125; for zoneID := range countsByZone &#123; if countsByZone[zoneID] &gt; maxCountByZone &#123; maxCountByZone = countsByZone[zoneID] &#125; &#125; haveZones := len(countsByZone) != 0 maxCountByNodeNameFloat64 := float64(maxCountByNodeName) maxCountByZoneFloat64 := float64(maxCountByZone) MaxPriorityFloat64 := float64(schedulerapi.MaxPriority) for i := range result &#123; // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName &gt; 0 &#123; // 匹配数量最多的node，count=maxCountByNodeName，fScore得分为0 // 匹配数量最少的node，假设count=0，则fScore得分为10 fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) &#125; // If there is zone information present, incorporate it if haveZones &#123; zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID != "" &#123; zoneScore := MaxPriorityFloat64 if maxCountByZone &gt; 0 &#123; zoneScore = MaxPriorityFloat64 * (float64(maxCountByZone-countsByZone[zoneID]) / maxCountByZoneFloat64) &#125; // 这里将zone层级参与了运算，zoneWeighting=2/3，则nodeWeight取1/3，混合计算最终得分 fScore = (fScore * (1.0 - zoneWeighting)) + (zoneWeighting * zoneScore) &#125; &#125; result[i].Score = int(fScore) if klog.V(10) &#123; klog.Infof( "%v -&gt; %v: SelectorSpreadPriority, Score: (%d)", pod.Name, result[i].Host, int(fScore), ) &#125; &#125; return nil&#125; 不难发现，这里的Reduce函数统计得分的方式，与传统Function最后一步统计最终得分，步骤可以说是一致的: 12// PodAffinityPriority统计最终得分fScore = float64(schedulerapi.MaxPriority) * ((pm.counts[node.Name] - minCount) / (maxCount - minCount)) 只不过这里是使用Map-Reduce风格思想将其步骤解耦为了两步。Reduce函数介绍到此结束 总结优先级算法相对而言比predicate断言算法要复杂一些，并且在当前版本的维度计算中存在传统Function函数与Map-Reduce风格函数混用的现象，一定程度上提高了阅读的难度，但相信仔细重复阅读代码，还是不难理解的，毕竟数据量还未到达大数据的级别，只是利用了其映射归纳的思想，解耦的同时提高一定的并发性能。 下一篇讲什么呢？我再研究研究，have fun!]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo图片展示: blog图床迁移至七牛云]]></title>
    <url>%2F2019%2F08%2F20%2Fhexo%E5%9B%BE%E7%89%87-blog%E5%9B%BE%E5%BA%8A%E8%BF%81%E7%A7%BB%E8%87%B3%E4%B8%83%E7%89%9B%E4%BA%91.html</url>
    <content type="text"><![CDATA[前言上一篇中，搭建好了hexo next主题的博客，将cxxn中的所有博客为md文档后再全部上传到hexo站内，但是有一个很大的问题：hexo本地图片无法显示，需要安装插件，然后以非md外链的格式插入图片，这使用起来相当不便且不通用，因此放弃插入本地图片的方案。导出的md中的图片外链有的是csdn站内的，有的是github仓库里的，为了保证md中的图片能在各平台都正常显示，了解到七牛云免费提供10G OSS对象存储空间，因此，本篇记录一下将md中的图片迁移至七牛云云床的过程。 注册七牛云注册 注册链接 注册后完成实名认证，可免费领取10G OSS空间。实名认证审核需要1个小时左右 假设你已经完成注册和实名，点击如下链接，免费领取空间: 领取10G OSS空间 创建OSS bucket 创建链接 创建好bucket后，可以上传一下文件测试 获取图片上传至七牛云图床这里写了一个python脚本，开20个线程处理md文档，逻辑是这样的: 12341.扫描md文档，获取到其中的图片url2.下载url将图片保存至本地3.将本地的图片上传至七牛云4.将md文档中的图片url替换成对应的七牛云外链url 七牛云OSS Python SDK地址: OSS Python SDK 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142import reimport requestsimport osfrom qiniu import Auth, put_filefrom concurrent.futures import ThreadPoolExecutor# 获取md文件中的图片链接,保存为文件，上传至七牛并返回七牛外链地址def get_pic_url(filename): url_map = &#123;&#125; with open(filename, 'r',) as f: content = f.read() img_patten = r'!\[.*?\]\((.*?)\)|&lt;img.*?src=[\'\"](.*?)[\'\"].*?&gt;' matches = list(re.compile(img_patten).findall(content)) if len(matches) &gt; 0: for url in matches: url = url[0] # CSDN图片直链有3种样式，真坑: # 这一种文件名太长，直接这样命名上传到七牛云之后，有时外链链接无法加载. # https://imgconvert.csdnimg.cn/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTE2MTQ1MjIxNTky, # 下面两种要去除后面的参数 # https://img-blog.csdnimg.cn/20181206193427845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l3cTkzNQ==,size_16,color_FFFFFF,t_70 # https://img-blog.csdn.net/20180626193940302?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l3cTkzNQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70 print("图片原url:", url) try: headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML,' ' like Gecko) Chrome/55.0.2883.87 Safari/537.36' &#125; if "csdn" in url: pic_name = url.split('csdnimg.cn/')[-1] # pic_name = "&#123;&#125;/&#123;&#125;".format(pic_path, url.split('csdnimg.cn/')[-1]) pic_name = pic_name.split('csdn.net/')[-1] pic_name = pic_name.split('?')[0] # csdn部分直链链接里没有格式，加个图片尾缀 if not pic_name.endswith('.jpg') and not pic_name.endswith('.png'): pic_name = pic_name + '.jpg' elif "github" in url: # 保存github图片，github直链的链接为文件链接加参数:?raw=true,例如: # https://github.com/yinwenqin/kubeSourceCodeNote/blob/master/scheduler/image/p2/schedule.jpg?raw=true pic_name = url.split('/')[-1] url += "?raw=true" else: # 别的来源的图片链接如有不同，请自行按对应格式修改 pic_name = url.split('/')[-1] response = requests.get(url, headers=headers).content pic_name_len = len(pic_name) # 名字太长截取一半 if pic_name_len &gt; 40: pic_name = pic_name[int(pic_name_len/2):] pic_name = "&#123;&#125;/&#123;&#125;".format(pic_path, pic_name) print(pic_name) with open(pic_name, 'wb') as f2: f2.write(response) new_pic_url = pic_upload(pic_name) url_map[url] = new_pic_url except Exception as e: print("文件:", filename, "url处理失败:", url, e) return &#123;&#125; print(url_map) return url_map# 获取所有的md文件def list_file(files, path): # 取出指定路径下的所有文件，包含所有子目录里的文件 items = os.listdir(path) for i in items: i_path = os.path.join(path, i) if os.path.isdir(i_path): list_file(i_path, files) else: if i_path.endswith(".md"): files.append(i_path) return files# pic上传七牛云图床，获取图片在图床中的链接，替换md文档def pic_upload(file): try: # 认证相关信息换成自己的 endpoint = "http://xxxxxx.bkt.clouddn.com" access_key = "xxxxxxxxxxxx" secret_key = "xxxxxxxxxxxxx" bucket_name = 'xxxxxxxx' key = os.path.basename(file) q = Auth(access_key, secret_key) token = q.upload_token(bucket_name, key, 3600) put_file(token, key, file) # 上传后得到的图片外链示例: http://mycloudn.kokoerp.com/AlgSchedule.jpg new_url = endpoint + '/' + os.path.basename(file) # print('new url', new_url) return new_url except Exception as e: print('upload image to QiNiu oss failed:', file, e) return ""# 替换md文件中的旧链接def modify_md(filename, url_map): try: with open(filename, "r") as f: content = f.read() for url, new_pic_url in url_map.items(): with open(filename, "w") as f: content = content.replace(url, new_pic_url) f.write(content) except Exception as e: print(filename, '文件修改失败:', e)def run(file): # &#123;old_url: new_url&#125; url_map = get_pic_url(file) if len(url_map.keys()) &gt; 0: modify_md(file, url_map)def main(path): # 获取所有的md文件 files = list_file([], path) if len(files) &gt; 0: th_pool = ThreadPoolExecutor(20) for file in files: th_pool.submit(run, file) th_pool.shutdown(wait=True) else: print("no markdown found, exit")if __name__ == "__main__": md_path = "./md/" pic_path = "../pic" if not os.path.exists(pic_path): os.makedirs(pic_path) main(md_path) 运行完成后，将替换后的md放入username.github.io/source/_posts内，push到仓库中，浏览器打开hexo blog可以看到，所有图片都可以正常显示 图床工具PicGomarkdown中插入图片一直以来都不太方便，即使是使用外链，一般也需要如下繁琐步骤： 1.保存图片 2.上传图片至云床 3.复制云床外链 4.md中插入外链 于是诞生了图床工具的需求，mac中有一款非常好用的图床上传工具iPic，支持七牛云但收费48/年，找了很久最后都打算自己撸工具的时候，发现了一款免费开源的神器PicGo，而且难能可贵的是win/linux/mac平台都能支持,试用了一番，同样相当强大。 mac一键安装: 1brew cask install picgo 其余平台参考项目地址: PicGo PicGo支持拖拽式上传图片，也支持直接使用快捷键将粘贴板中的图片上传至云床，上传完成后，自动将得到的外链地址写入系统粘贴板内，在md中直接粘贴即可显示图片。 上面繁琐的步骤，简化为了：截图至粘贴板 -&gt; 快捷键上传至图床(cmd + ctrl + u) -&gt; 粘贴使用。使用起来行云流水，一气呵成，非常之爽，良心工具，墙裂推荐！]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github pages: 免建站快速部署你的专属博客]]></title>
    <url>%2F2019%2F08%2F20%2FHexo-GithubPages%2C%E5%85%8D%E5%BB%BA%E7%AB%99%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%BD%A0%E7%9A%84%E4%B8%93%E5%B1%9E%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[前言某DN博客的页面样式实在不敢恭维，广告不少，最厉害的在于写作积分还不能用于下载，近两年Hexo挺火的，结合github pages使用，纯md文档输出，自动生成简洁好看的web页面，建站的步骤都可以省略了，试用一番下来体验确实尚可，记录一下部署过程 1. 创建github pages登录github，新建一个repo，repo名强制要求为username.github.io，其中username需与github账户owner一致，这是github pages的约定 2. 安装hexo假设你的git、nodejs等工具都已经安装好了，如果没有安装好，先安装好再进行后面的操作 安装hexo: npm install hexo-cli -g 3. 发布内容接下来用Hexo初始化一个博客，然后更改一些自定义的配置，然后发布到自己的个人Github网站(username.github.io)。 创建博客: hexo init username.github.io 安装hexo中最常用的next主题，其他主题参考hexo官网 12cd username.github.iogit clone https://github.com/iissnan/hexo-theme-next themes/next 博客配置 博客基础配置文件位于username.github.io/_config.yml,简单测试，先修改配置文件内的下面几项: 12345678title: 随笔漫谈瞎写 // 标题author: ywqlanguage: zh-Hans // 中文timezone: Asia/Chongqing theme: next //刚刚安装的主题名称deploy: type: git repo: https://www.github.com/yinwenqin/yinwenqin.github.io.git 主题配置 主题配置文件位于themes/next/_config.yml文件中,这里可以自定义的选项就很多了，next主题官网 写文档 md文档路径themes/next/_config.yml,后面需要展示的md文档都放在这个目录内，默认这个目录里有一个hello_world.md 测试 $ hexo s 测试服务启动，你可以在浏览器中输入https://localhost:4000 访问了 安装hexo-deployer-git自动部署发布工具 必须在username.github.io项目目录内安装: npm install hexo-deployer-git --save 发布 测试没问题后，生成静态网页文件发布至的Github pages 仓库中。$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 查看效果 浏览器打开，输入https://username.github.io ,记得替换成你的地址 默认的页面比这个要简陋一些，这里已经是做了一些美化的配置，如：头像、链接高亮、文章阴影、背景图、动态效果、访问数量统计等，具体怎么配置，建议安装好后浏览器搜索自己diy一番，可以部分参考下面链接,里面有些内容在当下已经不生效，需要再搜索一下别的文章: https://www.jianshu.com/p/3a05351a37dc 3. 自定义博客域名用github page的默认域名username.github.io太过千篇一律，好在github page支持自定义域名,参考: github pages自定义域名 首先假设你已经拥有一个域名，那么只需在你的域名服务商那里为它加一条CNAME记录,指向你的github pages地址: 回到你的项目中，在source目录下新建一个CNAME文件，里面加入刚刚新增的解析域名，注意，如果直接去仓库里添加Custom domain，那么每次push之后，Custom domain将会被还原为空，加上这一步之后，就不会有这个问题了 12yinwenqin.github.io ywq$ cat source/CNAMEblog.upweto.top 推送代码，过几分钟，你就可以通过自定义的域名访问github pages了: $ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d Last开始CxxN上的博客的迁移工作,下一篇记录一下图床迁移的过程]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-P3-Node筛选算法]]></title>
    <url>%2F2019%2F08%2F12%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-P3-Node%E7%AD%9B%E9%80%89%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[P3-Node筛选算法前言在上一篇文档中，我们找到调度器筛选node的算法入口pkg/scheduler/core/generic_scheduler.go:162 Schedule()方法 p2-调度器框架 那么在本篇，由此Schedule()函数展开，看一看调度器的node筛选算法，优先级排序算法留作下一篇. 正文Schedule()的筛选算法核心是findNodesThatFit()方法 ,直接跳转过去: pkg/scheduler/core/generic_scheduler.go:184 –&gt; pkg/scheduler/core/generic_scheduler.go:435 下面注释划出重点，篇幅有限省略部分代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) &#123; var filtered []*v1.Node failedPredicateMap := FailedPredicateMap&#123;&#125; if len(g.predicates) == 0 &#123; filtered = nodes &#125; else &#123; allNodes := int32(g.cache.NodeTree().NumNodes()) // 筛选的node对象的数量，点击进去可查看详情，当集群规模小于100台时，全部检查，当集群大于100台时， // 检查指定比例的机器，若指定比例范围内都没有找到合适的node，则继续查找 numNodesToFind := g.numFeasibleNodesToFind(allNodes) ... // 省略 ctx, cancel := context.WithCancel(context.Background()) // 负责筛选节点的匿名函数主体，核心实现在于内部的podFitsOnNode函数 checkNode := func(i int) &#123; nodeName := g.cache.NodeTree().Next() fits, failedPredicates, err := podFitsOnNode( pod, meta, g.nodeInfoSnapshot.NodeInfoMap[nodeName], g.predicates, g.schedulingQueue, g.alwaysCheckAllPredicates, ) if err != nil &#123; predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return &#125; if fits &#123; length := atomic.AddInt32(&amp;filteredLen, 1) if length &gt; numNodesToFind &#123; cancel() atomic.AddInt32(&amp;filteredLen, -1) &#125; else &#123; filtered[length-1] = g.nodeInfoSnapshot.NodeInfoMap[nodeName].Node() &#125; &#125; else &#123; predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() &#125; &#125; // 标记一下这里，并发执行筛选，待会儿看看它的并发是怎么设计的 // Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) // 调度器的扩展处理逻辑，如自定义的扩展筛选、优先级排序算法 if len(filtered) &gt; 0 &amp;&amp; len(g.extenders) != 0 &#123; ... // 省略 &#125; // 返回结果 return filtered, failedPredicateMap, nil&#125; 这里一眼就可以看出核心匿名函数内的主体是podFitsOnNode(),但是并不是直接执行podFitsOnNode()函数，而是又封装了一层函数，这个函数的作用是在外层使用nodeName := g.cache.NodeTree().Next()来获取要判断的node主体，传递给podFitsOnNode()函数，而后对podFitsOnNode函数执行返回的结果进行处理。着眼于其下的并发处理实现:workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode),就可以理解这样封装的好处了,来看看并发实现的内部吧: vendor/k8s.io/client-go/util/workqueue/parallelizer.go:38 12345678910111213141516171819202122232425262728293031323334func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) &#123; var stop &lt;-chan struct&#123;&#125; if ctx != nil &#123; stop = ctx.Done() &#125; toProcess := make(chan int, pieces) for i := 0; i &lt; pieces; i++ &#123; toProcess &lt;- i &#125; close(toProcess) if pieces &lt; workers &#123; workers = pieces &#125; wg := sync.WaitGroup&#123;&#125; wg.Add(workers) for i := 0; i &lt; workers; i++ &#123; go func() &#123; defer utilruntime.HandleCrash() defer wg.Done() for piece := range toProcess &#123; select &#123; case &lt;-stop: return default: doWorkPiece(piece) &#125; &#125; &#125;() &#125; wg.Wait()&#125; 敲黑板记笔记: 12345678910111.chan struct&#123;&#125;是什么鬼? struct&#123;&#125;类型的chan，不占用内存，通常用作go协程之间传递信号，详情可参考:https://dave.cheney.net/2014/03/25/the-empty-struct2.ParallelizeUntil函数接收4个参数,分别是父协程上下文,max workers,task number,task执行函数，它启动指定数量的worker协程，数量最大不超过max workers，共同完成指定数量(task number)的task，每个task执行指定的执行函数。这意味着，ParallelizeUntil函数只负责并发的数量，而并发的对象主体，需要由task执行函数自行获取。因此我们看到上面的checkNode匿名函数，内部通过nodeName := g.cache.NodeTree().Next()来获取task的对象主体，g.cache.NodeTree()对象内部必然维护了一个指针，来获取当前task所需的对象主体。这里使用的并发粒度是以node为单位的.ParallelizeUntil()的这种实现方式，可以很好地将并发实现和具体功能实现解耦，因此只要功能实现内部处理好指针，都可以复用ParallelizeUntil()函数来实现并发的控制。 来看看checkNode()内部是怎样获取每个子协程对应的node主体的: pkg/scheduler/core/generic_scheduler.go:460 --&gt; pkg/scheduler/internal/cache/node_tree.go:161 可以看到，这里有一个zone的逻辑层级，这个层级仿佛没有见过，google了一番才了解了这个颇为冷门的功能：这是一个轻量级的支持集群联邦特性的实现，单个cluster可以属于多个zone，但这个功能目前只有GCE和AWS支持，且绝大多数的使用场景也用不到，可以说是颇为冷门。默认情况下，cluster只属于一个zone，可以理解为cluster和zone是同层级，因此后面见到有关zone相关的层级，我们直接越过它。有兴趣的朋友可以了解一下zone的概念: https://kubernetes.io/docs/setup/best-practices/multiple-zones/ 继续往下, pkg/scheduler/internal/cache/node_tree.go:176 –&gt; pkg/scheduler/internal/cache/node_tree.go:47 1234567891011121314151617181920// nodeArray is a struct that has nodes that are in a zone.// We use a slice (as opposed to a set/map) to store the nodes because iterating over the nodes is// a lot more frequent than searching them by name.type nodeArray struct &#123; nodes []string lastIndex int&#125;func (na *nodeArray) next() (nodeName string, exhausted bool) &#123; if len(na.nodes) == 0 &#123; klog.Error("The nodeArray is empty. It should have been deleted from NodeTree.") return "", false &#125; if na.lastIndex &gt;= len(na.nodes) &#123; return "", true &#125; nodeName = na.nodes[na.lastIndex] na.lastIndex++ return nodeName, false&#125; 果然可以看到, nodeArray结构体内部维护了一个lastIndex指针来获取node，印证了上面的推测。 回到pkg/scheduler/core/generic_scheduler.go:461,正式进入podFitsOnNode内部: 1234567891011121314151617181920212223242526272829303132333435363738func podFitsOnNode( pod *v1.Pod, meta predicates.PredicateMetadata, info *schedulernodeinfo.NodeInfo, predicateFuncs map[string]predicates.FitPredicate, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool,) (bool, []predicates.PredicateFailureReason, error) &#123; var failedPredicates []predicates.PredicateFailureReason podsAdded := false for i := 0; i &lt; 2; i++ &#123; metaToUse := meta nodeInfoToUse := info if i == 0 &#123; podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue) &#125; else if !podsAdded || len(failedPredicates) != 0 &#123; break &#125; for _, predicateKey := range predicates.Ordering() &#123; var ( fit bool reasons []predicates.PredicateFailureReason err error ) //TODO (yastij) : compute average predicate restrictiveness to export it as Prometheus metric if predicate, exist := predicateFuncs[predicateKey]; exist &#123; fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) if err != nil &#123; return false, []predicates.PredicateFailureReason&#123;&#125;, err &#125; ... // 省略 &#125; &#125; &#125; return len(failedPredicates) == 0, failedPredicates, nil&#125; 注释和部分代码已省略,基于podFitsOnNode函数内的注释，来做一下说明: 1.通过指定pod.spec.priority,来为pod指定调度优先级的功能，在1.14版本已经正式GA，这里所有的调度相关功能都会考虑到pod优先级,因为优先级的原因，因此除了正常的Schedule调度动作外，还会有Preempt抢占调度的行为，这个podFitsOnNode()方法会被在这两个地方调用。 2.Schedule调度时，会取出当前node上所有已存在的pod，与被提名调度的pod进行优先级对比，取出所有优先级大于等于提名pod，将它们需求的资源加上提名pod所需求的资源，进行汇总，predicate筛选算法计算的时候，是基于这个汇总的结果来进行计算的。举个例子，node A memory cap = 128Gi，其上现承载有20个pod，其中10个pod的优先级大于等于提名pod，它们sum(request.memory) = 100Gi，若提名pod的request.memory = 32Gi, (100+32) &gt; 128,因此筛选时会在内存选项失败返回false；若提名pod的request.memory = 16Gi,(100+16) &lt; 128,则内存项筛选通过。那么剩下的优先级较低的10个pod就不考虑它们了吗，它们也要占用内存呀？处理方式是：如果它们占用内存造成node资源不足无法调度提名pod，则调度器会将它们剔出当前node，这即是Preempt抢占。Preempt抢占的说明会在后面的文章中补充. 3.对于每个提名pod,其调度过程会被重复执行1次，为什么需要重复执行呢？考虑到有一些场景下，会判断到pod之间的亲和力筛选策略，例如pod A对pod B有亲和性，这时它们一起调度到node上，但pod B此时实际并未完成调度启动，那么pod A的inter-pod affinity predicates一定会失败，因此，重复执行1次筛选过程是有必要的. 有了以上理解，我们接着看代码，图中已注释: 图中pkg/scheduler/core/generic_scheduler.go:608位置正式开始了逐个计算筛选算法，那么筛选方法、筛选方法顺序在哪里呢？在上一篇P2-框架篇)中已经有讲过，默认调度算法都在pkg/scheduler/algorithm/路径下，我们接着往下看. Predicates Ordering / Predicates Function 筛选算法相关的key/func/ordering，全部集中在pkg/scheduler/algorithm/predicates/predicates.go这个文件中 筛选顺序: pkg/scheduler/algorithm/predicates/predicates.go:142 12345678910// 默认predicate顺序var ( predicatesOrdering = []string&#123;CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, MaxCinderVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred&#125;) 官方的备注: 链接 筛选key 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576const ( // MatchInterPodAffinityPred defines the name of predicate MatchInterPodAffinity. MatchInterPodAffinityPred = "MatchInterPodAffinity" // CheckVolumeBindingPred defines the name of predicate CheckVolumeBinding. CheckVolumeBindingPred = "CheckVolumeBinding" // CheckNodeConditionPred defines the name of predicate CheckNodeCondition. CheckNodeConditionPred = "CheckNodeCondition" // GeneralPred defines the name of predicate GeneralPredicates. GeneralPred = "GeneralPredicates" // HostNamePred defines the name of predicate HostName. HostNamePred = "HostName" // PodFitsHostPortsPred defines the name of predicate PodFitsHostPorts. PodFitsHostPortsPred = "PodFitsHostPorts" // MatchNodeSelectorPred defines the name of predicate MatchNodeSelector. MatchNodeSelectorPred = "MatchNodeSelector" // PodFitsResourcesPred defines the name of predicate PodFitsResources. PodFitsResourcesPred = "PodFitsResources" // NoDiskConflictPred defines the name of predicate NoDiskConflict. NoDiskConflictPred = "NoDiskConflict" // PodToleratesNodeTaintsPred defines the name of predicate PodToleratesNodeTaints. PodToleratesNodeTaintsPred = "PodToleratesNodeTaints" // CheckNodeUnschedulablePred defines the name of predicate CheckNodeUnschedulablePredicate. CheckNodeUnschedulablePred = "CheckNodeUnschedulable" // PodToleratesNodeNoExecuteTaintsPred defines the name of predicate PodToleratesNodeNoExecuteTaints. PodToleratesNodeNoExecuteTaintsPred = "PodToleratesNodeNoExecuteTaints" // CheckNodeLabelPresencePred defines the name of predicate CheckNodeLabelPresence. CheckNodeLabelPresencePred = "CheckNodeLabelPresence" // CheckServiceAffinityPred defines the name of predicate checkServiceAffinity. CheckServiceAffinityPred = "CheckServiceAffinity" // MaxEBSVolumeCountPred defines the name of predicate MaxEBSVolumeCount. // DEPRECATED // All cloudprovider specific predicates are deprecated in favour of MaxCSIVolumeCountPred. MaxEBSVolumeCountPred = "MaxEBSVolumeCount" // MaxGCEPDVolumeCountPred defines the name of predicate MaxGCEPDVolumeCount. // DEPRECATED // All cloudprovider specific predicates are deprecated in favour of MaxCSIVolumeCountPred. MaxGCEPDVolumeCountPred = "MaxGCEPDVolumeCount" // MaxAzureDiskVolumeCountPred defines the name of predicate MaxAzureDiskVolumeCount. // DEPRECATED // All cloudprovider specific predicates are deprecated in favour of MaxCSIVolumeCountPred. MaxAzureDiskVolumeCountPred = "MaxAzureDiskVolumeCount" // MaxCinderVolumeCountPred defines the name of predicate MaxCinderDiskVolumeCount. // DEPRECATED // All cloudprovider specific predicates are deprecated in favour of MaxCSIVolumeCountPred. MaxCinderVolumeCountPred = "MaxCinderVolumeCount" // MaxCSIVolumeCountPred defines the predicate that decides how many CSI volumes should be attached MaxCSIVolumeCountPred = "MaxCSIVolumeCountPred" // NoVolumeZoneConflictPred defines the name of predicate NoVolumeZoneConflict. NoVolumeZoneConflictPred = "NoVolumeZoneConflict" // CheckNodeMemoryPressurePred defines the name of predicate CheckNodeMemoryPressure. CheckNodeMemoryPressurePred = "CheckNodeMemoryPressure" // CheckNodeDiskPressurePred defines the name of predicate CheckNodeDiskPressure. CheckNodeDiskPressurePred = "CheckNodeDiskPressure" // CheckNodePIDPressurePred defines the name of predicate CheckNodePIDPressure. CheckNodePIDPressurePred = "CheckNodePIDPressure" // DefaultMaxGCEPDVolumes defines the maximum number of PD Volumes for GCE // GCE instances can have up to 16 PD volumes attached. DefaultMaxGCEPDVolumes = 16 // DefaultMaxAzureDiskVolumes defines the maximum number of PD Volumes for Azure // Larger Azure VMs can actually have much more disks attached. // TODO We should determine the max based on VM size DefaultMaxAzureDiskVolumes = 16 // KubeMaxPDVols defines the maximum number of PD Volumes per kubelet KubeMaxPDVols = "KUBE_MAX_PD_VOLS" // EBSVolumeFilterType defines the filter name for EBSVolumeFilter. EBSVolumeFilterType = "EBS" // GCEPDVolumeFilterType defines the filter name for GCEPDVolumeFilter. GCEPDVolumeFilterType = "GCE" // AzureDiskVolumeFilterType defines the filter name for AzureDiskVolumeFilter. AzureDiskVolumeFilterType = "AzureDisk" // CinderVolumeFilterType defines the filter name for CinderVolumeFilter. CinderVolumeFilterType = "Cinder") 筛选Function 每个predicate key对应的function name一般为${KEY}Predicate,function的内容其实都比较简单,不一一介绍了，自行查看，这里仅列举一个: pkg/scheduler/algorithm/predicates/predicates.go:1567 123456789101112131415161718192021// CheckNodeMemoryPressurePredicate checks if a pod can be scheduled on a node// reporting memory pressure condition.func CheckNodeMemoryPressurePredicate(pod *v1.Pod, meta PredicateMetadata, nodeInfo *schedulernodeinfo.NodeInfo) (bool, []PredicateFailureReason, error) &#123; var podBestEffort bool if predicateMeta, ok := meta.(*predicateMetadata); ok &#123; podBestEffort = predicateMeta.podBestEffort &#125; else &#123; // We couldn't parse metadata - fallback to computing it. podBestEffort = isPodBestEffort(pod) &#125; // pod is not BestEffort pod if !podBestEffort &#123; return true, nil, nil &#125; // check if node is under memory pressure if nodeInfo.MemoryPressureCondition() == v1.ConditionTrue &#123; return false, []PredicateFailureReason&#123;ErrNodeUnderMemoryPressure&#125;, nil &#125; return true, nil, nil&#125; 筛选算法过程到这里就已然清晰明了！ 重点回顾筛选算法代码中的几个不易理解的点(亮点?)圈出: node粒度的并发控制 基于优先级的pod资源总和归纳计算 筛选过程重复1次 本篇调度器筛选算法篇到此结束，下一篇将学习调度器优先级排序的算法详情内容]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-P2-调度器框架]]></title>
    <url>%2F2019%2F08%2F09%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-P2-%E8%B0%83%E5%BA%A6%E5%99%A8%E6%A1%86%E6%9E%B6.html</url>
    <content type="text"><![CDATA[调度器框架前言在上一篇文档中，我们找到了sheduler调度功能主逻辑的入口: P1-调度器入口篇 那么在本篇，我们基于找到的入口，来进入调度器框架内部，看一看整体的逻辑流程,本篇先跳过调度的算法(Predicates断言选择、Priority优先级排序)，只关注pkg/scheduler目录内的scheduler框架相关的逻辑流向，摸清scheduler框架本身的代码结构，调度算法留在后面的文章再谈 框架流程回顾上一篇篇末，我们找到了调度框架的实际调度工作逻辑的入口位置，pkg/scheduler/scheduler.go:435, scheduleOne()函数内部，定位在pkg/scheduler/scheduler.go:457位置,是通过这个sched.schedule(pod)方法来获取与pod匹配的node的，我们直接跳转2次,来到了这里pkg/scheduler/core/generic_scheduler.go:107 通过注释可以知道，ScheduleAlgorithm interface中的Schedule方法就是用来为pod筛选node的，但这是个接口方法，并不是实际调用的，我们稍微往下,在pkg/scheduler/core/generic_scheduler.go:162这个位置，就可以找到实际调用的Schedule方法: 这个函数里面有4个重要的步骤: 123456789101112131415// 调度前预先检查pvc是否创建pkg/scheduler/core/generic_scheduler.go:166err := podPassesBasicChecks(pod, g.pvcLister)// 根据Predicate筛选nodepkg/scheduler/core/generic_scheduler.go:184filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes)// 给筛选出的node排出优先级pkg/scheduler/core/generic_scheduler.go:215 PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders)// 选出优先级最高的node作为fit nodepkg/scheduler/core/generic_scheduler.go:226g.selectHost(priorityList) 本篇我们不看Schedule方法内的具体调度算法细节，在这里标记一下，下一篇我们将从这里开始. 先来逆向回溯代码结构，找到哪里创建了scheduler，调度器的默认初始化配置，默认的调度算法来源等等框架相关的东西。Schedule()方法属于genericScheduler结构体，先查看genericScheduler结构体，再选中结构体名称，crtl + b组合键查看它在哪些地方被引用，找出创建结构体的位置: 通过缩略代码框，排除test相关的测试文件，很容易找出创建结构体的地方位于pkg/scheduler/core/generic_scheduler.go:1189，点击图中红框圈中位置，跳转过去，果然找到了NewGenericScheduler()方法，这个方法是用来创建一个genericScheduler对象的，那么我们再次crtl + b组合键查看NewGenericScheduler再什么地方被调用： 找出了在pkg/scheduler/factory/factory.go:441这个位置上找到了调用入口，这里位于CreateFromKeys()方法中，继续crtl + b查看它的引用,跳转到pkg/scheduler/factory/factory.go:336这个位置： 这里找到了algorithmProviderMap这个变量，顾名思义，这个变量里面包含的应该就是调度算法的来源,点击进去查看,跳转到了pkg/scheduler/factory/plugins.go:86这个位置,组合键查看引用，一眼就可以看出哪个引用为这个map添加了元素： 跳转过去，来到了pkg/scheduler/factory/plugins.go:391这个位置，这个函数的作用是为scheduler的配置指定调度算法，即FitPredicate、Priority这两个算法需要用到的metric或者方法,再次请出组合键，查找哪个地方调用了这个方法： 来到了pkg/scheduler/algorithmprovider/defaults/defaults.go:99，继续组合键向上查找引用,这次引用只有一个，没有弹窗直接跳转过去了pkg/scheduler/algorithmprovider/defaults/defaults.go:36: 我们来看看defaultPredicates(), defaultPriorities()这两个函数具体的内容: 我们随便点击进去一个predicates选项查看其内容: CheckNodeMemoryPressure这个词相应熟悉kubernetes 应用的朋友一定不会陌生，例如在node内存压力大无法调度的pod时，kubectl describe pod xxx就会在状态信息里面看到这个关键词。 让我们回到pkg/scheduler/algorithmprovider/defaults/defaults.go:102这个位置，查看factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet)方法的详情,可以看到参数factory.DefaultProvider值为字符串格式的DefaultProvider，先记住这个关键值，进入方法内部: pkg/scheduler/factory/plugins.go:387: 12345678910func RegisterAlgorithmProvider(name string, predicateKeys, priorityKeys sets.String) string &#123; schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) algorithmProviderMap[name] = AlgorithmProviderConfig&#123; FitPredicateKeys: predicateKeys, PriorityFunctionKeys: priorityKeys, &#125; return name&#125; 可以看到，这个方法为DefaultProvider绑定了配置：筛选算法和优先级排序算法的key集合，这些key只是字符串，那么是怎么具体落实到计算的方法过程上去的呢？让我们看看pkg/scheduler/algorithmprovider/defaults/目录下的register_predicates.go,register_priorities.go这两个文件: 它们同样也在init()函数中初始化时使用factory.RegisterFitPredicate()方法做了一些注册操作,这个方法的两个参数，前一个是筛选/计算优先级 的关键key名，后一个是具体计算的功能实现方法，点击factory.RegisterFitPredicate()方法，深入一级，查看内部代码， 123456789// RegisterFitPredicateFactory registers a fit predicate factory with the// algorithm registry. Returns the name with which the predicate was registered.func RegisterFitPredicateFactory(name string, predicateFactory FitPredicateFactory) string &#123; schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() validateAlgorithmNameOrDie(name) fitPredicateMap[name] = predicateFactory return name&#125; 可以看出，两者使用map[string]func()的方式关联在了一起，那么在后面实际调用的时候，必定是在map中基于key找出方法并执行。优先级相关的factory.RegisterPriorityFunction2()方法亦是同理。 生成默认配置还记得刚刚重点圈出的DefaultProvider关键值吗？通过上面我们知道了，所有默认Predicate/priority算法的实现都是绑定在这个默认的AlgorithmProvider身上的，那么，启动scheduler的时候，究竟是如何将DefaultProvider作为默认AlgorithmProvider呢？让我们回到最初的调度器启动命令入口位置cmd/kube-scheduler/app/server.go:62: 1234567891011121314151617181920212223242526272829303132333435363738394041424344opts, err := options.NewOptions()// 点击NewOptions跳转进入内部,来到了这个位置:cmd/kube-scheduler/app/options/options.go:75func NewOptions() (*Options, error) &#123; cfg, err := newDefaultComponentConfig() if err != nil &#123; return nil, err &#125; ... // 省略&#125;// 这个newDefaultComponentConfig方法特别有意思，从字面看它是用来为组件填充默认配置的// 来看看它的内容，点击来到了cmd/kube-scheduler/app/options/options.go:132func newDefaultComponentConfig() (*kubeschedulerconfig.KubeSchedulerConfiguration, error) &#123; cfgv1alpha1 := kubeschedulerconfigv1alpha1.KubeSchedulerConfiguration&#123;&#125; kubeschedulerscheme.Scheme.Default(&amp;cfgv1alpha1)... // 省略&#125;// 点击kubeschedulerscheme.Scheme.Default(&amp;cfgv1alpha1)中的Default跳转进入// 来到了这里:vendor/k8s.io/apimachinery/pkg/runtime/scheme.go:389func (s *Scheme) AddTypeDefaultingFunc(srcType Object, fn func(interface&#123;&#125;)) &#123; s.defaulterFuncs[reflect.TypeOf(srcType)] = fn&#125;// Default sets defaults on the provided Object.func (s *Scheme) Default(src Object) &#123; if fn, ok := s.defaulterFuncs[reflect.TypeOf(src)]; ok &#123; fn(src) &#125;&#125;// 看看defaulterFuncs的数据类型： // defaulterFuncs is an array of interfaces to be called with an object to provide defaulting // the provided object must be a pointer. defaulterFuncs map[reflect.Type]func(interface&#123;&#125;)// 不难看出，这个Default()方法是通过反射器，获取对象的类型，以类型作为map的key，从而获取该类型// 对应的defaulterFuncs，也即是该结构体填充默认配置的方法，最后执行该方法// 那么这个defaulterFuncs map[reflect.Type]func(interface&#123;&#125;)，里面的元素时怎么填充的呢？// 作者很贴心地将添加map元素的方法写在了Default()方法的正上方:func (s *Scheme) AddTypeDefaultingFunc(srcType Object, fn func(interface&#123;&#125;)) &#123; s.defaulterFuncs[reflect.TypeOf(srcType)] = fn&#125; 我们选中然后ctrl+b，查找AddTypeDefaultingFunc()的引用，弹窗中你可以看到有非常非常多的对象都引用了该方法，这些不同类型的对象相信无一例外都是通过Default()方法来生成默认配置的，我们找到其中的包含scheduler的方法: 跳转进去，来到了这个位置pkg/scheduler/apis/config/v1alpha1/zz_generated.defaults.go:31(原谅我的灵魂笔法): 进入SetDefaults_KubeSchedulerConfiguration()，来到pkg/scheduler/apis/config/v1alpha1/defaults.go:42: 看到了DefaultProvider吗？是不是觉得瞬间豁然开朗，原来是在这里调用指定了scheduler配置的AlgorithmSource.Provider。 调度功能实现的回溯让我们捋一捋调度器框架运行调度功能相关的流程: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// 1.获取AlgorithmSource.Provider(默认"DefaultProvider")，作为key从map中获取到pkg/scheduler/algorithmprovider包内为其初始化的两种算法key集合algorithmProviderMap[name] = AlgorithmProviderConfig&#123; FitPredicateKeys: predicateKeys, PriorityFunctionKeys: priorityKeys, &#125; // 2.填充genericScheduler对象的predicates元素:// pkg/scheduler/factory/plugins.go:411func getFitPredicateFunctions(names sets.String, args PluginFactoryArgs) (map[string]predicates.FitPredicate, error) &#123; schedulerFactoryMutex.Lock() defer schedulerFactoryMutex.Unlock() fitPredicates := map[string]predicates.FitPredicate&#123;&#125; for _, name := range names.List() &#123; factory, ok := fitPredicateMap[name] if !ok &#123; return nil, fmt.Errorf("invalid predicate name %q specified - no corresponding function found", name) &#125; fitPredicates[name] = factory(args) &#125; // Always include mandatory fit predicates. for name := range mandatoryFitPredicates &#123; if factory, found := fitPredicateMap[name]; found &#123; fitPredicates[name] = factory(args) &#125; &#125; return fitPredicates, nil&#125;// 3.对predicates内的每一个key，找到对应的检查方法，执行每一项检查,返回检查结果// pkg/scheduler/core/generic_scheduler.go:608func podFitsOnNode( pod *v1.Pod, meta predicates.PredicateMetadata, info *schedulernodeinfo.NodeInfo, predicateFuncs map[string]predicates.FitPredicate, queue internalqueue.SchedulingQueue, alwaysCheckAllPredicates bool,) (bool, []predicates.PredicateFailureReason, error) &#123; var failedPredicates []predicates.PredicateFailureReason podsAdded := false for i := 0; i &lt; 2; i++ &#123; metaToUse := meta nodeInfoToUse := info if i == 0 &#123; podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue) &#125; else if !podsAdded || len(failedPredicates) != 0 &#123; break &#125; for _, predicateKey := range predicates.Ordering() &#123; var ( fit bool reasons []predicates.PredicateFailureReason err error ) if predicate, exist := predicateFuncs[predicateKey]; exist &#123; fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) if err != nil &#123; return false, []predicates.PredicateFailureReason&#123;&#125;, err &#125; ... // 省略 &#125; &#125; &#125; &#125; &#125; return len(failedPredicates) == 0, failedPredicates, nil&#125; 目录结构总结最后，对pkg/scheduler路径下的各子目录的功能来一个图文总结吧: Last如果有沉下心来阅读代码，结合上面的图文讲解、代码块中的中文注释，相信你对调度器框架包内的代码结构会有一个较为清晰的整体掌握，本篇框架篇到此结束，下一篇来谈谈详细的调度算法的细节]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-总篇]]></title>
    <url>%2F2019%2F08%2F08%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E6%80%BB%E7%AF%87.html</url>
    <content type="text"><![CDATA[## 前言在熟悉kubernetes及常用组件、插件的管理使用后，总还觉得差了些什么，不够通透，是时候来读一读源码了，结合代码与实际使用场景来互相印证，有助于对kubernetes的理解更为透彻。这里将会分多篇介绍kubernetes各核心组件的工作模式、调度管理算法等。 版本Kubernetes v1.14.3 ,最新部署的一套环境是此版本，代码版本保持一致，方便后续测试调试 核心组件 Scheduler Controller 待补充 环境准备golang(安装步骤略，网上很多),我的环境如下: 12GOPATH="/Users/ywq/go"go version go1.11.6 darwin/amd64 注意:本系列文档中,GOPATH路径/Users/ywq/go/,在自己的环境下请替换成相应本地的GOPATH，这一点后面不再说明 1234567#拉取kubernetes项目源码:cd /Users/ywq/go/mkdir -p src/k8s.iocd src/k8s.iogit clone https://github.com/kubernetes/kubernetes.gitcd kubernetesgit checkout v1.14.3 # 切换到tag为v1.14.3的版本 源码拉取完成后，用自己的IDE打开，准备工作完毕. TipsKubernetes这一整个项目颇为庞大，一般情况下，如果熟悉kubernetes的应用，结合应用来理解源码的设计理念会容易许多，因此，对其应用不熟悉的朋友，不建议直接阅读源码。另外，目前处于边阅读代码边输出总结的阶段，如文中有误，请予以指正，非常感谢！ 在此立一个flag，源码干货总结笔记，至少做到周不断更！ 参考1.官方开发者向导md文档: https://github.com/kubernetes/community/tree/master/contributors/devel 2.http://hutao.tech/k8s-source-code-analysis/]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-P1-调度器入口篇]]></title>
    <url>%2F2019%2F08%2F05%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-P1-%E8%B0%83%E5%BA%A6%E5%99%A8%E5%85%A5%E5%8F%A3%E7%AF%87.html</url>
    <content type="text"><![CDATA[## 调度器入口前言本篇介绍scheduler的初始化相关逻辑 入口之前入口函数是位于cmd/kube-scheduler/scheduler.go中的main()方法,调用的是app.NewSchedulerCommand()方法，跳转至此方法，可以看到函数上方的注释： 1234// NewSchedulerCommand creates a *cobra.Command object with default parametersfunc NewSchedulerCommand() *cobra.Command &#123; ...&#125; NewSchedulerCommand创建的是一个cobra.Command对象，后续的命令行处理相关功能都是借助cobra来实现的，那么继续往下之前，为了避免从入口开始就一脸懵，有必要了解一下cobra这个工具 cobra什么是cobra?github主页: https://github.com/spf13/cobra主页的介绍是: Cobra是一个强大的用于创建现代化CLI命令行程序的库，用于生成应用程序和命令文件。众多高知名度的项目采用了它，例如我们熟悉的kubernetes和dockercobra创建的程序CLI遵循的模式是: APPNAME COMMAND ARG --FLAG，与常见的其他命令行程序一样，例如git: git clone URL --bare 安装：123456789101112131415#最简单的安装方式，但毫无意外，事情并没有那么简单，我们的网络的问题，导致无法正常安装依赖，go get -u github.com/spf13/cobra/cobra#怎么办呢？先进入GOPATH中，手动安装报错缺失的两个依赖:cd /Users/ywq/go/mkdir -p src/golang.org/xcd golang.org/xgit clone https://github.com/golang/text.gitgit clone https://github.com/golang/sys.git#然后执行:go install github.com/spf13/cobra/cobramatebook-x-pro:x ywq$ ls /Users/ywq/go/bin/cobra/Users/ywq/go/bin/cobra#安装完毕,记得把GOBIN加入PATH环境变量哦,否则无法直接运行cobra命令 简单试用cobra:123456789101112matebook-x-pro:local ywq$ cd /Users/ywq/go/src/local/matebook-x-pro:local ywq$ cobra init testapp --pkg-name=local/testappmatebook-x-pro:local ywq$ lstestappmatebook-x-pro:local ywq$ ls testapp/LICENSE cmd/ main.gomatebook-x-pro:local ywq$ ls testapp/cmd/root.gomatebook-x-pro:local ywq$ cd testappmatebook-x-pro:local ywq$ go run main.go # 报错：subcommand is required，要求提供子命令# 因需要多次测试，这里所有的测试步骤就把build的步骤跳过，直接使用go run main.go进行测试 我们打开IDE来查看一下testapp的代码结构: 123456789101112131415161718192021222324252627# 现在还未创建子命令，那么来创建几个试试:matebook-x-pro:testapp ywq$ cobra add getget created at /Users/ywq/go/src/local/testappmatebook-x-pro:testapp ywq$ cobra add deletedelete created at /Users/ywq/go/src/local/testappmatebook-x-pro:testapp ywq$ cobra add addadd created at /Users/ywq/go/src/local/testappmatebook-x-pro:testapp ywq$ cobra add updatematebook-x-pro:testapp ywq$ ls cmd/add.go delete.go get.go root.go update.go# 查看help，可以发现刚添加的子命令已经加入提示并可用了matebook-x-pro:testapp ywq$ go run main.go -h...Available Commands: add A brief description of your command delete A brief description of your command get A brief description of your command help Help about any command update A brief description of your command# 调用子命令试试:matebook-x-pro:testapp ywq$ go run main.go getget calledmatebook-x-pro:testapp ywq$ go run main.go addadd called 来看看新增的子命令是怎么运行的呢？截图圈中部分可以看出，子命令是在init()函数里为root级添加了一个子命令，先不去管底层实现，接着往下. 测试cobra的强大简洁的flag处理我们在cmd/delete.go的init()函数中，定义一个flag处理配置:12var obj stringdeleteCmd.PersistentFlags().StringVar(&amp;obj,"object", "", "A function to delete an test object") 在Run:func()匿名函数中添加一行输出:fmt.Println(&quot;delete obj:&quot;,cmd.Flag(&quot;object&quot;).Value) 运行结果: 123matebook-x-pro:testapp ywq$ go run main.go delete --object obj1delete calleddelete obj: obj1 如果觉得--flag符号太麻烦，cobra同样支持短符号-flag缩写: 运行结果: 123matebook-x-pro:testapp ywq$ go run main.go delete -o obj1delete calleddelete obj: obj1 这里只是两级命令加flag,但我们常见的，例如(kubectl delete pod xxx)，是有3级命令 + args的，怎么再多添加一级子命令呢？cobra帮你一条命令实现 123matebook-x-pro:testapp ywq$ cobra add pods -p deleteCmd # -p为父级命令，默认其名称格式为(parentCommandName)Cmdmatebook-x-pro:testapp ywq$ ls cmdadd.go delete.go get.go pods.go root.go update.go 可以发现,cmd/目录下多了一个pods.go文件，我们来看看它是怎么关联上delete父级命令的,同时为它添加一行输出:执行命令: 123matebook-x-pro:testapp ywq$ go run main.go delete pods pod1pods calleddelete pods: pod1 看到这里，相信对cobra的强大简洁已经有了初步的认知，建议自行进入项目主页了解详情并进行安装测试入口通过对上方cobra的基本了解，我们不难知道，cmd/kube-scheduler/scheduler.go内的main()方法内部实际调用的是cobra.Command.Run内的匿名函数，我们可以进入NewSchedulerCommand()内部确认: 可以看到，调用了Run内部runCommand方法，再来看看Run方法内部需要重点关注的几个点： 其中，上方是对命令行的参数、选项校验的步骤，跳过，重点关注两个变量:cc和stopCh，这两个变量会作为最后调用Run()方法的参数，其中stopCh作用是作为主程序退出的信号通知其他各协程进行相关的退出操作的，另外一个cc变量非常重要，可以点击c.Complete()方法，查看该方法的详情：Complete()方法本质上返回的是一个Config结构体，该结构体内部的元素非常丰富，篇幅有限就不一一点开截图了，大家可以自行深入查看这些元素的作用，这里简单概括一下其中几个: 1234567891011121314151617// scheduler 本身相关的配置都集中于此，例如名称、调度算法、pod亲和性权重、leader选举机制、metric绑定地址，健康检查绑定地址，绑定超时时间等等ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration// 这几个元素都是与apiserver认证授权相关的InsecureServing *apiserver.DeprecatedInsecureServingInfo // nil will disable serving on an insecure portInsecureMetricsServing *apiserver.DeprecatedInsecureServingInfo // non-nil if metrics should be served independentlyAuthentication apiserver.AuthenticationInfoAuthorization apiserver.AuthorizationInfoSecureServing *apiserver.SecureServingInfo// Clientset.Interface内部封装了向apiServer所支持的所有apiVersion(apps/v1beta2,extensions/v1beta1...)之下的resource(pod/deployment/service...)发起查询请求的功能Client clientset.Interface// 这几个元素都是与Event资源相关的，实现rest api处理以及记录、通知等功能EventClient v1core.EventsGetterRecorder record.EventRecorderBroadcaster record.EventBroadcaster 这里层级非常深，不便展示，Config这一个结构体非常重要，可以认真读一读代码。回到cmd/kube-scheduler/app/server.go.runCommand这里来,接着往下，进入其最后return调用的Run()函数中，函数中的前部分都是启动scheduler相关的组件，如event broadcaster、informers、healthz server、metric server等，重点看图中红框圈出的sched.Run(),这才是scheduler主程序的调用运行函数: 进入sched.Run(): wait.Until这个调用的逻辑是，直到收到stop信号才终止，在此之前循环运行sched.scheduleOne。代码走到这里，终于找到启动入口最内部的主体啦: sched.scheduleOne这个函数有代码点长，整体的功能可以概括为: 1.获取需调度的pod 2.使用调度算法寻找匹配node、发起绑定到node请求、绑定检查等一系列操作. 3.若匹配node失败，则尝试根据pod的指定优先级来抢占资源 本篇入口篇到这里就先告一段落，下一篇开始阅读学习调度过程的逻辑！]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes源码学习-Scheduler-总览篇]]></title>
    <url>%2F2019%2F08%2F05%2FKubernetes%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-Scheduler-%E6%80%BB%E8%A7%88%E7%AF%87.html</url>
    <content type="text"><![CDATA[## 调度器总体设计调度器源码分段阅读目录 调度器入口 调度器框架 Node筛选算法 待补充 概述首先列出官方md链接，讲解颇为生动：https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md这里用结合自己阅读代码的理解做一下翻译。 工作模式Kubernetes scheduler独立运作与其他主要组件之外(例如API Server)，它连接API Server，watch观察，如果有PodSpec.NodeName为空的Pod出现，则开始工作，通过一定得筛选算法，筛选出合适的Node之后，向API Server发起一个绑定指示，申请将Pod与筛选出的Node进行绑定。 代码层级回归到代码本身，scheduler的设计分为3个主要代码层级： cmd/kube-scheduler/scheduler.go: 这里的main()函数即是scheduler的入口，它会读取指定的命令行参数，初始化调度器框架，开始工作 pkg/scheduler/scheduler.go: 调度器框架的整体代码，框架本身所有的运行、调度逻辑全部在这里 pkg/scheduler/core/generic_scheduler.go: 上面是框架本身的所有调度逻辑，包括算法，而这一层，是调度器实际工作时使用的算法，默认情况下，并不是所有列举出的算法都在被实际使用，参考位于文件中的Schedule()函数 调度算法逻辑逻辑图：123456789101112131415161718192021222324252627282930313233343536373839404142一个没有指定Spec.NodeName的: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ 断言(硬性指标)筛选: node 3 资源不足 +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | 剩余可选nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ 优先级判断: node 1: priority=2 node 2: priority=5 +-------------------+-------------------------+ | | v 选择 max&#123;node priority&#125; = node 2 node2则成为成功筛选出的与pod绑定的节点 为了给pod挑选出合适的node，调度器做出如下尝试步骤： 第一步，通过一系列的predicates(断言)指标，排除不合适的node，例如：pod.resources.requests.memory: 16Gi, node则计算：node.capacity 减去node上现有的所有pod的pod.resources.requests.memory的总和，如果差小于16Gi，那么则此项predicates结果为false，排除此节点 第二步，对通过了上一步筛选的node，执行一系列的优先级计算函数，计算的对象是node的负载情况，负载是即是node上现有的所有pod的pod.resources.requests的资源的总和除以node.capacity，值越高则负载越高，优先级越低 最终，挑选出了最高优先级的node，若有多个，则随机挑选其中一个 Predicates and priorities policies调度算法一共由Predicates和priorities这两部分组成，Predicates(断言)是用来过滤node的一系列策略集合，Priorities是用来优选node的一系列策略集合。默认情况下，kubernetes提供内建predicates/priorities策略，代码集中于pkg/scheduler/algorithm/predicates/predicates.go 和 pkg/scheduler/algorithm/priorities内. 调度策略扩展管理员可以选择要应用的预定义调度策略中的哪一个，开发者也可以添加自定义的调度策略。 修改调度策略默认调度策略是通过defaultPredicates() 和 defaultPriorities()这两个函数定义的，源码在 pkg/scheduler/algorithmprovider/defaults/defaults.go，我们可以通过命令行flag –policy-config-file CONFIG_FILE 来修改默认的调度策略。除此之外，也可以在pkg/scheduler/algorithm/predicates/predicates.go pkg/scheduler/algorithm/priorities源码中添加自定义的predicate和prioritie策略，然后注册到defaultPredicates()/defaultPriorities()中来实现自定义调度策略。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Golang</tag>
        <tag>读源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang算法练习：排序]]></title>
    <url>%2F2019%2F07%2F22%2Fgolang%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0%EF%BC%9A%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[## 需求排序操作，列举常见的几种排序模型:冒泡、选择、插入、快排 备注：需求和运行输出结果均已在代码中注释 冒泡代码： 12345678910111213141516171819202122package mainimport "fmt"/*思路：从第一个元素开始循环，与其相邻的元素两两比较，若左边元素大于右边元素，则两者互换位置，保持右边的元素比左边元素大的排序原则分两层循环，外层循环的成员是所有元素，内层循环是成员右边的元素*/func main() &#123; arr := [7]int&#123;9, 11, 4, 7, 15, 12, 6&#125; fmt.Println("排序前:", arr) for i := 0; i &lt; len(arr); i++ &#123; for j := i + 1; j &lt; len(arr); j++ &#123; if arr[i] &gt; arr[j] &#123; arr[i], arr[j] = arr[j], arr[i] &#125; &#125; &#125; fmt.Println("排序后:", arr)&#125; 选择代码: 12345678910111213141516171819202122232425262728293031package mainimport ( "fmt")/*思路：假设arr有n个元素第1次循环，从第0位元素开始，依次与arr[1..n-1]元素进行比较，取出最小的元素，与arr[0]的位置互相交换第2次循环，从第1位元素开始，依次与arr[2..n-1]元素进行比较，取出最小的元素，与arr[1]的位置互相交换..第n-1次循环，就剩最后2个元素arr[n-2, n-1]，n-2位元素与arr[n-1]元素进行比较，取出最小的元素，与arr[n-2]的位置互相交换剩下最后一个元素，就是最大的元素，循环结束。核心思想：每次循环确定一个小元素的位置，n个元素，外层循环n-1次*/func main() &#123; arr := [7]int&#123;9, 11, 4, 7, 15, 12, 6&#125; for i := 0; i &lt; len(arr); i++ &#123; minIndex := i for j := i + 1; j &lt; len(arr); j++ &#123; if arr[j] &lt; arr[minIndex] &#123; minIndex = j &#125; &#125; arr[i], arr[minIndex] = arr[minIndex], arr[i] &#125; fmt.Println("排序后:", arr)&#125; 插入代码：1234567891011121314151617181920212223242526272829303132333435363738package mainimport "fmt"/*思路：假设arr有n个元素，无序的，新建一个newArr数组，这个数组要求有序(假设从小至大)，默认只放入arr[0]这一个元素，arr后面的元素，依次比较，按照合理的位置插入newArr数组中最后组合成新的完整有序的newArr*/func main() &#123; arr := [7]int&#123;9, 11, 4, 7, 15, 12, 6&#125; newArr := [7]int&#123;&#125; newArr[0] = arr[0] /* newArr首位元素与arr首元素相等无需比较，因此arr从第1位开始取出元素与newArr中所有的进行比较，即arr[i]中的元素,与newArr[0..i-1] 进行倒序逐个比较，arr[i]大则插入比较元素newArr[j]的右边，arr[i]小则继续向左移动。在选择插入合适位置时，如果插入之后再来移动其右 边的元素，复杂度高，因此： 1.若比较元素newArr[j] &gt; arr[i]大，则提前将j的下一位newArr[j+1]赋值成newArr[j]，相当于变相地将newArr[j]往后移动一位，给前方提前预留一个位置出来，合适的时候给arr[i]插入 2.若newArr[j] &lt; arr[i]，则说明arr[i]已经找到了合适的位置，即j的下一位newArr[j+1]，将newArr[j+1]赋值为arr[i],本次循环完成,进入下一次循环 */ for i := 1; i &lt; len(arr); i++ &#123; for j := i - 1; j &gt;= 0; j-- &#123; if arr[i] &lt; newArr[j] &#123; newArr[j+1] = newArr[j] // 如果是arr[i]比newArr[0]还小，则arr[i]成为新的newArr[0] if j == 0 &#123; newArr[0] = arr[i] &#125; &#125; else &#123; newArr[j+1] = arr[i] break &#125; &#125; &#125; fmt.Println(newArr)&#125; 快排112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package mainimport ( "fmt" "math/rand" "time")func QuickSort(left int, right int, array *[8000000]int) &#123; /* params: ;left: 左下标 ;right: 右下表 ;array: 无序数组 思路总结: 1.在数组内部进行顺序调换，这一点与冒泡的思路一致 2.选取位于中间位置的元素作为一个中间值进行比较，找到一个大于它的元素和一个小于它的元素，两个进行位置交换 3.左右双方向递归，保证数组内的每个元素都能找到比自身大和比自身小的元素，进行位置互换。这样，每个元素都出现在它合适的位置(即保证左边比 小，右边比自身大，也就完成了排序) */ l := left r := right // pivot 是中轴， 支点 pivot := array[(left+right)/2] temp := 0 //for 循环的目标是将比 pivot 小的数放到 左边 // 比 pivot 大的数放到 右边 for l &lt; r &#123; //从 pivot 的左边找到大于等于pivot的值 for array[l] &lt; pivot &#123; l++ &#125; //从 pivot 的右边边找到小于等于pivot的值 for array[r] &gt; pivot &#123; r-- &#125; // 1 &gt;= r 表明本次分解任务完成, break if l &gt;= r &#123; break &#125; //交换 temp = array[l] array[l] = array[r] array[r] = temp //优化 if array[l] == pivot &#123; r-- &#125; if array[r] == pivot &#123; l++ &#125; &#125; // 如果 1== r, 再移动下 if l == r &#123; l++ r-- &#125; // 分为两段，向左递归 if left &lt; r &#123; QuickSort(left, r, array) &#125; // 分为两段，向右递归 if right &gt; l &#123; QuickSort(l, right, array) &#125;&#125;func main() &#123; // arr := [9]int &#123;-9,78,0,23,-567,70, 123, 90, -23&#125; // fmt.Println("初始", arr) var arr [8000000]int for i := 0; i &lt; 8000000; i++ &#123; arr[i] = rand.Intn(900000) &#125; //fmt.Println(arr) start := time.Now() //调用快速排序 QuickSort(0, len(arr)-1, &amp;arr) end := time.Since(start).Seconds() fmt.Println("main..") fmt.Printf("快速排序法耗时%f秒", end) // 快速排序法耗时1.279648秒 //fmt.Println(arr)&#125; 快排212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package mainimport ( "fmt" "math/rand" "time")func sort(arr, orderArr []int) (subOrderArr []int) &#123; /* params: ;arr: 无序数组 ;orderArr: 有序数组 思路总结: 1.创建一个默认的无序数组arr，一个空的有序数组orderArr，作为参数传给递归函数体 2.每次选取一个无序数组中的元素，作为参照的中间值，小于它的放到一个临时无序数组(leftSlice)，大于它的也放到一个临时无序数组(rightSlice) 3.将新的无序数组(leftSlice/rightSlice)和现有的有序数组作为参数，递归传递给函数体本身。递归打破的条件是临时无序数组为空或只有 1个元素，就说明本次递归中的无序数组无需再对比了，拼接入现有的orderArr中，同一递归层级内，拼接顺序永远保持:左侧+中间值+右侧，拼接完成 后返回给递归上一级，进入下一步。 4.用递归思想将数组分割成多个以两个值为一组的单位，进行单位内部的比较，再串成一个完整的数组。递归最内层，就是值最小的一组组合。 Tips: 递归函数编写思路: 1.需要提取出共同的参数条件，递归函数执行条件，递归打破条件 2.若变量在递归过程需要作为参数被继承且反复使用，则一定要将该参数同时作为返回值返回，并在递归栈的上一层接收返回值重新给变 量赋值，否则回到上一层递归后，该变量的变化不会生效。 */ //fmt.Println("递归前", orderArr) midValue := arr[0] var leftSlice []int var rightSlice []int for _, i := range arr &#123; if i &lt; midValue &#123; leftSlice = append(leftSlice, i) &#125; if i &gt; midValue &#123; rightSlice = append(rightSlice, i) &#125; &#125; //fmt.Println("中间值:", midValue, "leftSlice:", leftSlice, "rightSlice:", rightSlice) if len(leftSlice) &gt; 1 &#123; //fmt.Println("进入左侧递归") orderArr = sort(leftSlice, orderArr) &#125; else &#123; // 当无序数组元素数量为0或1时，直接合并它 orderArr = append(orderArr, leftSlice...) //fmt.Println("本次递归左侧最小:", leftSlice[0], "已加入有序数组", orderArr) &#125; orderArr = append(orderArr, midValue) //fmt.Println("加入中间值:", midValue, orderArr) if len(rightSlice) &gt; 1 &#123; //fmt.Println("进入右侧递归") orderArr = sort(rightSlice, orderArr) &#125; else &#123; // 当无序数组元素数量为0或1时，直接合并它 orderArr = append(orderArr, rightSlice...) //fmt.Println("本次递归右侧最小:", rightSlice[0], "已加入有序数组", orderArr) &#125; //fmt.Println("递归后:", orderArr) return orderArr&#125;func main() &#123; arr := make([]int, 8000000) for i := 0; i &lt; 8000000; i++ &#123; tmp := rand.Intn(900000) arr = append(arr, tmp) &#125; orderArr := make([]int, len(arr)) start := time.Now() orderArr = sort(arr, orderArr) //fmt.Println(orderArr) end := time.Since(start).Seconds() fmt.Printf("快速排序法耗时%f秒", end) // 快速排序法耗时2.116863秒&#125; 总结时间复杂度对比:冒泡:O(n^2)选择:O(n^2)插入:O(n^2)快排:O(nlogn) 冒泡和选择排序复杂度基本一致，插入排序在最差情况下复杂度与前两者一致，但平均复杂度低于前两者，快排要快上不少，基于快排1稍稍改动了一下思路，实现的快排2，思路方面个人认为快排2更便于理解，同时更贴近递归的深度优先理念，深入到最内层先找到最小的元素。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang算法练习:单链表/双链表/环形链表]]></title>
    <url>%2F2019%2F07%2F18%2Fgolang%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0%EF%BC%9A%E5%8D%95%E9%93%BE%E8%A1%A8_%E5%8F%8C%E9%93%BE%E8%A1%A8_%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[## 需求链表，常见且非常灵活的数据模型，可定制性强，可根据需求调整满足不同的使用需求，如FIFO\LIFO，快速查找等，这里分别列举基础的单向链表和双向链表增删改查操作备注：需求和运行输出结果均已在代码中注释 单向链表代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170package mainimport ( "errors" "fmt")type Node struct &#123; id int name string next *Node&#125;type SingleLink struct &#123; head *Node&#125;func (singleLink *SingleLink) orderInsert(node *Node) &#123; // 按Node.id从小到大的顺序，插入链表中 curNode := singleLink.head if curNode.id == 0 &#123; // 第一次插入元素的空链表 singleLink.head = node &#125; else &#123; // 如果node.id比队首id小，则队首让位后移，要保证队首最小。因为取值比较的时候， 是取curNode.next节点与node进行比较，如果满足大小顺序， // 则将node插入curNode和curNode.next之间 if curNode.id &gt; node.id &#123; node.next = curNode singleLink.head = node &#125; else &#123; // 如果node.id不比队首id小，则往后取一位，用后一位与node比较，匹配则插入当前位与后一位中间 for &#123; if curNode.next == nil &#123; // 循环到最后一位都没有插入成功，说明node.id比现有链表中的所有node的id都大，则node成为新的队尾 curNode.next = node break &#125; if curNode.next.id &gt; node.id &#123; node.next = curNode.next curNode.next = node break &#125; curNode = curNode.next &#125; &#125; &#125;&#125;func (singleLink *SingleLink) pop(id int) (node *Node, err error) &#123; // 弹出链表中指定id的node curNode := singleLink.head if curNode.id == id &#123; // 要取的是队首 singleLink.head = singleLink.head.next return curNode, nil &#125; flag := false // 是否查找到的标志 for &#123; if curNode.next == nil &#123; break &#125; if curNode.next.id == id &#123; flag = true selectNode := curNode.next curNode.next = curNode.next.next return selectNode, nil &#125; curNode = curNode.next &#125; if !flag &#123; // 如果循环结束还没有查找到，输出信息 return nil, errors.New("node not found") &#125; return&#125;func (singleLink *SingleLink) getLength() (number int) &#123; curNode := singleLink.head if singleLink.head.id == 0 || singleLink.head == nil &#123; // 空链表 return &#125; else &#123; for &#123; number += 1 curNode = curNode.next if curNode == nil &#123; break &#125; &#125; &#125; fmt.Println("current link length:", number) return number&#125;func (singleLink *SingleLink) list() &#123; if singleLink.head == nil || singleLink.head.id == 0 &#123; fmt.Println("link is empty") &#125; else &#123; curNode := singleLink.head for &#123; if curNode == nil &#123; break &#125; fmt.Print(*curNode, "==&gt;") curNode = curNode.next &#125; &#125; fmt.Println()&#125;func main() &#123; // 约定空链表的header初始id为0，后面加入的实例id必须大于0 var originalHead = Node&#123; id: 0, &#125; var s = SingleLink&#123; head: &amp;originalHead, &#125; var node1 = Node&#123; id: 1, name: "001", &#125; var node2 = Node&#123; id: 2, name: "002", &#125; var node3 = Node&#123; id: 3, name: "003", &#125; var node4 = Node&#123; id: 4, name: "004", &#125; var node5 = Node&#123; id: 5, name: "005", &#125; s.orderInsert(&amp;node4) s.list() s.orderInsert(&amp;node2) s.list() s.orderInsert(&amp;node3) s.list() s.orderInsert(&amp;node5) s.list() s.orderInsert(&amp;node1) s.list() s.getLength() popNode, err := s.pop(3) if err != nil &#123; fmt.Println(err) &#125; fmt.Println("pop node successfully:", *popNode) s.list() /* output: &#123;4 004 &lt;nil&gt;&#125;==&gt; &#123;2 002 0xc00000a0e0&#125;==&gt;&#123;4 004 &lt;nil&gt;&#125;==&gt; &#123;2 002 0xc00000a0c0&#125;==&gt;&#123;3 003 0xc00000a0e0&#125;==&gt;&#123;4 004 &lt;nil&gt;&#125;==&gt; &#123;2 002 0xc00000a0c0&#125;==&gt;&#123;3 003 0xc00000a0e0&#125;==&gt;&#123;4 004 0xc00000a100&#125;==&gt;&#123;5 005 &lt;nil&gt;&#125;==&gt; &#123;1 001 0xc00000a0a0&#125;==&gt;&#123;2 002 0xc00000a0c0&#125;==&gt;&#123;3 003 0xc00000a0e0&#125;==&gt;&#123;4 004 0xc00000a100&#125;==&gt;&#123;5 005 &lt;nil&gt;&#125;==&gt; current link length: 5 pop node successfully: &#123;3 003 0xc00000a0e0&#125; &#123;1 001 0xc00000a0a0&#125;==&gt;&#123;2 002 0xc00000a0e0&#125;==&gt;&#123;4 004 0xc00000a100&#125;==&gt;&#123;5 005 &lt;nil&gt;&#125;==&gt; */&#125; 问题单向链表的操作必须从首部开始，那么可不可以灵活一些，比如从尾部开始呢？当然可以，改造成双向链表即可满足 双向链表代码：为了以示区别，增删改查全部改为从尾部开始循环123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202package mainimport ( "errors" "fmt")type Node struct &#123; id int name string next *Node prev *Node&#125;type CircleLink struct &#123; head *Node tail *Node&#125;func (circleLink *CircleLink) orderInsert(node *Node) &#123; // 按Node.id从小到大的顺序，插入链表中 curNode := circleLink.head if curNode.id == 0 &#123; // 第一次插入元素的空链表 circleLink.head = node circleLink.tail = node &#125; else &#123; // 如果node.id比队首id小，则队首让位后移，要保证队首最小。因为取值比较的时候， 是取curNode.next节点与node进行比较，如果满足大小顺序， // 则将node插入curNode和curNode.next之间 if curNode.id &gt; node.id &#123; node.next = curNode curNode.prev = node circleLink.head = node &#125; else &#123; // 如果node.id不比队首id小，则往后取一位，用后一位与node比较，匹配则插入当前位与后一位中间 for &#123; if curNode.next == nil &#123; // 循环到最后一位都没有插入成功，说明node.id比现有链表中的所有node的id都大，则node成为新的队尾 curNode.next = node node.prev = curNode circleLink.tail = node break &#125; if curNode.next.id &gt; node.id &#123; node.next = curNode.next node.prev = curNode curNode.next.prev = node curNode.next = node break &#125; curNode = curNode.next &#125; &#125; &#125;&#125;func (circleLink *CircleLink) pop(id int) (node *Node, err error) &#123; // 弹出链表中指定id的node.这次从链表尾部往首部循环 curNode := circleLink.tail if curNode.id == id &#123; // 要取的是队尾 circleLink.tail = circleLink.tail.prev return curNode, nil &#125; flag := false // 是否查找到的标志 for &#123; if curNode.prev == nil &#123; // 已经从队尾达到队首,全部都没命中 break &#125; if curNode.id == id &#123; flag = true selectNode := curNode if curNode == circleLink.head &#123; // 如果命中的节点正好是队首 if circleLink.head.next != nil &#123; // 如果队首后面还有节点，则第二个节点置为新的首节点，且第二个节点的prev指向置为nil，这样原head没有被引用，内存会释放出来 circleLink.head.next.prev = nil circleLink.head = circleLink.head.next &#125; else &#123; // 如果队首之后没有节点了，那么直接首位都置空 circleLink.head = nil circleLink.tail = nil &#125; &#125; else &#123; // 命中的节点在队中间,则取出命中的节点，并维护好其前后指向关系 curNode.prev.next = curNode.next curNode.next.prev = curNode.prev &#125; return selectNode, nil &#125; curNode = curNode.prev &#125; if !flag &#123; // 如果循环结束还没有查找到，输出信息 return nil, errors.New("node not found") &#125; return&#125;func (circleLink *CircleLink) getLength() (number int) &#123; curNode := circleLink.tail if circleLink.tail.id == 0 || circleLink.tail == nil &#123; // 空链表 return &#125; else &#123; for &#123; number += 1 curNode = curNode.prev if curNode == nil &#123; break &#125; &#125; &#125; fmt.Println("current link length:", number) return number&#125;func (circleLink *CircleLink) list() &#123; if circleLink.tail == nil || circleLink.tail.id == 0 &#123; fmt.Println("link is empty") &#125; else &#123; curNode := circleLink.tail var nodes []*Node for &#123; if curNode == nil &#123; break &#125; //fmt.Print("&lt;==", *curNode) // 从尾部到首部的循环，为了展示链接效果，将节点先加入一个slice中，稍后从slice的后面开始循环展示数据.此时slice是从尾部到首部排序的 nodes = append(nodes, curNode) curNode = curNode.prev &#125; for i := 0; i &lt; len(nodes); i++ &#123; index := len(nodes) - i - 1 // 颠倒一下顺序，现在slice中的顺序和链表顺序恰好是相反的，为了打印的效果，让前面的节点先输出 fmt.Print("&lt;==&gt;", nodes[index]) &#125; &#125; fmt.Println()&#125;func main() &#123; // 约定空链表的header初始id为0，后面加入的实例id必须大于0 var originalHead = Node&#123; id: 0, &#125; var s = CircleLink&#123; head: &amp;originalHead, &#125; var node1 = Node&#123; id: 1, name: "001", &#125; var node2 = Node&#123; id: 2, name: "002", &#125; var node3 = Node&#123; id: 3, name: "003", &#125; var node4 = Node&#123; id: 4, name: "004", &#125; var node5 = Node&#123; id: 5, name: "005", &#125; s.orderInsert(&amp;node4) s.list() s.orderInsert(&amp;node2) s.list() s.orderInsert(&amp;node3) s.list() s.orderInsert(&amp;node5) s.list() s.orderInsert(&amp;node1) s.list() s.getLength() popNode, err := s.pop(3) if err != nil &#123; fmt.Println(err) &#125; fmt.Println("pop node successfully:", *popNode) s.list() /* output: &lt;==&amp;&#123;4 004 &lt;nil&gt; &lt;nil&gt;&#125; &lt;==&amp;&#123;2 002 0xc0000761e0 &lt;nil&gt;&#125;&lt;==&amp;&#123;4 004 &lt;nil&gt; 0xc000076180&#125; &lt;==&amp;&#123;2 002 0xc0000761b0 &lt;nil&gt;&#125;&lt;==&amp;&#123;3 003 0xc0000761e0 0xc000076180&#125;&lt;==&amp;&#123;4 004 &lt;nil&gt; 0xc0000761b0&#125; &lt;==&amp;&#123;2 002 0xc0000761b0 &lt;nil&gt;&#125;&lt;==&amp;&#123;3 003 0xc0000761e0 0xc000076180&#125;&lt;==&amp;&#123;4 004 0xc000076210 0xc0000761b0&#125;&lt;==&amp;&#123;5 005 &lt;nil&gt; 0xc0000761e0&#125; &lt;==&amp;&#123;1 001 0xc000076180 &lt;nil&gt;&#125;&lt;==&amp;&#123;2 002 0xc0000761b0 0xc000076150&#125;&lt;==&amp;&#123;3 003 0xc0000761e0 0xc000076180&#125;&lt;==&amp;&#123;4 004 0xc000076210 0xc0000761b0&#125;&lt;==&amp;&#123;5 005 &lt;nil&gt; 0xc0000761e0&#125; current link length: 5 pop node successfully: &#123;3 003 0xc0000761e0 0xc000076180&#125; &lt;==&amp;&#123;1 001 0xc000076180 &lt;nil&gt;&#125;&lt;==&amp;&#123;2 002 0xc0000761e0 0xc000076150&#125;&lt;==&amp;&#123;4 004 0xc000076210 0xc000076180&#125;&lt;==&amp;&#123;5 005 &lt;nil&gt; 0xc0000761e0&#125; */&#125; 问题从尾部开始查询的操作复杂度依旧与首部开始一样，为O(n)，有没有办法降低复杂度？当然可以，按照自定义链节点串联规则，完全可以使用二分等其他查找的方法降低复杂度，这个后面再补充 环形链表直接以约瑟夫问题，又称丢手帕游戏举例，此场景非常适用于使用环形链表模型，游戏规则见代码顶部注释代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package mainimport ( "fmt" "math/rand")/*约瑟夫问题，又叫丢手帕游戏，若干人围成一圈，从头开始，每次递进若干位随机数，然后出列一人，接着再从出列者的下一位开始继续循环，直到圈内只剩最后一人,游戏结束*/type Child struct &#123; Id int next *Child&#125;type Ring struct &#123; Head *Child Tail *Child&#125;func (ring *Ring) list() &#123; curChild := ring.Head for &#123; fmt.Printf("child %d ==&gt;\t", curChild.Id) if curChild.next == ring.Head &#123; break &#125; curChild = curChild.next &#125; fmt.Println()&#125;func (ring *Ring) play() &#123; // 开始转圈,第一次是从头开始 startIndex := 1 n := 0 startChild := ring.Head for &#123; if startChild.next == startChild &#123; fmt.Printf("游戏结束，圈圈中的最后一位child是%d号\n", startChild.Id) break &#125; n += 1 randInt := rand.Intn(10) curChild := startChild for i := 0; i &lt; randInt-1; i++ &#123; // 在出列child的前一位停下，因为是链表关系，所以需要维护好出列child的前一位的指向关系 curChild = curChild.next &#125; chooseChild := curChild.next if chooseChild == ring.Head &#123; ring.Head = chooseChild.next &#125; curChild.next = chooseChild.next // chooseChild的指针取消，chooseChild会被回收，出列 fmt.Printf("第%d轮,从第%d号开始,前进%d位,出列的是%d号child,game continue!\n", n, startIndex, randInt, chooseChild.Id) startIndex = (randInt + startIndex + 1) % 20 startChild = chooseChild.next // 下一轮，从chooseChild.next节点开始 &#125;&#125;func ringInit(number int) (ring Ring) &#123; // 构建一个指定成员数量的环形链表 for i := 1; i &lt;= number; i++ &#123; var child = Child&#123; Id: i, &#125; fmt.Println(child) if i == 1 &#123; // 插入第一个节点 ring.Head = &amp;child ring.Tail = &amp;child ring.Tail.next = ring.Head // 尾节点下一个指针指向首节点 &#125; else &#123; // 后面的节点，陆续成为新的尾结点 child.next = ring.Head ring.Tail.next = &amp;child ring.Tail = &amp;child &#125; &#125; return&#125;func main() &#123; ring := ringInit(20) fmt.Println(*ring.Head, *ring.Tail) ring.list() ring.play()&#125;/* output: 第1轮,从第1号开始,前进1位,出列的是2号child,game continue! 第2轮,从第3号开始,前进7位,出列的是10号child,game continue! 第3轮,从第11号开始,前进7位,出列的是18号child,game continue! 第4轮,从第19号开始,前进9位,出列的是9号child,game continue! 第5轮,从第9号开始,前进1位,出列的是12号child,game continue! 第6轮,从第11号开始,前进8位,出列的是3号child,game continue! 第7轮,从第0号开始,前进5位,出列的是11号child,game continue! 第8轮,从第6号开始,前进0位,出列的是14号child,game continue! 第9轮,从第7号开始,前进6位,出列的是4号child,game continue! 第10轮,从第14号开始,前进0位,出列的是6号child,game continue! 第11轮,从第15号开始,前进4位,出列的是16号child,game continue! 第12轮,从第0号开始,前进1位,出列的是19号child,game continue! 第13轮,从第2号开始,前进2位,出列的是5号child,game continue! 第14轮,从第5号开始,前进9位,出列的是13号child,game continue! 第15轮,从第15号开始,前进8位,出列的是20号child,game continue! 第16轮,从第4号开始,前进4位,出列的是17号child,game continue! 第17轮,从第9号开始,前进1位,出列的是7号child,game continue! 第18轮,从第11号开始,前进5位,出列的是1号child,game continue! 第19轮,从第17号开始,前进7位,出列的是15号child,game continue! 游戏结束，圈圈中的最后一位child是8号*/]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang算法练习：基于array的队列实现]]></title>
    <url>%2F2019%2F07%2F18%2Fgolang%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0%EF%BC%9A%E5%9F%BA%E4%BA%8Earray%E7%9A%84%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[## 需求队列，很常用的FIFO(先入先出)数据模型，下面尝试使用golang的array数据结构来实现队列模型备注：需求和运行输出结果均已在代码中注释 简单队列代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package mainimport ( "fmt")type SingleQueue struct &#123; Cap int `json:cap` // 容量 Arr [5]int `json:arr` // 成员数组 Head int `json:head` // index编号 Tail int `json:tail` // index编号&#125;func (singleQueue *SingleQueue) append(i int) &#123; // 队尾加入 if singleQueue.isFull() &#123; fmt.Println("Queue is already full") return &#125; if singleQueue.Head == -1 &#123; // 第一次加入元素时 singleQueue.Head = 0 singleQueue.Tail = 0 singleQueue.Arr[singleQueue.Tail] = i &#125; else &#123; singleQueue.Tail += 1 singleQueue.Arr[singleQueue.Tail] = i &#125; fmt.Printf("Append %d ok!\n", i)&#125;func (singleQueue *SingleQueue) isFull() (isFull bool) &#123; if singleQueue.Tail == (singleQueue.Cap - 1) &#123; return true &#125; return&#125;func (singleQueue *SingleQueue) getQueLength() (length int) &#123; // 获取singleQueue当前元素数量 return singleQueue.Tail - singleQueue.Head + 1&#125;func (singleQueue *SingleQueue) isEmpty() (isEmpty bool) &#123; // 队列是否为空 if singleQueue.Tail == singleQueue.Head &#123; return true &#125; return&#125;func (singleQueue *SingleQueue) pop() (elem int) &#123; // 队首弹出 if singleQueue.isEmpty() &#123; fmt.Println("Queue is already empty") return &#125; elem = singleQueue.Arr[singleQueue.Head] singleQueue.Head += 1 fmt.Printf("Pop %d ok!\n", elem) return&#125;func (singleQueue *SingleQueue) list() &#123; // 列出singleQueue当前所有的元素 fmt.Println("Here are all elements in this queue:") for i := singleQueue.Head; i &lt;= singleQueue.Tail; i++ &#123; fmt.Printf("index[%d],value=%d\n", i, singleQueue.Arr[i]) &#125;&#125;func main() &#123; var s = SingleQueue&#123; Cap: 5, Head: -1, Tail: -1, &#125; s.append(1) s.append(2) s.append(3) s.append(4) s.append(5) s.append(6) // Queue is already full s.list() fmt.Println(s.getQueLength()) // 5 s.pop() // Pop 1 ok! fmt.Println(s.getQueLength()) // 4 s.append(6) // Queue is already full /* 分析:单向队列，队首弹出后，腾出了空间，却无法给新的元素加入使用，因为容量评估无法感知head的偏移，因此，实用性很低，改进呢？改为可循环队列 */&#125; 问题array容量是有限的，这个队列存在空间浪费的问题，拥有空闲空间却无法再插入元素。怎么解决? 对代码简单改造,实现为循环队列即可，在关键位置进行取模运算。 循环队列代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package mainimport ( "fmt")type CircleQueue struct &#123; Cap int `json:cap` // 容量 Arr [5]int `json:arr` // 成员数组 Head int `json:head` // index编号 Tail int `json:tail` // index编号&#125;func (circleQueue *CircleQueue) append(i int) &#123; // 队尾加入 if circleQueue.isFull() &#123; fmt.Println("Queue is already full") return &#125; if circleQueue.Head == -1 &#123; // 第一次加入元素时 circleQueue.Head = 0 circleQueue.Tail = 0 circleQueue.Arr[circleQueue.Tail] = i &#125; else &#123; circleQueue.Tail = (circleQueue.Tail + 1) % circleQueue.Cap circleQueue.Arr[circleQueue.Tail] = i &#125; fmt.Printf("Append %d ok!\n", i)&#125;func (circleQueue *CircleQueue) isFull() (isFull bool) &#123; if (circleQueue.Tail+1)%circleQueue.Cap == circleQueue.Head &#123; // 环形队列，尾部下一个是首部，则说明空间已用完，因为是环形的，所以需要取模后再比较 return true &#125; return&#125;func (circleQueue *CircleQueue) getQueLength() (length int) &#123; // 获取circleQueue当前元素数量 length = ((circleQueue.Tail + circleQueue.Cap - circleQueue.Head) % circleQueue.Cap) + 1 return&#125;func (circleQueue *CircleQueue) isEmpty() (isEmpty bool) &#123; // 队列是否为空 if circleQueue.Tail == circleQueue.Head &#123; return true &#125; return&#125;func (circleQueue *CircleQueue) pop() (elem int) &#123; // 队首弹出 if circleQueue.isEmpty() &#123; fmt.Println("Queue is already empty") return &#125; circleQueue.Head = (circleQueue.Head + 1) % circleQueue.Cap elem = circleQueue.Arr[circleQueue.Head] fmt.Printf("Pop %d ok!\n", elem) return&#125;func (circleQueue *CircleQueue) list() &#123; // 列出circleQueue当前所有的元素 //fmt.Println(circleQueue.Head) fmt.Println("Here are all elements in this queue:") for i := 0; i &lt; circleQueue.getQueLength(); i++ &#123; index := (i + circleQueue.Head) % circleQueue.Cap fmt.Printf("index[%d],value=%d\n", index, circleQueue.Arr[index]) &#125;&#125;func main() &#123; var c = CircleQueue&#123; Cap: 5, Head: -1, Tail: -1, &#125; c.append(1) c.append(2) c.append(3) c.append(4) c.append(5) c.append(6) // Queue is already full c.list() fmt.Println(c.getQueLength()) // 5 c.pop() // Pop 1 ok! fmt.Println(c.getQueLength()) // 4 c.append(6) // Append 6 ok! c.list() // 6加入了队尾 /* 分析:环形队列，弹出后的空间可循环复利用，符合实际使用价值 */&#125; 问题循环队列更满足实际使用需求，但毕竟array容量有限，一次申请太大的容量也很浪费资源，怎么解决这个问题？答曰：链表，动态加减元素，独立的内存空间，不需要一次性申请大量内存。链表实现参考下篇]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十六): VXLAN和Flannel]]></title>
    <url>%2F2019%2F07%2F17%2Fk8s(%E5%8D%81%E5%85%AD)-VXLAN%E5%92%8CFlannel.html</url>
    <content type="text"><![CDATA[## 前言目前k8s中比较常用的cni有calico、flannel、kube-router等，个人比较熟悉的是kube-router的bgp纯直通方案，flannel则是另一款使用较多的比较简单易用的方案，这里着重了解一下flannel的vxlan这一常用模式 VXLAN在了解flannel之前，有必要掌握vxlan的基础知识。首先来看看vxlan协议的报文封装格式： 说明：vxlan，给数据中心大二层网络使用，何为大二层，又为何需要vxlan？ 1.vlan数量有限vlan的tag位一个4个字节，其中12bit是给vlan id使用，也即是一共有4096个vlan id支持，在大型数据中心中不够用，但这个问题其实QinQ（IEEE 802.1 ad）标准可以解决，重点原因是因为第二点 2.mac地址表限制虚拟化/容器化场景下，一台物理机内部有众多虚拟网卡，而外部的二层设备(交换机)是将mac地址都缓存入内存中的，且交换机内存空间一般较小，在虚拟化的数据中心中，如果mac表满了，会出现大量的广播泛洪，造成极大的网络资源和计算资源的浪费 3.灵活部署限制如果虚机和物理机之间使用桥接网络，那么虚拟机则同样受物理机网络的限制，只能使用那一个vlan的地址，不同vlan下的虚机受制于vlan无法顺畅的进行互相迁移，容易主机的资源利用率的不均衡。虽然可以通过物理机与交换设备全部通过trunk的方式连接来解决vlan切换这个问题，但是这样无疑会形成一个巨大的传统二层网络空间，相应的BUM（Broadcast，Unknown Unicast，Multicast）风暴和交换机MAC地址表的问题也会随之产生，这是无法接受的。 那么vxlan是怎么解决上面几个问题的呢？ 从上面的报文封装图可以看出，vxlan外层(二层头部、三层头部)继续沿用现有标准，只是在传输层，使用了udp封装，udp封装之内，就是vxlan的协议封装。VXLAN Tunnel EndPoint(简称VTE, 是指支持vxlan封装、拆包的交换设备，一般可理解为承载着虚拟机的物理机)，通过vxlan header内部封装的信息，拆包后直接将original layer 2 frame数据，转发给相应的虚拟网卡。 如此，在外部的网络设备层面，vxlan保持着传统的通信方式，即各个VTE之间以UDP的方式互相通信；而在VTE层面，则通过报文封装，实现了内部众多的虚拟网卡mac地址之间的数据交换： 123其一，24bit的VNID标识，实现了1600多万个不同二层子网的区分；其二，通过报文封装的格式，其三，通过VXLAN的封装，实现了虚拟网卡之间越过传统三层网络，以特殊的二层数据帧交换的方式通信，走的是一套独立于物理网络（underlay network）的overlay network，在物理网络上可以保持不变，但是同时，虚拟机的部署和迁移，又不用受物理网络的限制，有利于物理资源利用率维持均衡 以上也就是为什么vxlan又称为虚拟大二层网络，有了以上基础铺垫，下面来简述一下flannel 的vxlan实现的工作具体流程。 Flannel按照惯例，先上流程图： 步骤详解：源端：1）源容器veth0向目标容器发送数据，根据容器内的默认路由，数据首先发送给宿主机的docker0网桥 2）宿主机docker0网桥接受到数据后，宿主机查询路由表，pod相关的路由都是交由flannel.1网卡，因此，将其转发给flannel.1虚拟网卡处理 3）flannel.1接受到数据后，查询etcd数据库，获取目标pod网段对应的目标宿主机地址、目标宿主机的flannel网卡的mac地址、vxlan vnid等信息。然后对数据进行udp封装如下： 123456udp头封装：source port 8285，target port 8285udp内部封装： 1.vxlan封装：vxlan vnid等信息 2.original layer 2 frame封装：source &#123;源 flannel.1网卡mac地址&#125; target&#123;目标flannel.1网卡的mac地址&#125;完成以上udp封装后，将数据转发给物理机的eth0网卡 4）宿主机eth0接收到来自flannel.1转发的udp包，还需要将其封装成正常的通信用的数据包，为它加上通用的ip头、二层头，这项工作在由linux内核来完成。封装后： 123456Ethernet Header的信息：source:&#123;源宿主机机网卡的MAC地址&#125;target:&#123;目标宿主机网卡的MAC地址&#125;IP Header的信息：source:&#123;源宿主机网卡的IP地址&#125;target:&#123;目标宿主机网卡的IP地址&#125; 通过此次封装，一个真正可用的数据包就封装完成，可以通过物理网络传输了。 目标端：5）目标宿主机的eth0接收到数据后，对数据包进行拆封，拆到udp层后，将其转发给8285端口的flannel进程 6）目标宿主机端flannel拆除udp头、vxlan头，获取到内部的原始数据帧，在原始数据帧内部，解析出源容器ip、目标容器ip，重新封装成通用的数据包，查询路由表，转发给docker0网桥； 7）最后，docker0网桥将数据送达目标容器内的veth0网卡，完成容器之间的数据通信。 总结：通过对vxlan设计思想和flannel的落地方案的互相印证，对这两者会有更为深刻的理解，同时，不难发现，flannel性能表现不佳的原因很可能是因为反复拆封包过程、etcd查询这几个步骤带来的额外开销。 参考：https://blog.csdn.net/u010039418/article/details/90451081https://www.cnblogs.com/cwind/p/10085146.htmlhttps://events.static.linuxfound.org/sites/events/files/slides/2013-linuxcon.pdf]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十五)、Kubernetes v1.14多master集群部署]]></title>
    <url>%2F2019%2F06%2F18%2Fk8s(%E5%8D%81%E4%BA%94)%E3%80%81Kubernetes%20v1.14%E5%A4%9Amaster%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[## 前言距离上一次搭1.9版本，已经过去一年了，这一年时间里，kubernetes已经迭代到了v1.14.3版本了，为了体验新版本的特性，今天来搭建最新版的集群，部署步骤与之前的一致，CNI kube-router经过了线上一年的验证，这里继续沿用kube-router的bgp直通网络方案。具体步骤与之前的v1.9版本几乎完全一致，本篇只列出差异部分，详情可参考此前的文章：k8s（一）、 1.9.0高可用集群本地离线部署记录 规划信息 应用安装kube组件安装12345678910111213141516171819## centos:# 配置yum源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repobaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg# 安装最新版yum install -y kubelet kubectl kubeadm## ubuntu apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat &gt; /etc/apt/sources.list.d/kubernetes.list &lt;&lt; EOF deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get updateapt-get -y install kubelet kubectl kubeadm etcd版本注意:特别注意，v1.9 k8s用的是v3.10版本的etcd，现在最新的v1.14版本k8s支持的etcd最低版本为3.12.x，因此这里etcd的版本替换成了3.13版本的1wget https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz 安装步骤与此前一致，不再复述 集群初始化1.14版本的初始化配置文件与此前以前产生了较大的差异，这里单独列出来，如下是kubeadm v1.14初始化使用的配置文件: 123456789101112131415161718192021222324252627282930313233# cat kubeadmin-config.yamlapiVersion: kubeadm.k8s.io/v1beta1kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: "192.168.8.15" bindPort: 6443---apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationetcd: external: endpoints: - "https://192.168.8.19:2379" - "https://192.168.8.20:2379" - "https://192.168.8.21:2379" caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pemnetworking: podSubnet: "172.36.0.0/16" serviceSubnet: 10.112.0.0/12apiServer: certSANs: - "host008019" - "host008020" - "host008021" - "192.168.8.19" - "192.168.8.20" - "192.168.8.21" - "192.168.8.15"kubernetesVersion: "v1.14.3"imageRepository: "registry.cn-hangzhou.aliyuncs.com/google_containers" # 如果不能科学上网，添加一行这个后，kubeadm初始化镜像会从阿里云拉取 可配置选项很多，本样本仅作参考，具体可以查阅官方文档：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/ master初始化： 1kubeadm init --config kubeadmin-config.yaml 其余节点照做，参考1.9部署文档 集群检查12345root@host008019:~# kubectl get nodesNAME STATUS ROLES AGE VERSIONhost008019 Ready master 14h v1.14.3host008020 Ready master 14h v1.14.3host008021 Ready master 14h v1.14.3 总结kubeadm作为官方推荐的GA，在经历了这么多版本后，使得集群的安装、升级等操作变得极为简便，只要仔细、逐步按步骤操作，一定没有问题可以部署成功。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[inux TCP连接失败(不回复SYN,ACK)问题分析]]></title>
    <url>%2F2019%2F06%2F10%2Flinux%20TCP%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5(%E4%B8%8D%E5%9B%9E%E5%A4%8DSYN%2CACK)%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[## 问题现象在某一台服务器无法访问另一台服务器的https服务，telnet检测端口发现端口无法连接:12[root@020113 ~]# telnet 192.168.5.27 443Trying 192.168.5.27... 问题分析第一步：网络检查同网段其他主机访问服务端是正常的，因此网络问题可以排除 第二步：查看防火墙步骤略，这里检查双方iptables规则均无问题 第三步：抓包分析服务端抓包并将结果保存,因为443端口连接较多，有一些其他干扰，因此抓22端口: root@005027 tcpdump host 192.168.20.113 and port 22 -w dump.pcap 客户端同时连接服务端的22端口: 1[root@020113 ~]# telnet 192.168.5.27 22 dump.pcap文件拿到本地用wireshark打开分析：抓包的结果可以看出，客户端(20.113)发起SYN请求后，即使多次重传，服务端也没有回复过带SYN, ACK标志位的响应，那么看来问题出现在服务端。这时隐隐有了一些猜测，很有可能是之前碰到过的一个问题导致的，在此前的文章也有写过:Kubernetes踩坑(二): Service IP(LVS)间断性TCP连接故障排查 为了印证自己的猜测，检查一下状态:12root@005027:~# netstat -s | grep timestamp 2984 packets rejects in established connections because of timestamp 果然，提示有一些包因为时间戳问题丢弃了。 解决:查看内核参数配置: 1net.ipv4.tcp_tw_recycle = 1 果然是打开状态，将其改成0, sysctl -p使其生效后，问题解决。 深挖为什么会出现这种情况呢？这里再次回顾一下linux内核时间戳相关的3个参数: 12345678# 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_reuse = 1# 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。net.ipv4.tcp_tw_recycle = 1# tcp缓存时间戳,RFC1323中描述了，系统缓存每个ip最新的时间戳，后续请求中该ip的tcp包中的时间戳如果小于缓存的时间戳(即非最新的数据包)，即视为无效，相应的数据包会被丢弃，而且是无声的丢弃。在默认情况下，此机制不会有问题，但在nat模式时,不同的ip被转换成同一个ip再去请求真正的server端，在server端看来，源ip都是相同的，且此时包的时间戳顺序可能不一定保持递增，由此会出现丢包的现象，因此，如果是nat模式工作，建议关闭此选项。net.ipv4.tcp_timestamps = 0 说明：设计这几个参数是为什么呢？在tcp四次挥手完成之后，服务端会有一个TIME_WAIT的时间等待释放sock连接的端口，默认为2*MSL(60s)，而端口资源是有限的，在繁忙的服务器了，等待这么长的时间是非常低效的，因此为了加速这个效率，linux内核设计了tcp_tw_reuse和tcp_tw_recycle这两个参数，这两个参数一般都是一起使用的，开启后允许time_wait状态(后面简写tw)的端口被回收并重用。但是，回收重用容易引发一个问题，就是可能导致回收的端口收到了来自之前的对端连接的数据，因此为了避免这个问题，又设计了另一个参数tcp_timestamps，在上面的注释中也说明了，非最大时间戳的包会丢弃。 因此，郑重建议： 这3个参数建议不要同时开启，要么干脆不开启回收重用，要开启的话，建议不要开始时间戳标记。]]></content>
      <tags>
        <tag>Network</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang基础学习: Struct和Json]]></title>
    <url>%2F2019%2F05%2F03%2FGolang%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0_%20Struct%E5%92%8CJson.html</url>
    <content type="text"><![CDATA[前言结构体是将多个任意类型的命名变量组合在一起的聚合数据类型，通过结构体，可以多维度/方面的聚合逻辑数据，形成一个整体，其中的这些命名变量叫做结构体的成员。 Struct声明：12345678910// 声明一个结构体类型type Employee struct &#123; ID int Name string Address string DoB string Position string Salary int ManagerID int&#125; 结构体成员(变量名称)推荐使用大写字母开头，目的是为了方便与json进行转换，具体说明可以看后面的部分。 简单使用123456789101112131415161718// 声明结构体类型的实例，三种方式,前两种声明的都是指针,后面一种是直接生产对象本身，初始属性值都是零值:var bob *Employee = new(Employee) // &amp;&#123;0 0 0&#125;var claier *Employee = &amp;Employee&#123;&#125; // &amp;&#123;0 0 0&#125;var alice Employee // &#123;0 0 0&#125;fmt.Println(bob, claier, alice)// 访问和赋值alice.Salary = 3000fmt.Println(alice, alice.Salary) // &#123;0 3000 0&#125; 3000// 通过指针访问和赋值position := &amp;bob.Position*position = "Senior"fmt.Println(bob) // &amp;&#123;0 Senior 0 0&#125;c := &amp;claier(*c).Name = "claier"fmt.Println(*c) // &amp;&#123;0 alice Senior 3000 0&#125; 构造和比较123456789101112131415161718192021222324/* 构造函数: struct结构体默认没有构造函数，需要自己写一个*/ func NewEmployee(id int, name string) *Employee &#123; return &amp;Employee&#123;ID: id, Name: name&#125; &#125;david := NewEmployee(4, "david")fmt.Println(*david) // &#123;4 david 0 0&#125;/* struct之间的比较*/// struct是可以直接进行比较的，编译不会报错。此前学习过，map的key必须是可以直接比较的类型，因此struct可以作为map的keyfmt.Println(alice == *bob) // falsetype url struct &#123; host string port int&#125;pageViews := make(map[url]int)pageViews[url&#123;"www.baidu.com", 443&#125;]++ 结构体嵌套123456789101112131415161718192021222324252627282930313233/* struct嵌套 struct支持多层嵌套，且可以将一个命名结构体当做另一个结构体的匿名成员使用,下面举一个例子： 层级关系为: 学生 -&gt; 班级 -&gt; 年级 -&gt; 学校*/// 基本用法type School struct &#123; name string&#125;type Grade struct &#123; level int school School&#125;type Classes struct &#123; grade Grade id int&#125;type Student struct &#123; name string age int classes Classes&#125;var xiaoming Studentxiaoming.name = "xiaoming"xiaoming.classes.grade.school.name = "NO.1 School"xiaoming.classes.grade.level = 3xiaoming.classes.id = 2xiaoming.age = 8fmt.Println(xiaoming) // &#123;xiaoming 8 &#123;&#123;3 &#123;NO.1 School&#125;&#125; 2&#125;&#125; 匿名结构体嵌套123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/*匿名结构体嵌套： 匿名用法,结构体内的不带名称的结构体成员，该成员必须是其他类型的对象或指针。使用匿名成员时，可以以穿透的方式给属性赋值，即当前结构体不包含该 \n 属性名时，则往向上层级的结构体上查找该属性名，最近的匹配属性将获得赋值(最近原则)。例如下方，xiaoli.name的值赋给了Student2.name， 而 \n xiaoli.classes.grade.school.name，则可以通过xiaoli.School.name来赋值。虽然是比直接用法简化了一些，但感觉层次还是不够清晰，谨慎使用。*/type Grade2 struct &#123; level int School&#125;type Classes2 struct &#123; Grade2 id int&#125;type Student2 struct &#123; name string age int Classes2&#125;var xiaoli Student2xiaoli.name = "xiaoli"xiaoli.School.name = "NO.2 School" // 等价于上面的 xiaoli.classes.grade.school.namexiaoli.level = 4 // 等价于上面的 xiaoli.classes.grade.levelxiaoli.id = 3 // 等价于上面的 xiaoli.classes.idxiaoli.age = 9fmt.Println(xiaoli) // &#123;xiaoli 9 &#123;&#123;4 &#123;NO.2 School&#125;&#125; 3&#125;&#125;// 匿名结构体赋值xiaohong := Student2&#123;"xiaohong", 11, Classes2&#123;Grade2&#123;5, School&#123;"NO.2 School"&#125;&#125;, 4&#125;&#125;// 等价于上方xiaotian := Student2&#123; "xiaotian", 12, Classes2&#123; Grade2&#123; 6, School&#123; "NO.2 School", &#125;, &#125;, 5, &#125;,&#125;fmt.Println(xiaohong) // &#123;xiaohong 11 &#123;&#123;5 &#123;NO.2 School&#125;&#125; 4&#125;&#125;fmt.Println(xiaotian) // &#123;xiaotian 12 &#123;&#123;6 &#123;NO.2 School&#125;&#125; 5&#125;&#125; JSONJson是非常常见的数据接收和发送的格式化标准，其逻辑结构与struct非常相似，go提供标准库encoding/json来支持对json数据的处理和转换操作。下面举例说明struct和json之间的转换： struct =&gt; json序列化12345678910111213141516171819type Movie struct &#123; Name string Year int actor []string&#125;var movies = []Movie&#123; &#123;Name: "movie1", Year: 1999, actor: []string&#123;"zhangsan", "lisi"&#125;&#125;, &#123;Name: "movie2", Year: 2000, actor: []string&#123;"wangwu", "zhaoliu"&#125;&#125;,&#125;data, err := json.Marshal(movies)if err != nil &#123; fmt.Println("Json parse error") return&#125;fmt.Println(movies) // [&#123;movie1 1999 [zhangsan lisi]&#125; &#123;movie2 2000 [wangwu zhaoliu]&#125;]fmt.Printf("%s\n", data) // [&#123;"Name":"movie1","Year":1999&#125;,&#123;"Name":"movie2","Year":2000&#125;]//这里的打印结果可以看出一个问题: actor属性并没有体现出来，这是因为JSON在处理struct转换的时候，只会处理属性名为大写的属性，小写的直接跳过不处理。 上面这个例子中，json序列化时会忽略小写开头的结构体成员，如果想要在json处理时显示成员为小写或别名，可以显示地为其添加标签： 1234567891011type Movie2 struct &#123; Name string Year int Actor []string `json:"actor"`&#125;var movies2 = []Movie2&#123; &#123;Name: "movie1", Year: 1999, Actor: []string&#123;"zhangsan", "lisi"&#125;&#125;, &#123;Name: "movie2", Year: 2000, Actor: []string&#123;"wangwu", "zhaoliu"&#125;&#125;,&#125;data2, _ := json.Marshal(movies2)fmt.Printf("%s\n", data2) // [&#123;"Name":"movie1","Year":1999,"actor":["zhangsan","lisi"]&#125;,&#123;"Name":"movie2","Year":2000,"actor":["wangwu","zhaoliu"]&#125;] 这时输出的成员actor就是小写了，同理，标签这里也可以写为别名。注意:json:&quot;actor&quot;，冒号中间不要有空格。 json =&gt; struct反序列化123456789101112131415 // 沿用上面例子的变量和结构体//var movies3 Movie2// 这里犯了一个错误，导致运行报错.记一笔:// 将变量定义为Movie2结构体类型之后去接收这里的data2的json反序列化数据时，报错:json: cannot unmarshal array into Go value of type main.Movie2// 原因为: 当json串处理之后的结果是数组类型的时候,不能直接赋值给struct类型的值,因此，这里定义的movies3变量应该是[]Movie2的slice类型。go语言之中类型必须严格匹配，注意!// 另外，除了这里的unmarshal方式之外，还有一种json.Decoder的方式可以依次从字节流里逐个解码多个json对象，这个在后面的例子json_template.go中做解释。var movies3 []Movie2 err2 := json.Unmarshal(data2, &amp;movies3)if err2 != nil &#123; fmt.Println("Json parse error", err2)&#125;fmt.Printf("%v\n", movies3) // [&#123;movie1 1999 [zhangsan lisi]&#125; &#123;movie2 2000 [wangwu zhaoliu]&#125;]fmt.Println(movies3[0].Year) // 1999 这里没注意踩了一个坑，data2在序列化之前是[]Movie2的slice结构，在声明了一个Movie2 struct类型的变量指针去接收json反序列化的数据时抛出了异常，这里应该同样使用[]Movie2的slice类型变量. 这里使用的Unmarshal方法适用的对象为已经在单个或已内存中的json对象，而涉及文件(socket/http等)读取的byte流json数据，则适合使用另一种反序列化方式json.Decoder(),这种方式不会一次把字节流读取进内存中，而是会逐个读取，适用于这些场景。 使用举例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package main/*通过Github issue api获取到json数据并解析为go struct的demo*/import ( "encoding/json" "fmt" "log" "net/http" "net/url" "os" "strings" "time")const IssueURL = "https://api.github.com/search/issues"type IssuesSearchResult struct &#123; TotalCount int `json:"total_count"` Items []*Issue&#125;type Issue struct &#123; Number int HTMLURL string `json:"html_url"` Title string State string User *User CreateAt time.Time `json:"created_at"` Body string&#125;type User struct &#123; Login string HTMLURL string `json:"html_url"`&#125;func SearchIssues(terms []string) (*IssuesSearchResult, error) &#123; q := url.QueryEscape(strings.Join(terms, " ")) // slice组合为字符串 resp, err := http.Get(IssueURL + "?q=" + q) if err != nil &#123; return nil, err &#125; if resp.StatusCode != http.StatusOK &#123; resp.Body.Close() return nil, err &#125; var result IssuesSearchResult // 使用json.NewDecoder().decode()方法来解码整个字节流,这里不使用unmarshal方法了，unmarshal方法只能解码单个对象。 if err := json.NewDecoder(resp.Body).Decode(&amp;result); err != nil &#123; resp.Body.Close() return nil, err &#125; resp.Body.Close() return &amp;result, nil&#125;func main() &#123; res, err := SearchIssues(os.Args[1:]) // 运行命令为: go run ./json_template.go repo:golang/go is:open json decoder 后方为参数,可以组合为url去浏览器打开对比 if err != nil &#123; log.Fatal(err) &#125; fmt.Println(res.Items[0].Title) fmt.Printf("%d issues: \n", res.TotalCount) for _, item := range res.Items &#123; fmt.Printf( "IssueID: #%d\tTime: %s\tUser: %s\tTitle: %s\n", item.Number, item.CreateAt, item.User.Login, item.Title) &#125;&#125; 运行结果： 1234567891011121314151617181920212223242526272829303132encoding/json: second decode after error impossible35 issues: IssueID: #31701 Time: 2019-04-26 20:50:17 +0000 UTC User: lr1980 Title: encoding/json: second decode after error impossibleIssueID: #31789 Time: 2019-05-01 18:30:06 +0000 UTC User: mgritter Title: encoding/json: provide a way to limit recursion depthIssueID: #29688 Time: 2019-01-11 17:07:05 +0000 UTC User: sheerun Title: proposal: encoding/json: add InputOffset to json decoderIssueID: #29686 Time: 2019-01-11 16:38:04 +0000 UTC User: sheerun Title: json: Add InputOffset for stream byte offset accessIssueID: #28923 Time: 2018-11-22 13:50:18 +0000 UTC User: mvdan Title: encoding/json: speed up the decoding scannerIssueID: #11046 Time: 2015-06-03 19:25:08 +0000 UTC User: kurin Title: encoding/json: Decoder internally buffers full inputIssueID: #29035 Time: 2018-11-30 11:21:31 +0000 UTC User: jaswdr Title: proposal: encoding/json: add error var to compare the returned error when using json.Decoder.DisallowUnknownFields()IssueID: #30301 Time: 2019-02-18 19:49:27 +0000 UTC User: zelch Title: encoding/xml: option to treat unknown fields as an errorIssueID: #30701 Time: 2019-03-09 12:16:34 +0000 UTC User: LouAdrien Title: encoding/json: ignore tag "-" not working on embedded sub structure when decodingIssueID: #12001 Time: 2015-08-03 19:14:17 +0000 UTC User: lukescott Title: encoding/json: Marshaler/Unmarshaler not stream friendlyIssueID: #16212 Time: 2016-06-29 16:07:36 +0000 UTC User: josharian Title: encoding/json: do all reflect work before decodingIssueID: #28143 Time: 2018-10-11 07:08:25 +0000 UTC User: Carpetsmoker Title: proposal: encoding/json: add "readonly" tagIssueID: #26946 Time: 2018-08-12 18:19:01 +0000 UTC User: deuill Title: encoding/json: clarify what happens when unmarshaling into a non-empty interface&#123;&#125;IssueID: #5901 Time: 2013-07-17 16:39:15 +0000 UTC User: rsc Title: encoding/json: allow override type marshalingIssueID: #27179 Time: 2018-08-23 18:21:32 +0000 UTC User: lavalamp Title: encoding/json: no way to preserve the order of map keysIssueID: #21823 Time: 2017-09-09 21:43:31 +0000 UTC User: 243083df Title: encoding/xml: very low performance in xml parserIssueID: #22752 Time: 2017-11-15 23:46:13 +0000 UTC User: buyology Title: proposal: encoding/json: add access to the underlying data causing UnmarshalTypeErrorIssueID: #14750 Time: 2016-03-10 13:04:44 +0000 UTC User: cyberphone Title: encoding/json: parser ignores the case of member namesIssueID: #20754 Time: 2017-06-22 14:19:31 +0000 UTC User: rsc Title: encoding/xml: unmarshal only processes first XML elementIssueID: #28189 Time: 2018-10-13 16:22:53 +0000 UTC User: adnsv Title: encoding/json: confusing errors when unmarshaling custom typesIssueID: #7213 Time: 2014-01-27 00:23:01 +0000 UTC User: davecheney Title: cmd/compile: escape analysis oddityIssueID: #7872 Time: 2014-04-26 17:47:25 +0000 UTC User: extemporalgenome Title: encoding/json: Encoder internally buffers full outputIssueID: #20528 Time: 2017-05-30 11:45:14 +0000 UTC User: jvshahid Title: net/http: connection reuse does not work happily with normal use of json packageIssueID: #17609 Time: 2016-10-26 16:07:27 +0000 UTC User: nathanjsweet Title: encoding/json: ambiguous fields are marshalledIssueID: #19636 Time: 2017-03-21 12:25:10 +0000 UTC User: josselin-c Title: encoding/base64: decoding is slowIssueID: #22816 Time: 2017-11-20 13:12:06 +0000 UTC User: ganelon13 Title: encoding/json: include field name in unmarshal error messages when extracting time.TimeIssueID: #21092 Time: 2017-07-19 23:11:40 +0000 UTC User: trotha01 Title: encoding/json: unmarshal into slice reuses element data between len and capIssueID: #28941 Time: 2018-11-25 14:06:38 +0000 UTC User: mvdan Title: cmd/compile: teach prove about slice expressionsIssueID: #15808 Time: 2016-05-24 02:13:10 +0000 UTC User: randall77 Title: cmd/compile: loads/constants not lifted out of loopIssueID: #28952 Time: 2018-11-26 09:59:26 +0000 UTC User: mvdan Title: cmd/compile: consider teaching prove about unexported integer fields 总结Golang结构体有许多特性，且与json结合紧密，日后会经常用到，需要熟练掌握。]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang基础学习: array和slice对比和使用]]></title>
    <url>%2F2019%2F04%2F27%2FGolang%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0_%20array%E5%92%8Cslice%E5%AF%B9%E6%AF%94%E5%92%8C%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言在golang中，常见的序列型数据类型有array和slice这两种，但array因为其固定长度的限制，在实际使用中用得不多，slice则更为常用。下面简单介绍和对比一下这两种相似却又有很多差异的数据类型。 Array：概念: 在golang中，数组由相同类型的元素组成的具有固定长度的一种序列型复合数据类型。 声明和使用：12345678910111213141516171819202122232425262728293031323334353637383940package mainimport "fmt"/*数组： array在go里是固定长度、每个元素类型必须相同的，因此使用较少，slice使用反而较多*/func main() &#123; var a [3]int // 3个整数的数组 fmt.Println(a) // [0 0 0] fmt.Println(a[0]) for i, v := range a &#123; fmt.Printf("%d %d\n", i, v) // 打印索引和元素 &#125; for _, v2 := range a &#123; fmt.Printf("%d\n", v2) // 打印元素 &#125; // 默认情况下，数组创建后元素都是零值，可以使用数组字面量来初始化一个数组的元素 var q [3]int = [3]int&#123;1, 2, 3&#125; fmt.Println(q) // [1 2 3] var q2 [3]int = [3]int&#123;1, 2&#125; fmt.Println(q2) // [1 2 0] var q3 = [3]int&#123;1, 2, 3&#125; fmt.Println(q3) // 缩写用这个方式比较简洁 /* 数组长度是数组类型的一部分，所以q4和q5是不同的类型. 数组的长度必须是常量表达式，因此数组长度在声明时必须确定 */ q4 := [...]int&#123;1, 2, 3, 4&#125; q5 := [...]int&#123;1, 2, 3, 4, 5&#125; fmt.Printf("Type: %T\n", q4) // Type: [4]int fmt.Printf("Type: %T\n", q5) // Type: [5]int //q4 = q5 // 编译错误，赋值失败，不同类型不能赋值&#125; 总结 在golang中array的长度是固定的，声明时必须以数值形式明确指定或者以’…’形式间接指定元素长度，且不同的长度的array类型不同不能转换，因此使用场景有限。 Slice概念在golang中，数组由相同类型的元素组成的可变长度的一种序列型复合数据类型。在理解上来说，slice和array唯一的区别是长度可变 声明和使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131package mainimport ( "bytes" "fmt")/* slice数据类型在声明时如果不指定len/cap，则slice的cap/len值默认以声明时的数组的长度为准。 len: 长度。slice包含的元素的个数，可变 cap: 容量。slice当前可接纳的元素的个数，根据了len值动态变化*/func main() &#123; // 声明方式1 var b []int // 声明一个空slice，默认len和cap的值都为0。声明长度为0的array: var b [0]int，这两者声明方式非常相似 fmt.Printf("Slice a , length: %d cap: %d content: %v\n", len(b), cap(b), b) // Slice a , length: 0 cap: 0 content: [] // 声明方式2 a := []int&#123;1, 2, 3, 4, 5&#125; fmt.Printf("Slice a , length: %d cap: %d content: %v\n", len(a), cap(a), a) // Slice a , length: 5 cap: 5 content: [1 2 3 4 5] a = append(a, 6) // 追加元素 fmt.Printf("Slice a , length: %d cap: %d content: %v\n", len(a), cap(a), a) // Slice a , length: 6 cap: 10 content: [1 2 3 4 5 6] // 声明方式3 c := make([]int, 5, 10) // make(type, len[, cap]) fmt.Printf("Slice a , length: %d cap: %d content: %v\n", len(c), cap(c), c) // Slice a , length: 5 cap: 10 content: [0 0 0 0 0] // copy方法 d := []int&#123;1, 2&#125; e := []int&#123;3, 4, 5, 6&#125; copy(e, d) // copy(to,from) ,第一个参数是目标slice，第二个参数是源slice fmt.Println(d, e) // [1 2] [1 2 5 6] f := []int&#123;3, 4, 5, 6&#125; copy(d, f) // copy(to,from) ,第一个参数是目标slice，第二个参数是源slice fmt.Println(d, f) // [3 4] [3 4 5 6] // slice的比较 g := []string&#123;"a", "b"&#125; h := []string&#123;"a", "b", "c"&#125; fmt.Println(g != nil) // true //fmt.Println(g == h) // 这里会编译错误，slice没有==比较方法 // 只能自定义方法来比较两个slice，例如下方自定义的equal方法: fmt.Println(equal(g, h)) // 检查slice是否为空 fmt.Println(len(g) != 0) fmt.Println(g != nil) // 不推荐用这个方法，因为即使g != nil,g也有可能是空值 /* ------------------------------------Slice增加/删除/反转元素/转换格式等常用操作------------------------------------ */ // 尾部操作 j := []int&#123;1, 2, 3, 4, 5&#125; fmt.Println(append(j, 6)) // [1 2 3 4 5 6] 尾部插入 top := j[len(j)-1] fmt.Println(top) // 5 获取尾部(栈顶)元素 j = j[:len(j)-1] // 弹出尾部元素 fmt.Println(j) // [1 2 3 4] // slice中间移除/插入操作，没有自带函数，需要自己写方法 j = sliceRemove(j, 2) fmt.Println(j) // [1 2 4] j = sliceInsert(j, 2, 3) fmt.Println(j) // [1 2 3 4] // 反转元素 j = sliceReverse(j) fmt.Println(j) // [4 3 2 1] // 转换为字符串格式 fmt.Println(intsToString([]int&#123;1, 2, 3&#125;)) // [1 &amp; 2 &amp; 3]&#125;func sliceRemove(slice []int, i int) []int &#123; // 移除slice的指定索引位置的元素 copy(slice[i:], slice[i+1:]) return slice[:len(slice)-1]&#125;func sliceInsert(slice []int, i int, n int) []int &#123; // 将元素插入到slice的指定索引位置 sub := []int&#123;n&#125; for _, e := range slice[i:] &#123; sub = append(sub, e) &#125; copy(slice[i:], sub) slice = append(slice, sub[len(sub)-1]) return slice&#125;func sliceReverse(slice []int) []int &#123; // 反转slice for i, j := 0, len(slice)-1; i &lt; j; i, j = i+1, j-1 &#123; slice[i], slice[j] = slice[j], slice[i] &#125; return slice&#125;func equal(x, y []string) bool &#123; if len(x) != len(y) &#123; return false &#125; flag := true for i, e := range x &#123; if e != y[i] &#123; flag = false break &#125; &#125; return flag&#125;func intsToString(values []int) string &#123; var buf bytes.Buffer buf.WriteByte('[') for i, v := range values &#123; if i &gt; 0 &#123; buf.WriteString(" &amp; ") &#125; //fmt.Println(v) fmt.Fprintf(&amp;buf, "%d", v) //输出重定向 &#125; buf.WriteByte(']') return buf.String()&#125; Slice说明1.声明一个空slice，默认len和cap的值都为0。声明长度为0的array的方式: var b [0]int，这两者声明方式非常相似2.slice在新增元素的时候，如果len超过cap，会动态扩容cap，根据网上查到的扩容函数的源码: 123456789101112131415161718192021222324func growslice(et *_type, old slice, cap int) slice &#123; newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; newcap = cap &#125; else &#123; if old.len &lt; 1024 &#123; newcap = doublecap &#125; else &#123; // Check 0 &lt; newcap to detect overflow // and prevent an infinite loop. for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap &lt;= 0 &#123; newcap = cap &#125; &#125; &#125;...&#125; 可概括扩容策略如下:当cap小于len时启动扩容，若cap &lt; 1024，则申请新的内存cap翻倍，若大于2014，则cap提升1/4。(这与python的list扩容策略很相似) 3.使用make方式声明可以指定cap以直接申请一块连续的内存空间，避免频繁地改变cap容量而带来额外的开销。 4.copy函数复制slice时，假设源slice长度为x，目标slice长度为y，则覆盖策略为: 如果 x&lt;y,则用源slice的所有元素逐个覆盖目标slice的前x个元素； 如果 x&gt;y,则用源slice前y个元素逐个覆盖目标slice的所有元素。]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s踩坑(三)、kubeadm证书/etcd证书过期处理]]></title>
    <url>%2F2019%2F03%2F08%2Fk8s%E8%B8%A9%E5%9D%91(%E4%B8%89)%E3%80%81kubeadm%E8%AF%81%E4%B9%A6_etcd%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[故障现象使用kubeadm部署的集群，在运行了一年之后的今天，出现k8s api无法调取的现象，使用kubectl命令获取资源均返回如下报错:1Unable to connect to the server: x509: certificate has expired or is not yet valid 故障排查查看apiserver.crt证书的签署期和过期期： 123root@9027:/etc/etcd/ssl# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not ' Not Before: Mar 8 03:49:29 2018 GMT Not After : Mar 7 05:44:27 2020 GMT 发现恰好刚到过期日期，去github查询才得知kubeadm默认的证书签署过期时间为1年，大坑！google找到方法说可以通过修改kubeadm源代码调整证书签署的过期时间然后重新编译，即可预防这个问题。但是现在证书过期的问题已经发生了，只能通过更换证书的办法解决，在github上有详细的说明，issue链接如下:https://github.com/kubernetes/kubeadm/issues/581 开始替换apiserver证书进入master节点123456789101112131415161718192021222324cd /etc/kubernetes# 备份证书和配置mkdir ./pki_bakmkdir ./conf_bakmv pki/apiserver* ./pki_bak/mv pki/front-proxy-client.* ./pki_bak/ mv ./admin.conf ./conf_bak/ mv ./kubelet.conf ./conf_bak/ mv ./controller-manager.conf ./conf_bak/ mv ./scheduler.conf ./conf_bak/# 创建证书kubeadm alpha phase certs apiserver --apiserver-advertise-address $&#123;MASTER_API_SERVER_IP&#125;kubeadm alpha phase certs apiserver-kubelet-clientkubeadm alpha phase certs front-proxy-client# 生成新配置文件kubeadm alpha phase kubeconfig all --apiserver-advertise-address $&#123;MASTER_API_SERVER_IP&#125;# 将新生成的admin配置文件覆盖掉原本的admin文件mv $HOME/.kube/config $HOME/.kube/config.oldcp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/configsudo chmod 777 $HOME/.kube/config 完成上方操作后，docker restart重启kube-apiserver,kube-controller,kube-scheduler这3个容器 查看证书的过期时间: 123root@009027:~# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text |grep ' Not ' Not Before: Mar 8 03:49:29 2018 GMT Not After : Mar 7 05:44:27 2020 GMT 可以看到证书有效期延长了一/ 如果有多台master节点，先仿照上方将证书文件和配置文件进行备份，然后将这一台配置完成的master上的证书和配置scp过去: 12scp pki/* root@$&#123;other_master&#125;:/etc/kubernetes/pki/scp admin.conf kubelet.conf controller-manager.conf scheduler.conf root@$&#123;other_master&#125;:/etc/kubernetes/ scp完成后记得重启相应master上的3个kubernetes组件容器。 验证kubectl命令发现还是无法查看资源，检查apiserver的志: kubectl logs ${api-server_container_id}12345678910111213141516171819202122I0308 06:26:52.956024 1 serve.go:89] Serving securely on [::]:6443I0308 06:26:52.956168 1 available_controller.go:262] Starting AvailableConditionControllerI0308 06:26:52.956180 1 apiservice_controller.go:112] Starting APIServiceRegistrationControllerI0308 06:26:52.956244 1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controllerE0308 06:26:52.956279 1 cache.go:35] Unable to sync caches for APIServiceRegistrationController controllerI0308 06:26:52.956291 1 apiservice_controller.go:116] Shutting down APIServiceRegistrationControllerI0308 06:26:52.956325 1 crd_finalizer.go:242] Starting CRDFinalizerI0308 06:26:52.956195 1 cache.go:32] Waiting for caches to sync for AvailableConditionController controllerI0308 06:26:52.956342 1 crd_finalizer.go:246] Shutting down CRDFinalizerI0308 06:26:52.956363 1 customresource_discovery_controller.go:152] Starting DiscoveryControllerE0308 06:26:52.956376 1 customresource_discovery_controller.go:155] timed out waiting for caches to syncI0308 06:26:52.956392 1 naming_controller.go:274] Starting NamingConditionControllerE0308 06:26:52.956399 1 cache.go:35] Unable to sync caches for AvailableConditionController controllerI0308 06:26:52.956403 1 naming_controller.go:278] Shutting down NamingConditionControllerI0308 06:26:52.956435 1 controller.go:84] Starting OpenAPI AggregationControllerI0308 06:26:52.956457 1 controller.go:90] Shutting down OpenAPI AggregationControllerI0308 06:26:52.956476 1 crdregistration_controller.go:110] Starting crd-autoregister controllerI0308 06:26:52.956487 1 controller_utils.go:1019] Waiting for caches to sync for crd-autoregister controllerE0308 06:26:52.956504 1 controller_utils.go:1022] Unable to sync caches for crd-autoregister controllerI0308 06:26:52.957364 1 customresource_discovery_controller.go:156] Shutting down DiscoveryControllerI0308 06:26:52.958437 1 available_controller.go:266] Shutting down AvailableConditionControllerI0308 06:26:52.959508 1 crdregistration_controller.go:115] Shutting down crd-autoregister controller 发现kube-apiserver容器无法正常启动，怀疑是证书配置过程中出错，重复了如上步骤多次，发现问题依旧。etcd和kubeadm是同一时间部署的，这个时候考虑etcd是否有问题 etcd证书过期处理检查etcd的证书，发现etcd同样也过期了：123root@yksp009027:/etc/etcd/ssl# openssl x509 -in etcd.pem -noout -text |grep ' Not ' Not Before: Mar 7 00:59:00 2018 GMT Not After : Mar 7 00:59:00 2019 GMT 查看此前部署的时候配置etcd的ca配置文件，发现指定的证书签署过期时间居然是1/（8760h）。。太坑了！(不清楚证书生成以及etcd安装流程的，可以参考此前的文章：部署篇) 12345678910111213141516171819root@009027:~/ssl# cat config.json&#123;"signing": &#123; "default": &#123; "expiry": "8760h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" &#125; &#125;&#125;&#125; 居然和kubeadm生产的api-server证书在同一时间过期了，两个坑连在一起，简直防不胜防！ 知道问题所在了，那么现在开始替换etcd的证书： 首先备份etcd数据: 12cd /var/libtar -zvcf etcd.tar.gz etcd/ 修改ca配置文件，将默认证书签署过期时间修改为10/: 1234567891011121314151617181920root@009027:~/ssl# cat config.json&#123;"signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125;&#125;&#125; 生成新的证书: 1234567891011121314151617#删除过期证书rm -f /etc/etcd/ssl/*# 创建新证书cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcdcp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/# 查看新证书root@009027:/etc/etcd/ssl# openssl x509 -in etcd.pem -noout -text |grep ' Not ' Not Before: Mar 8 06:59:00 2019 GMT Not After : Mar 5 06:59:00 2029 GMT#拷贝到其他etcd节点scp -r /etc/etcd/ssl root@$&#123;other_node&#125;:/etc/etcd/# 重启etcd服务(记住，要3个节点一起重启，不然会hang住)systemctl restart etcd 处理结果etcd证书更换完毕后，再次尝试重启apiserver的容器，启动成功，集群api-server恢复,故障解决。连续2个证书过期，一坑接一坑，卡在apiserver一直无法恢复的时候，一度有些恐慌，担心整个集群由此崩溃数据丢失，还在后面顺利解决。在部署之初没有注意到这些细节问题，例如很明显的是etcd证书的签署ca配置的超时时间居然只有1年，才导致后面才踩了这些坑。平时也要及时备份重要数据，要多多考虑极端情况下的服务恢复方案。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python子线程异常捕获 & Python自定义优化线程池]]></title>
    <url>%2F2019%2F01%2F21%2FPython%E5%AD%90%E7%BA%BF%E7%A8%8B%E5%BC%82%E5%B8%B8%E6%8D%95%E8%8E%B7-Python%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[前言虽然CPython因为GIL的存在导致多线程的并发计算能力大打折扣，但是在i/o密集型的场景时，使用多线程还是能带来效率上的飞跃。近期在使用多线程时遇到了主线程无法捕获子线程抛出的异常问题，这里记录一下解决的办法。 需求将某一指定目录下所有的文件(包含子目录下的文件)中所有被$[]$字符包裹起来的变量替换成指定的值，这是一个典型的io密集的场景，因此考虑使用多线程提升效率 原demo12345678910111213141516171819202122232425262728def main(): conf_map = &#123;'DB_HOST': "X.X.X.X",'DB_USER': "root",'DB_PASSWD': 'abcd1234'&#125; conf_files=['/etc/abc/', '/var/abc'] thpool = ThreadPoolExecutor(5) for file in conf_files: thpool.submit(replace_config, file, conf_map) thpool.shutdown(wait=True)def replace_config(file, conf_map, tmp_conf_path): with open(file, 'r') as f: content = f.read() # 需替换的变量的样式为: $[PASSWORD]$ wrapper_pattern = re.compile('\$\[([\w-]+)\]\$') var_list = wrapper_pattern.findall(content) for var in var_list: try: value = conf_map[var] wrapper = "$[%s]$" % var content = content.replace(wrapper, value) except KeyError: print('key error') os.system("mv &#123;&#125; /tmp".format(tmp_conf_path)) raise Exception('[%s]文件中存在未知的key: %s' % (file, var)) with open(file, 'w') as f: f.write(content) demo内直接使用python3.2版本以后引入的ThreadPoolExecutor库使用多线程，在子线程无异常时是正常运行的，但是在子线程出现异常时(比如子线程的内部逻辑里发现了不存在key时， except KeyError会捕获到异常)，但是你会发现子线程异常终止了，主线程但是却没有异常抛出。 经过一番搜索，在python官方手册中了解到了原因： 参考官方文档链接：https://docs.python.org/zh-cn/3/library/_thread.html 原因即为：使用start()方法启动子线程时，解释器会为子线程开辟独立的栈空间，主线程自然就无法获取子线程栈的信息。当线程异常中止时，会自行退出而不会将此异常raise到主线程。那么得知了原因，就可以找到解决的办法了。思路是继承标准库的Thread类，进行一些小的改写封装。 修改后的demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class ReplaceThread(Thread): def __init__(self, file, conf_map): super(ReplaceThread, self).__init__() self.file = file self.conf_map = conf_map self.exitcode = 0 self.exception = None def run(self): try: self._run() except Exception as e: # 如果捕获到异常，返回值改为不等于0 self.exitcode = 1 self.exception = e def _run(self): with open(self.file, 'r') as f: content = f.read() # 需替换的变量的样式为: $[PASSWORD]$ wrapper_pattern = re.compile('\$\[([\w-]+)\]\$') var_list = wrapper_pattern.findall(content) for var in var_list: try: value = self.conf_map[var] wrapper = "$[%s]$" % var content = content.replace(wrapper, value) except KeyError: raise Exception('[%s]文件中存在未知的key: %s‘ % (self.file, var)) with open(self.file, 'w') as f: f.write(content) # 主线程无法直接捕获子线程内的异常，因此自定义了Thread类，在子线程内定义其出现异常时的返回值，在主线程内根据返回值# 来判断是否出现异常，并进行下一步操作 def main(): conf_files=['/etc/abc/', '/var/abc'] conf_map = &#123;'DB_HOST': "X.X.X.X",'DB_USER': "root",'DB_PASSWD': 'abcd1234'&#125; t_objs = [] for file in conf_files: t = ReplaceThread(file=file, conf_map=conf_map) t.start() t_objs.append(t) for t in t_objs: t.join() if t.exitcode != 0: os.system("mv &#123;&#125; /tmp".format(tmp_conf_path)) raise Exception(t.exception) 如此这般，在主线程里通过自定义的子线程返回值来判断子线程是否有异常，如果子线程有异常则主线程接替抛出子线程里的异常。这里另外还要注意，子线程的join()操作要放到start()操作全部完成了之后再进行，避免主线程被子线程阻塞，这样就变成了串行执行多线程就失去了意义了。 问题这里是以迭代对象来循环启动多线程的，假设迭代对象数量很长，那就会启动成百上千个线程，这是不愿意看到的，为了避免这种情况，可自定义线程池，仅需对上方demo中的main()方法再做一点小改动即可。 实现 123456789101112131415161718192021222324252627282930def main(): conf_files=['/etc/abc/', '/var/abc'] conf_map = &#123;'DB_HOST': "X.X.X.X",'DB_USER': "root",'DB_PASSWD': 'abcd1234'&#125; # 实现的方法是对迭代的对象进行截取，每次只截取前10个对象，执行完这10个对象的操作后再截取随后后的10个对象，直到迭代对象为空。 n = 0 pool_size = 10 files = conf_files file_num = len(files) while files: t_objs = [] start_index = n * pool_size end_index = (n + 1) * pool_size for i in range(pool_size): current_index = start_index + i if current_index &lt; file_num: file = self.conf_files[current_index] t = ReplaceThread(file=file, conf_map=conf_map) t.start() t_objs.append(t) for t in t_objs: t.join() if t.exitcode != 0: os.system("rm -rf &#123;&#125;".format(base_conf_path)) raise Exception(t.exception) n += 1 files = conf_files[end_index:] 复杂度优化：上方的方法是对迭代的对象列表进行切片，每次只截取前10个对象，执行完这10个对象的操作后再截取随后后的10个对象，直到迭代对象为空。这种方式使用list的切片，时间复杂度为O(k),k为截取长度。 有没有更好的方式？这里列举一种复杂度更低的方式：12345678910111213141516171819202122232425def main(): conf_files=['/etc/abc/', '/var/abc'] conf_map = &#123;'DB_HOST': "X.X.X.X",'DB_USER': "root",'DB_PASSWD': 'abcd1234'&#125; pool_size = 10 files = self.conf_files file_num = len(files) while files: if file_num &lt; pool_size: pool_size = file_num t_objs = [] for i in range(pool_size): file = files.pop() t = ReplaceThread(file=file, conf_map=conf_map) t.start() t_objs.append(t) for t in t_objs: t.join() if t.exitcode != 0: os.system("rm -rf &#123;&#125;".format(tmp_conf_path)) raise Exception(t.exception) file_num -= pool_size 说明： 利用了顺序表(list)的 尾部操作/获取长度操作 时间复杂度均为O(1)的特性，每次操作list的尾部元素，这个方式的复杂度更低 总结子线程异常处理问题由此就得以解决,通过一些小改动也可以实现自定义的低复杂度线程池。重要的事情只说一遍：遇到问题查官方文档]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes踩坑(二): Service IP(LVS)间断性TCP连接故障排查]]></title>
    <url>%2F2018%2F12%2F11%2FKubernetes%E8%B8%A9%E5%9D%91(%E4%BA%8C)_%20Service%20IP(LVS)%E9%97%B4%E6%96%AD%E6%80%A7TCP%E8%BF%9E%E6%8E%A5%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5.html</url>
    <content type="text"><![CDATA[问题阶段(一):用户反应某个redis使用卡顿，连接该redis服务使用的是svc代理，即ipvs snat的方式，ipvsadm -L发现，VIP收到的6379端口的数据包，会以rr的方式分别转发到pod的80 6379端口上，相当于会有50%的丢包，不卡才怪： 1234# ipvsadm | grep -2 10.108.152.210TCP 10.108.152.210:6379 rr -&gt; 172.26.6.185:http Masq 1 0 0 -&gt; 172.26.6.185:6379 Masq 1 3 3 2.检查svc： 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: creationTimestamp: 2018-12-06T02:45:49Z labels: app: skuapi-rds-prod run: skuapi-rds-prod name: skuapi-rds-prod namespace: default resourceVersion: "41888273" selfLink: /api/v1/namespaces/default/services/skuapi-rds-prod uid: 09e1a61f-f901-11e8-878f-141877468256spec: clusterIP: 10.108.152.210 ports: - name: redis port: 6379 protocol: TCP targetPort: 6379 selector: app: skuapi-rds-prod sessionAffinity: None type: ClusterIP 可以看出svc配置没有问题，回忆起此前配置此svc时，一开始失误将port和targetPort设置为了默认值80忘记修改了，后面手动kubectl edit svc修改端口为6379。应该是修改保存之后，原本的targetPort 80依然在ipvs中保留为RIP之一，这应该是一个bug。 3.解决办法：使用ipvsadm工具删除多余的RIP，或者删除此svc然后重建。 123# ipvsadm | grep -2 10.108.152.210TCP 10.108.152.210:6379 rr -&gt; 172.26.6.185:6379 Masq 1 4 0 问题阶段(二):现象：有用户反馈svc vip连接经常失败，不限于redis的多个ip有这种情况，于是开始排查 排查:通过对ipvs和系统的连接状态排查，发现了两个问题:1.发现有大量TIME_WAIT的tcp连接,证明系统连接大多是短连接，查看ipvs tcpfin的等待时间为2分钟，两分钟还是过长，有必要修改短一些，30秒是一个比较合理的值，如果比较繁忙的服务，这个值可以改到更低。2.有不少丢包 123456789[root@p020107 ~]# ipvsadm -lnc |wc -l23776[root@p020107 ~]# ipvsadm -lnc |grep TIME_WAIT | wc -l10003[root@p020107 ~]# ipvsadm -L --timeout Timeout (tcp tcpfin udp): 900 120 300root@p020107:~# netstat -s | grep timestamp 17855 packets rejects in established connections because of timestamp 备注：根据tcp 四次挥手协议，主动发起断开连接请求的的一端(简单理解为客户端)，在其发送断开连接请求开始，其连接的生命周期会经历4个阶段分别是FIN-WAIT1 –&gt; FIN_WAIT2 –&gt; TIME_WAIT –&gt; CLOSE，其中2个FIN-WAIT阶段等待的时间就是内核配置的net.ipv4.tcp_fin_timeout 的值，为了快速跳过前两个FIN-WAIT阶段从而进入TIME_WAIT状态，net.ipv4.tcp_fin_timeout值建议缩短。在进入TIME_WAIT状态后，默认等待2个MSL(Max Segment Lifetime)时间，到达最后一步CLOSE状态，关闭tcp连接释放资源。注意：MSL时间在不同平台一般是30s-2min不等，并且基本都是不可修改的(linux将这一时间值写死在了内核中)。 那么为什么要等待2*MSL呢？在stackoverflow中找到了一个较为易懂的解释： So the TIME_WAIT time is generally set to double the packets maximum age. This value is the maximum age your packets will be allowed to get to before the network discards them.That guarantees that, before you’re allowed to create a connection with the same tuple, all the packets belonging to previous incarnations of that tuple will be dead. 翻译一下：time_wait时间设计为tcp分片的最大存活时间的两倍，这么设计的原因是，网络是存在延迟的，同时tcp分片在网络传输中可能出现意外，发送端在确认意外(例如到达MSL时间后)后发出数据分片的重传。假如socket连接不经等待直接关闭了，然后再重新打开了一个端口号一致的连接，可能导致新启动的socket连接，接收到了此前销毁关闭的socket连接的数据。因此，设计TIME_WAIT等待时间为2MSL，是为了保证在等待2MSL之后，此前旧socket的数据分片即使还没有到达接收端，也已经在网络传输中过期消逝了，新启动的socket不会接收到此前的旧数据分片。 优化方式优化的思路1.断开连接时加速进入TIME_WAIT状态，以快速提供可用的连接端口2.解决timestamps丢包问题 查看ipvs设置的各类连接的超时时间，修改默认的tcpfin 2分钟为30秒 123456[root@p020107 ~]# ipvsadm -L --timeout Timeout (tcp tcpfin udp): 900 120 300[root@p020107 ~]# ipvsadm --set 900 30 300[root@p020107 ~]# ipvsadm -L --timeout Timeout (tcp tcpfin udp): 900 30 300 内核参数优化 添加入/etc/sysctl.conf文件中 12345678910111213141516# 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； net.ipv4.tcp_tw_reuse = 1 # 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。注意，net.ipv4.tcp_timestamps默认为开启，tcp_tw_recycle此选项也开启后，tcp timestamp机制就会激活，在部分场景下，需要关闭tcp_timestamps功能，见下方此选项的说明。 net.ipv4.tcp_tw_recycle = 1 # 修改tcp会话进去fin状态后的等待时间，超时则关闭会话 net.ipv4.tcp_fin_timeout = 30 # 处于TIME_WAIT最大的socket数量，默认为180 000，超过这个数目的socket立即被清除 net.ipv4.tcp_max_tw_buckets=180000 # tcp缓存时间戳,RFC1323中描述了，系统缓存每个ip最新的时间戳，后续请求中该ip的tcp包中的时间戳如果小于缓存的时间戳(即非最新的数据包)，即视为无效，相应的数据包会被丢弃，而且是无声的丢弃。在默认情况下，此机制不会有问题，但在nat模式时,不同的ip被转换成同一个ip再去请求真正的server端，在server端看来，源ip都是相同的，且此时包的时间戳顺序可能不一定保持递增，由此会出现丢包的现象，因此，如果是nat模式工作，建议关闭此选项。 net.ipv4.tcp_timestamps = 0 这几个参数之间还有一些关联关系，参考此篇文章,写得非常详细:http://www.freeoa.net/osuport/cluster/lvs-inuse-problem-sets_3111.html 问题阶段(三):现象完成上面的操作后，TIME_WAIT数量下降到了4位数，丢包数量没有再增加。但是过了一些天之后，再一次出现了偶尔个别VIP无法建立连接的情况。挑选了其中一个VIP 10.111.99.131开始排查client: 192.168.58.36DIP: 10.111.99.131RIP: 172.26.8.17 开始排查三层连接没有问题: 12345678910111213ywq@ywq:~$ traceroute 10.111.99.131traceroute to 10.111.99.131 (10.111.99.131), 30 hops max, 60 byte packets 1 192.168.58.254 (192.168.58.254) 7.952 ms 8.510 ms 9.131 ms 2 10.111.99.131 (10.111.99.131) 0.253 ms 0.243 ms 0.226 msywq@ywq:~$ ping 10.111.99.131PING 10.111.99.131 (10.111.99.131) 56(84) bytes of data.64 bytes from 10.111.99.131: icmp_seq=1 ttl=63 time=0.296 ms64 bytes from 10.111.99.131: icmp_seq=2 ttl=63 time=0.318 ms^C--- 10.111.99.131 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1020msrtt min/avg/max/mdev = 0.296/0.307/0.318/0.011 ms tcp连接无法建立 123ywq@ywq:~$ telnet 10.111.99.131 80Trying 10.111.99.131...telnet: Unable to connect to remote host: Connection timeout 在lvs diretor server上查看连接状态： 12345[root@p020107 ~]# ipvsadm -lnc | grep 58.36TCP 00:59 TIME_WAIT 192.168.9.13:58236 10.97.85.43:80 172.26.5.209:80TCP 00:48 SYN_RECV 192.168.58.36:57964 10.111.99.131:80 172.26.8.17:80TCP 00:48 SYN_RECV 192.168.58.36:57963 10.111.99.131:80 172.26.8.17:80TCP 00:48 SYN_RECV 192.168.58.36:57965 10.111.99.131:80 172.26.8.17:80 发现tcp连接状态为SYN_RECV状态，那么根据三次握手协定，再结合lvs的工作流程，说明LVS server接收到了clien的syn包,向client回复了syn+ack然后进入了SYN_RECV状态，同时direct server会向后端的real server发起建立连接的请求。既然现在direct server能与client端交互，那么当前的问题应该在于：direct Server和real Server之间没有正常地进行数据包交互或者出现了丢包，查阅了很多资料，rp_filter这一内核参数可能导致这一问题。 来看一下官方关于这一参数的解释: 1234567891011121314151617181920rp_filter - INTEGER 0 - No source validation. 1 - Strict mode as defined in RFC3704 Strict Reverse Path Each incoming packet is tested against the FIB and if the interface is not the best reverse path the packet check will fail. By default failed packets are discarded. 2 - Loose mode as defined in RFC3704 Loose Reverse Path Each incoming packet's source address is also tested against the FIB and if the source address is not reachable via any interface the packet check will fail. Current recommended practice in RFC3704 is to enable strict mode to prevent IP spoofing from DDos attacks. If using asymmetric routing or other complicated routing, then loose mode is recommended. The max value from conf/&#123;all,interface&#125;/rp_filter is used when doing source validation on the &#123;interface&#125;. Default value is 0. Note that some distributions enable it in startup scripts. 简单解释一下:0:表示不开启源检测1:严格模式，根据数据包的源，通过查FIB表(Forward Information Table,可以理解为路由表)，检查数据包进入端口是同时也是出端口，以视为最佳路径，如果不符合最佳路径，则丢弃数据包2:松散模式,检查数据包的来源，查FIB表，如果通过任意端口都无法到达此源，则丢包 结合使用场景来说:在LVS (nat)+k8s的工作场景下，LVS Server送往Real Server的包可能走的tunnel接口，而Real Server通过tunnel接口收到包后，查路由表发现回包要走物理eth/bond之类接口，如果rp_filter开启了严格模式，会导致网络异常状况 检查每一台kube node的网卡配置参数,发现centos7.4的几台node默认确实开启了rp_filter，ubuntu大部分则没有:12345678910111213141516171819# 容器内的veth网卡可忽略，因为容器本身只有一块对外的网卡[root@p020114 ~]# sysctl -a | grep rp_filter | grep -v 'veth'net.ipv4.conf.all.rp_filter = 1net.ipv4.conf.bond0.rp_filter = 1net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.docker0.rp_filter = 2net.ipv4.conf.dummy0.rp_filter = 0net.ipv4.conf.em1.rp_filter = 1net.ipv4.conf.em2.rp_filter = 1net.ipv4.conf.em3.rp_filter = 1net.ipv4.conf.em4.rp_filter = 1net.ipv4.conf.kube-bridge.rp_filter = 0net.ipv4.conf.kube-dummy-if.rp_filter = 0net.ipv4.conf.lo.rp_filter = 0net.ipv4.conf.tun-192168926.rp_filter = 1net.ipv4.conf.tun-192168927.rp_filter = 1net.ipv4.conf.tun-192168928.rp_filter = 1net.ipv4.conf.tun-192168929.rp_filter = 1net.ipv4.conf.tunl0.rp_filter = 0 关闭此功能 12345678910111213141516171819echo "net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.bond0.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.docker0.rp_filter = 2net.ipv4.conf.dummy0.rp_filter = 0net.ipv4.conf.em1.rp_filter = 0net.ipv4.conf.em2.rp_filter = 0net.ipv4.conf.em3.rp_filter = 0net.ipv4.conf.em4.rp_filter = 0net.ipv4.conf.kube-bridge.rp_filter = 0net.ipv4.conf.kube-dummy-if.rp_filter = 0net.ipv4.conf.lo.rp_filter = 0net.ipv4.conf.tun-192168926.rp_filter = 0net.ipv4.conf.tun-192168927.rp_filter = 0net.ipv4.conf.tun-192168928.rp_filter = 0net.ipv4.conf.tun-192168929.rp_filter = 0net.ipv4.conf.tunl0.rp_filter = 0" &gt;&gt; /etc/sysctl.conf 加载生效 1sysctl -p 总结VIP偶尔无法建立TCP连接的问题已解决，一个星期过去了没有再复现，继续观察ing.]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十四)、RBAC权限控制]]></title>
    <url>%2F2018%2F12%2F06%2Fk8s(%E5%8D%81%E5%9B%9B)%E3%80%81RBAC%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.html</url>
    <content type="text"><![CDATA[前言kubernetes 集群相关所有的交互都通过apiserver来完成，对于这样集中式管理的系统来说，权限管理尤其重要，在1.5版的时候引入了RBAC(Role Base Access Control)的权限控制机制. RBAC也是初接触k8s的用户较难理解的一部分，同时RBAC作为集群管理的基础组件之一，本该放在最前面几篇，但是最近才想起来这个方面的知识点，赶紧梳理总结输出了本篇 工作流程流程图 流程拆解一、基于资源申明和管理方法申明组成rule规则，和Role(命名空间范围))或ClusterRole(集群范围)对象进行绑定，Role类的对象将拥有所申明资源及的指定方法的权限。二、将Role权限落到实际用户或程序上，即RoleBinding(命名空间范围)或CLusterRoleBinding(集群范围) 有两种方式：1.用户使用：如果是用户需求权限，则将Role与User(或Group)绑定(这需要创建User/Group)；2.程序使用：如果是程序需求权限，将Role与ServiceAccount指定(这需要创建ServiceAccount并且在deployment中指定ServiceAccount)。相当于Role是一个类，用作权限申明，User/Group/ServiceAccount将成为类的实例 实战一、用户使用安装cfssl工具，生成ca-config.json文件。这一步骤在安装集群的时候已经做过，这里不再复述，参考：k8s（一）、 1.9.0高可用集群本地离线部署记录 准备ca配置json文件如下：1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt;EOF&#123;"signing": &#123;"default": &#123; "expiry": "8760h"&#125;,"profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" &#125;&#125;&#125;&#125;EOF 安装好的k8s集群，默认在ca文件在如下路径： 12～ /rbac-test# ls /etc/kubernetes/pki/ca*/etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/ca.key 创建用户的证书签署请求配置json文件： 123456789101112131415161718~/rbac-test# cat opsuser-csr.json&#123; "CN": "opsuser", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125; CN即comman name，后面用户证书认证时使用的用户名 生成opsuser的证书： 1cfssl gencert -ca=/etc/kubernetes/pki/ca.crt -ca-key=/etc/kubernetes/pki/ca.key -config=./ca-config.json -profile=kubernetes ./opsuser-csr.json | cfssljson -bare opsuser 会生成下面3个文件： 1opsuser-key.pem opsuser.pem opsuser.csr 生成用户的专属配置文件完整的配置文件包含3块，分别是cluster/context/user部分，包含相应的内容，分3个步骤生成，后方详解 1.cluster部分：生成此部分需要有集群的ca.pem文件，此文件在/etc/kubernetes/pki目录下默认没有，可以使用openssl命令基于ca.key文件去生成ca.pem文件；也可以直接使用已有的/etc/kubernetes/admin.conf文件中cluster部分： 123456789101112131415161718# 只取cluster的上下文~/rbac-test# cat /etc/kubernetes/admin.conf apiVersion: v1clusters:- cluster: certificate-authority-data: [CERT DATA] server: https://10.90.1.238:6443 name: kubernetes# 将cluster部分写入opsuser的配置文件内cat &gt; opsuser.kubeconfig &lt;&lt;EOFapiVersion: v1clusters:- cluster: certificate-authority-data: [CERT DATA] server: https://10.90.1.238:6443 name: kubernetesEOF 2.context部分 12345kubectl config set-context kubernetes \--cluster=kubernetes \--user=opsuser \--namespace=default \--kubeconfig=opsuser.kubeconfig 3.user认证部分 12345kubectl config set-credentials opsuser \--client-certificate=./opsuser.pem \--client-key=./opsuser-key.pem \--embed-certs=true \--kubeconfig=opsuser.kubeconfig 查看最终生成的配置文件： 12345678910111213141516171819202122# cat opsuser.kubeconfig apiVersion: v1clusters:- cluster: certificate-authority-data: [CRET DATA] server: https://10.90.1.238:6443 name: kubernetescontexts:- context: cluster: kubernetes namespace: default user: opsuser name: kubernetescurrent-context: kuberneteskind: Configpreferences: &#123;&#125;users:- name: opsuser user: as-user-extra: &#123;&#125; client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURZekNDQWt1Z0F3SUJBZ0lVV2ZKMlNLb1NJWWhBMHFJdzRaVUZERnlWY1pzd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZURVRNQkVHQTFVRUF4TUthM1ZpWlhKdVpYUmxjekFlRncweE9ERXlNRFV3T0RNek1EQmFGdzB4T1RFeQpNRFV3T0RNek1EQmFNR0l4Q3pBSkJnTlZCQVlUQWtOT01SQXdEZ1lEVlFRSUV3ZENaV2xLYVc1bk1SQXdEZ1lEClZRUUhFd2RDWldsS2FXNW5NUXd3Q2dZRFZRUUtFd05yT0hNeER6QU5CZ05WQkFzVEJsTjVjM1JsYlRFUU1BNEcKQTFVRUF4TUhiM0J6ZFhObGNqQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU93Wgo4d1hpdFhrOGtKNk9PSkFvUHZGaWw2ZkdKUDFWT3VyYWFqTWMyZ2ROSkV0Q3o2Mzk2eTNIZGw1T2NXdlBNNURvCjJ3a0NGKzlJd09jVzljVHhTZFZMUEg3OVEvQVhHcm9Nd2ltZk9WRUNlYm9aVkswRUx3azk1K2piRUpVVHY4eTYKMForM2JvMGtwL2p2Q2pzdmwxWUR3RGtGeGtwckZ3SXliOGt0cVBLMVFkb2ZrOE16Z3NTMnNZYTR1MWFydHd0dQozSTZZbEViUFNIOXhGbmx1UUFvbjZnSmkrM0c1YTdVRGxvRm9MSHlqL00wMStPRi9YWVpEcWgwK3hoZTlVRWdHClZBZkNSQkYyZlg2WjdCaFkybnpsblZOejV5SXhmSlpJclNIYnFhUlAxZXY1Tk1ndVNRcnl3UGQ5OG5jM201RnkKMkVXa28xeEJmQ1FXSHJKd04vMENBd0VBQWFOZU1Gd3dEZ1lEVlIwUEFRSC9CQVFEQWdXZ01CMEdBMVVkSlFRVwpNQlFHQ0NzR0FRVUZCd01CQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjBHQTFVZERnUVdCQlFXCmpDR2NYVjRPOTRqZlV1Mk9Kd2hqTC9xMVZEQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFBeXdlZk5uckNKU0MKdDdrT2tHL1RUSHA1NG1tT0QxS2FPZW82TnpycFRuNVQwa0h3VEdrYmNHcWxvcDIvM1lhQXJDUHo2Q1J4VDd0QQpaTUNiOXhRZkJ0cllpeHJUdGF0TTZ4YlJuQTBqaC96Y2tvRHVuYjFEYkhxbG1JQW5jLzdaTEthQklKSnA5L0IwCjB2NnJXTEd2N0F1YmZJT1ZZVTdBVTVnZmh1LzJoWC93MzFJWlgwaXZ0WEtDR0lJb1Bya2hsL3NvUTEwdVFhUmgKeFFBdzhUbTc0dTBJTDE3OFFsRlhSYm1JQXpuWk42Skh1YW5QZE82TVI1Z0dobVMzRDRaUDg2RE5DenZCOVZFcQpWa0w4R0E2bHJHNm1sem5MN0pxODlQajdOdElwZnRaeDBsdTBYRmsxTm9DWk9VMFl5Yk9peWlPS2N0NVNia3VKClVLUjVYMC9GTUE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== client-key-data: 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 最后，加载启用此配置文件 1kubectl config use-context kubernetes --kubeconfig=opsuser.kubeconfig 此时，用户已经创建完成，下面开始进行RoleBinding步骤赋予权限。 RoleBinding绑定创建Role： 12345678910111213141516171819202122232425262728~/rbac-test# cat pod-reader.yaml kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: kube-system name: pod-readerrules:- apiGroups: [""] # ""代表核心api组 resources: ["pods"] verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]~/rbac-test# cat opsuser-role-bind.yamlkind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: read-pods namespace: kube-systemsubjects:- kind: User name: opsuser # 目标用户 apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: pod-reader # 角色信息 apiGroup: rbac.authorization.k8s.iokubectl apply -f pod-reader.yamlkubectl apply -f opsuser-role-bind.yaml RoleBinding步骤已完成检验 123456789101112131415# 将此配置文件作为当前kubectl配置文件使用~/rbac-test# mv /root/.kube/config /root/.kube/config.bak~/rbac-test# cp opsuser.kubeconfig /root/.kube/config# 查看资源:~/rbac-test# kubectl get pods NAME READY STATUS RESTARTS AGEappdev-5c4b65bbbb-t4lvr 1/1 Running 2 44ddeptest13dev-fb79d9957-d55vb 0/1 Pending 0 35ddeptest13test-8687769d58-lzm2r 0/1 Pending 0 41ddeptest14dbtm-ffbd5c56c-ph68l 0/2 Pending 0 41ddeptest14test-7cb7bf6cc-xnrxw 0/1 Pending 0 41d:~/rbac-test# kubectl get nodesError from server (Forbidden): nodes is forbidden: User "opsuser" cannot list nodes at the cluster scope 可以看到，基于生成的用户配置文件，已经可以管理申明的pod资源，无法管理申明以外的资源类型。测试成功 二、程序使用与用户使用User不同的是，程序的权限赋予是通过ServiceAccount来作为权限载体的，因此，首先要拥有程序/deployment/ServiceAccount等资源，这里以前面文章里没有讲的kube dashboard为例进行说明。 首先我们来看官方最新版本的kube dashboard的yaml文件(服务暴露方式稍作修改成了NodePort)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319~/dashboard# cat kubernetes-dashboard.yaml# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.- apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster.- apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"]- apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1beta2metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard:~/dashboard# lsadmin-token.yaml kubernetes-dashboard.yaml:~/dashboard# vim kubernetes-dashboard.yaml :~/dashboard# cat kubernetes-dashboard.yaml # Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.- apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster.- apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"]- apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1beta2metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 部署dashboard： 1kubectl apply -f kubernetes-dashboard.yaml 浏览器访问dashboard需要token，或者kube-config文件，这里使用token，在创建好ServiceAccount且和role进行bind绑定之后，会自动创建一个名为${ServiceAccountName}-token-xxx 的Secret，其内包含token，为base64编码，可以使用kubectl describe直接查看： 123456789101112131415161718192021:~/dashboard# kubectl get secret -o wide -n kube-system | grep kubernetes-dashboardistio.kubernetes-dashboard istio.io/key-and-cert 3 2dkubernetes-dashboard-certs Opaque 0 2dkubernetes-dashboard-key-holder Opaque 2 2dkubernetes-dashboard-token-t2mmr kubernetes.io/service-account-token 3 2d:~/dashboard# kubectl describe secret -n kube-system kubernetes-dashboard-token-t2mmrName: kubernetes-dashboard-token-t2mmrNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=kubernetes-dashboard kubernetes.io/service-account.uid=4f237c6c-f7ad-11e8-9859-0050569da8f2Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi10Mm1tciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjRmMjM3YzZjLWY3YWQtMTFlOC05ODU5LTAwNTA1NjlkYThmMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.UEJ0MBO9RhSF_fzZHPKezWqkZsWG3atFQQgoPGlyyIDXtQN-ZYlZPEScjunwp4FVgX5uRMpxHGyexJN5e8zYouMTRrGaDaIu7cGj5ICP80iabi8wyMCnJrR5_p0Tmy2PkNZDCyh0MbDCJE6lbOv7Lu7CldivEMtr4BAvxvMmBdmoT0CixQexfw2tioeeoYBwEKgxjL6mMBIGKE0spGZQova7bzTN8f2ZGpNn8jFI_EW9Y7Et5sitw3rCRwxxMSDwUwwCOv26NXPnPVA7cWJeuzPRTS0BHE0euRYojQN7F77NkfwCynx8iWGpM2MEuHIj6coCUcTuCyN5yMD1Zag2zg 使用此token登录dashboard： 第二张截图可以发现，dashboard无多种资源类型的list权限，这是因为在上面的yaml文件中，定义的role权限是限定的： 1234567891011121314151617181920212223242526272829303132333435# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.- apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster.- apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"]- apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"] 可以修改这里的Role内部的权限，也可以自行配置更高级的权限，我这里为了方便，直接配置了一个admin的ServiceAccount:12345678910111213141516171819202122232425~/dashboard# cat admin-token.yamlkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: "true"roleRef: kind: ClusterRole # ClusterRole拥有集群级别权限，于此同时绑定时不能再使用RoleBinding而应该使用ClusterRoleBinding name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile 创建： 1~/dashboard# kubectl apply -f admin-token.yaml 查看token： 1234567891011121314151617:~/dashboard# kubectl get secret -n kube-system | grep adminadmin-token-b9r26 kubernetes.io/service-account-token 3 1d~/dashboard# kubectl describe secret -n kube-system admin-token-b9r26Name: admin-token-b9r26Namespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=admin kubernetes.io/service-account.uid=d471a78c-f833-11e8-9859-0050569da8f2Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1iOXIyNiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImQ0NzFhNzhjLWY4MzMtMTFlOC05ODU5LTAwNTA1NjlkYThmMiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.euwxJkmtt7mjbpzHdMzJWmxO0swuVRjYFGkEwzX1E6caXrUj1XXrSQO4J3mttJvMOVQaGEztInI-cKMiTDzLYHE_85VISHbZTUA8dcSFDIuGOt4SRI_XoA11pIlUAmH4b_4kxgaA3DA5TdUX6O2mo93QEvLOOngBkLO55Z9y980H_J5JiBgbndnycOGXVIW-gEgEjBewtiH1ZCiha-lwUlTvSNJkLrTPlm5Rjoao8lONn4vcD1sgzrSrW8VMxxclw8Cu3C79zyujHo0F_YuwHteIlBwOPSTaiokbuyQr-IS2UTpwUxlzf__wXgidc_SXqTC9vecM7hcRVhEi_d_hnw dashboard退出之前的账号，使用admin-token重新登录，可以发现已经拥有全部权限： 总结k8s角色授权使用的流程有多个环节多种资源类型搭配完成，权限粒度明确、用户与进程分工有序，理解了顶部的流程图，相信你将不再为RBAC疑惑。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-router工作模式抓包分析]]></title>
    <url>%2F2018%2F11%2F13%2Fkube-router%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[## 前言kube-router是由CNCF官方孵化的CNI插件项目，灵活度很高，支持Service网段、pod网段bgp宣告，与集群内外构建直连网络，同时支持ipvs Service代理，目前已有不少大中型公司采用此CNI，github的star数量已经超越calico，因此决定采用kube-router的cni解决方案。 IPVS Servicekube-router本身提供ipvs模式的service代理，默认与Kube-proxy的ipvs模式可以说是几乎一致的，都为ipvs nat模式，但kube-router额外提供ipvs的DR(direct return)模式，需配合tunnel模式和Service annotation使用。网络组件过多反而会导致故障不易排查，因此，直接使用kube-router官方推荐的ipvs Service模式，弃用kube-proxy。删除kube-proxy12KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n kube-system delete ds kube-proxydocker run --privileged -v /lib/modules:/lib/modules --net=host k8s.gcr.io/kube-proxy-amd64:v1.14.3 kube-proxy --cleanup kuberouter ipvs的工作模式kube-router会自动发现Service资源与endpoint资源，创建相应的ipvs条目,ipvs条目中的vip是Service的ClusterIP，realServer是Serivce关联的endpoints.123456789101112131415# service$:~# kubectl get svc | grep 10.96.20.77svc1 ClusterIP 10.96.20.77 &lt;none&gt; 80/TCP 92d# endpoint$:~# kubectl get ep svc1NAME ENDPOINTS AGEsvc1 172.26.13.31:80,172.26.2.54:80,172.26.5.46:80 92d# kube-router自动创建ipvs条目$ ipvsadmTCP 10.96.20.77:http rr -&gt; 172.26.2.54:http Masq 1 0 0 -&gt; 172.26.5.46:http Masq 1 0 0 -&gt; 172.26.13.31:http Masq 1 0 0 同时若配置好了bgp网络，会将Service ClusterIP通过bgp路由宣告出去,进入kube-router容器内部查看宣告的路由表:12$:~ #gobgp global rib | grep 10.96.20.77*&gt; 10.96.20.77/32 192.168.9.27 00:02:08 [&#123;Origin: i&#125;] 流量是如何转发给ipvs的？流量是如何转发到direct server的呢？查看direct server的接口，你会发现多了一个虚拟接口，地址正是service clusterIP：12$:~# ip a | grep 10.96.20.77 inet 10.96.20.77/32 brd 10.96.20.77 scope link kube-dummy-if 假设你的路由已经都配置好了，发往service clusterIP的流量被送达了direct server，那么它是如何处理流量将其转发给内核的ipvs模块的呢？来看看iptables吧，你会发现kube-router在iptables里新建了一个chain:12345Chain KUBE-ROUTER-SERVICES (1 references)target prot opt source destinationACCEPT all -- anywhere anywhere /* allow input traffic to ipvs services */ match-set kube-router-ipvs-services dst,dstACCEPT icmp -- anywhere anywhere /* allow icmp echo requests to service IPs */ icmp echo-requestREJECT all -- anywhere anywhere /* reject all unexpected traffic to service IPs */ ! match-set kube-router-local-ips dst reject-with icmp-port-unreachable 注释中有写，它是将去往services clusterIP的流量转发进下一步，其余的流量会过滤掉，那么它是怎么匹配到services clusterIP的呢？iptabels里面并没有其他的规则了呀，答案是ipset:12$:~# ipset list kube-router-ipvs-services | grep 10.96.20.7710.96.20.77,tcp:80 timeout 0 匹配到ipset里的条目，则正常转发进入ipvs内核模块 抓包验证了解完了上面的工作模式，光说理论也不行，下面就来实际抓包验证一下吧，分三种模式分别抓包： 集群外部访问Service ClusterIP 集群外部通过Ingress http访问集群内服务 集群内部访问Service ClusterIP bgp网络直通方案(三层)集群外部访问Service ClusterIP环境:1234client: 192.168.17.3 #集群外direct server: 192.168.9.27service clusterIp: 10.104.87.154endpoint pod: 172.26.9.31 172.26.9.32 ipvs条目: 123TCP 10.104.87.154:http rr -&gt; 172.26.9.31:http Masq 1 0 0 -&gt; 172.26.9.32:http Masq 1 0 0 部署好pod上的http服务后，首先在client端发起两次curl http请求,同时在service入口的主机上抓包:1234567891011121314151617181920$:~# tcpdump host 10.104.87.154 or host 172.26.9.31 or host 172.26.9.32tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes17:10:34.272343 IP 192.168.17.4.53643 &gt; 10.104.87.154.http: Flags [S], seq 478846184, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1042528917 ecr 0,sackOK,eol], length 017:10:34.272364 IP yksp009027.youkeshu.com.53643 &gt; 172.26.9.31.http: Flags [S], seq 478846184, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1042528917 ecr 0,sackOK,eol], length 017:10:34.273032 IP 172.26.9.31.http &gt; yksp009027.youkeshu.com.53643: Flags [S.], seq 420235101, ack 478846185, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:10:34.273040 IP 10.104.87.154.http &gt; 192.168.17.4.53643: Flags [S.], seq 420235101, ack 478846185, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:10:34.273371 IP 192.168.17.4.53643 &gt; 10.104.87.154.http: Flags [.], ack 1, win 4096, length 017:10:34.273379 IP yksp009027.youkeshu.com.53643 &gt; 172.26.9.31.http: Flags [.], ack 1, win 4096, length 017:10:34.273447 IP 192.168.17.4.53643 &gt; 10.104.87.154.http: Flags [P.], seq 1:78, ack 1, win 4096, length 77: HTTP: GET / HTTP/1.117:12:23.661092 IP 192.168.17.4.53644 &gt; 10.104.87.154.http: Flags [S], seq 1822033847, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1042638076 ecr 0,sackOK,eol], length 017:12:23.661110 IP yksp009027.youkeshu.com.53644 &gt; 172.26.9.32.http: Flags [S], seq 1822033847, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1042638076 ecr 0,sackOK,eol], length 017:12:23.661248 IP 172.26.9.32.http &gt; yksp009027.youkeshu.com.53644: Flags [S.], seq 1318960293, ack 1822033848, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:12:23.661256 IP 10.104.87.154.http &gt; 192.168.17.4.53644: Flags [S.], seq 1318960293, ack 1822033848, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:12:23.661471 IP 192.168.17.4.53644 &gt; 10.104.87.154.http: Flags [.], ack 1, win 4096, length 017:12:23.661482 IP yksp009027.youkeshu.com.53644 &gt; 172.26.9.32.http: Flags [.], ack 1, win 4096, length 017:12:23.661557 IP 192.168.17.4.53644 &gt; 10.104.87.154.http: Flags [P.], seq 1:78, ack 1, win 4096, length 77: HTTP: GET / HTTP/1.117:12:23.661563 IP yksp009027.youkeshu.com.53644 &gt; 172.26.9.32.http: Flags [P.], seq 1:78, ack 1, win 4096, length 77: HTTP: GET / HTTP/1.1 通过抓包结果可以看到，标准的ipvs nat模式，5个步骤: 12345671.源client ip -&gt; 目的direct server2.direct server收到包后，进行dnat转换，变为:源direct server -&gt; 目的endpoint ip3.endpoint回包给direct server4.direct server将收到的包snat转换,变为: 源direct server -&gt; 目的client ip,发送5.client收到回包同时也看到了,2次调用，ipvs默认rr算法生效，分别调度到了不同的endpoint 示意图 集群内部访问Service ClusterIP环境:12345678client: 172.26.0.103 #集群内podclient pod所在node: 192.168.9.28direct server: 192.168.9.27service clusterIp: 10.104.87.154endpoint pod: 172.26.9.34endpoint pod所在node：192.168.20.203 ipvs条目: 12TCP 10.104.87.154:http rr -&gt; 172.26.9.34:http Masq 1 0 0 部署好endpoint pod上的http服务后，首先在client pod端发起curl http请求:curl 10.104.87.154 在client pod所在node、service入口(direct server)主机上和endpoint pod内部同时一起抓包:123456789101112131415161718192021222324252627282930# 192.168.9.27$:~# tcpdump host 10.104.87.154 or host 172.26.9.34 and port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes# 192.168.9.28 ,client pod所在node$:~# tcpdump host 10.104.87.154 or host 172.26.9.34 and port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes17:22:08.200255 IP 172.26.0.103.59848 &gt; 172.26.9.34.http: Flags [S], seq 2739388076, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:22:08.227382 IP 172.26.9.34.http &gt; 172.26.0.103.59848: Flags [S.], seq 1541751593, ack 2739388077, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 017:22:08.227456 IP 172.26.0.103.59848 &gt; 172.26.9.34.http: Flags [.], ack 1, win 251, length 017:22:08.227588 IP 172.26.0.103.59848 &gt; 172.26.9.34.http: Flags [P.], seq 1:78, ack 1, win 251, length 77: HTTP: GET / HTTP/1.117:22:08.256779 IP 172.26.9.34.http &gt; 172.26.0.103.59848: Flags [.], ack 78, win 251, length 017:22:08.257093 IP 172.26.9.34.http &gt; 172.26.0.103.59848: Flags [P.], seq 1:311, ack 78, win 251, length 310: HTTP: HTTP/1.1 403 Forbidden17:22:08.257136 IP 172.26.0.103.59848 &gt; 172.26.9.34.http: Flags [.], ack 311, win 268, length 017:22:08.285245 IP 172.26.9.34.http &gt; 172.26.0.103.59848: Flags [F.], seq 311, ack 79, win 251, length 017:22:08.285297 IP 172.26.0.103.59848 &gt; 172.26.9.34.http: Flags [.], ack 312, win 268, length 0# endpoint pod 172.26.9.34root@gittestdev-65f985fdcb-tplj2:~# tcpdump port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes09:15:26.939961 IP 172.26.0.103.52736 &gt; gittestdev-65f985fdcb-tplj2.http: Flags [S], seq 3722842581, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 009:15:26.940024 IP gittestdev-65f985fdcb-tplj2.http &gt; 172.26.0.103.52736: Flags [S.], seq 4165097507, ack 3722842582, win 16060, options [mss 1460,nop,nop,sackOK,nop,wscale 6], length 009:15:26.940199 IP 172.26.0.103.52736 &gt; gittestdev-65f985fdcb-tplj2.http: Flags [.], ack 1, win 251, length 009:15:26.940266 IP 172.26.0.103.52736 &gt; gittestdev-65f985fdcb-tplj2.http: Flags [P.], seq 1:78, ack 1, win 251, length 77: HTTP: GET / HTTP/1.109:15:26.940291 IP gittestdev-65f985fdcb-tplj2.http &gt; 172.26.0.103.52736: Flags [.], ack 78, win 251, length 009:15:26.941167 IP gittestdev-65f985fdcb-tplj2.http &gt; 172.26.0.103.52736: Flags [P.], seq 1:311, ack 78, win 251, length 310: HTTP: HTTP/1.1 403 Forbidden09:15:26.941375 IP 172.26.0.103.52736 &gt; gittestdev-65f985fdcb-tplj2.http: Flags [.], ack 311, win 268, length 0 查看client pod所在node上的ipvs会话信息是否包含刚访问的vip: 12root@yksp009028:~# ipvsadm -lnc | grep 10.104.87.154TCP 01:56 TIME_WAIT 172.26.0.103:42228 10.104.87.154:80 172.26.9.34:80 从以上几处结果可以总结出： 去往vip的路由指向的默认direct server上没有流量经过; client pod发起(172.26.0.103) 去往Service ClusterIP(10.104.87.154)的数据包，它所属的node在转发前，读取ipvs条目，根据ipvs条目内的信息，将数据包的目的地转换成了后端pod的地址，然后直接使用3层网络转发，将数据包转发到了后端pod。即数据包在本机内就已经基于ipvs，将目的地址从vip转换成了real server ip，出本机后是基于real server ip走的3层路由。 综上，集群内部service调用，走的是基于real server ip的路由，而不是如上面案例中的集群外部调用一样走基于vip的路由。 示意图 Ingress Gateway入口web代理方案(七层http协议):环境:12345678910client: 192.168.17.3target domain: test.domain.comingress gateway: 192.168.9.200service name: testservice clusterIp: 10.104.87.154endpoint pod: 172.26.9.31 172.26.9.32 部署好pod上的http服务后，首先在client端发起两次curl请求,同时在ingress gateway入口的主机上抓包:123456789101112131415161718192021222324252627282930313233$:~# tcpdump host 10.104.87.154 or host 172.26.9.31 or host 172.26.9.32 or host 192.168.17.4tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond0, link-type EN10MB (Ethernet), capture size 262144 bytes18:06:36.805139 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [S], seq 194757065, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1045875096 ecr 0,sackOK,eol], length 018:06:36.805182 IP 192.168.9.200.http &gt; 192.168.17.4.54349: Flags [S.], seq 3062575155, ack 194757066, win 15928, options [mss 1460,sackOK,TS val 1690332192 ecr 1045875096,nop,wscale 6], length 018:06:36.805419 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [.], ack 1, win 2058, options [nop,nop,TS val 1045875096 ecr 1690332192], length 018:06:36.805500 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [P.], seq 1:88, ack 1, win 2058, options [nop,nop,TS val 1045875096 ecr 1690332192], length 87: HTTP: GET / HTTP/1.118:06:36.805517 IP 192.168.9.200.http &gt; 192.168.17.4.54349: Flags [.], ack 88, win 249, options [nop,nop,TS val 1690332192 ecr 1045875096], length 018:06:36.806109 IP 192.168.9.29.52620 &gt; 172.26.9.32.http: Flags [P.], seq 2174011696:2174011984, ack 1336389137, win 268, length 288: HTTP: GET / HTTP/1.118:06:36.807499 IP 172.26.9.32.http &gt; 192.168.9.29.52620: Flags [P.], seq 1:315, ack 288, win 285, length 314: HTTP: HTTP/1.1 403 Forbidden18:06:36.807520 IP 192.168.9.29.52620 &gt; 172.26.9.32.http: Flags [.], ack 315, win 285, length 018:06:36.807764 IP 192.168.9.200.http &gt; 192.168.17.4.54349: Flags [P.], seq 1:287, ack 88, win 249, options [nop,nop,TS val 1690332193 ecr 1045875096], length 286: HTTP: HTTP/1.1 403 Forbidden18:06:36.807947 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [.], ack 287, win 2054, options [nop,nop,TS val 1045875098 ecr 1690332193], length 018:06:36.808319 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [F.], seq 88, ack 287, win 2054, options [nop,nop,TS val 1045875098 ecr 1690332193], length 018:06:36.808393 IP 192.168.9.200.http &gt; 192.168.17.4.54349: Flags [F.], seq 287, ack 89, win 249, options [nop,nop,TS val 1690332193 ecr 1045875098], length 018:06:36.808586 IP 192.168.17.4.54349 &gt; 192.168.9.200.http: Flags [.], ack 288, win 2054, options [nop,nop,TS val 1045875098 ecr 1690332193], length 0---------------------------18:07:03.718999 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [S], seq 2867048974, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1045901890 ecr 0,sackOK,eol], length 018:07:03.719049 IP 192.168.9.200.http &gt; 192.168.17.4.54356: Flags [S.], seq 3283487703, ack 2867048975, win 15928, options [mss 1460,sackOK,TS val 1690338921 ecr 1045901890,nop,wscale 6], length 018:07:03.719350 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [.], ack 1, win 2058, options [nop,nop,TS val 1045901890 ecr 1690338921], length 018:07:03.719464 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [P.], seq 1:88, ack 1, win 2058, options [nop,nop,TS val 1045901890 ecr 1690338921], length 87: HTTP: GET / HTTP/1.118:07:03.719490 IP 192.168.9.200.http &gt; 192.168.17.4.54356: Flags [.], ack 88, win 249, options [nop,nop,TS val 1690338921 ecr 1045901890], length 018:07:03.719996 IP 192.168.9.29.50680 &gt; 172.26.9.31.http: Flags [P.], seq 1:289, ack 1, win 268, length 288: HTTP: GET / HTTP/1.118:07:03.726891 IP 172.26.9.31.http &gt; 192.168.9.29.50680: Flags [P.], seq 1:315, ack 289, win 285, length 314: HTTP: HTTP/1.1 403 Forbidden18:07:03.726920 IP 192.168.9.29.50680 &gt; 172.26.9.31.http: Flags [.], ack 315, win 285, length 018:07:03.727225 IP 192.168.9.200.http &gt; 192.168.17.4.54356: Flags [P.], seq 1:287, ack 88, win 249, options [nop,nop,TS val 1690338923 ecr 1045901890], length 286: HTTP: HTTP/1.1 403 Forbidden18:07:03.727542 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [.], ack 287, win 2054, options [nop,nop,TS val 1045901897 ecr 1690338923], length 018:07:03.727796 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [F.], seq 88, ack 287, win 2054, options [nop,nop,TS val 1045901897 ecr 1690338923], length 018:07:03.727896 IP 192.168.9.200.http &gt; 192.168.17.4.54356: Flags [F.], seq 287, ack 89, win 249, options [nop,nop,TS val 1690338923 ecr 1045901897], length 018:07:03.728119 IP 192.168.17.4.54356 &gt; 192.168.9.200.http: Flags [.], ack 288, win 2054, options [nop,nop,TS val 1045901897 ecr 1690338923], length 0 通过抓包结果可以得出，ingress gateway web代理的流程大致分为如下5个步骤: 123451.源client http request -&gt; 目的ingress gateway(web)2.ingress gateway 通过host及path，查询k8s Ingress资源，查询出相应的Ingress后，获取关联的service，再通过service获取相应的endpoint3.根据调度算法挑选出合适的endpoint，L7 web流量直接代理过去4.endpoint返回http响应到ingress gateway5.ingress gateway转发返回http响应给client 整个过程如我们猜想一致，中间并没有经过Service的ipvs或iptables的转发，而是直接将请求转发给了实际的endpoint,所以严格来说，这种方式，只使用了kube-router的pod网段路由，并没有使用其ipvs服务代理. 示意图 总结通过抓包分析，对kube-router的工作模式会有一个更为具体的认知，熟悉你的cni插件，在遇到集群网络故障排查问题时是极有帮助的，无论使用的是何种cni，一定要对其充分了解才能较放心地使用。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker registry仓库历史镜像批量清理]]></title>
    <url>%2F2018%2F11%2F07%2FDocker%20registry%E4%BB%93%E5%BA%93%E5%8E%86%E5%8F%B2%E9%95%9C%E5%83%8F%E6%89%B9%E9%87%8F%E6%B8%85%E7%90%86.html</url>
    <content type="text"><![CDATA[前言在jenkins CI/CD流水线中以自动打包并push镜像的方式运行了一段时间之后,docker registry中堆积的历史镜像数量极多,磁盘空间告急,为此,有必要定期做镜像的清理,并释放镜像占用的存储空间 清除原理Docker registry提供有restful api进行镜像管理,参考官方文档:https://docs.docker.com/registry/spec/api/ Docker存储使用的aufs文件系统分层存储结构,将容器文件以读写分层的形式存储在宿主机中.在registry容器中,存放镜像的分层数据在宿主机上的挂载路径为:/var/lib/docker/volumes/{container_id}/_data/docker/registry/v2/blobs关于docker aufs的存储模式,这篇文章写得非常通俗易懂,可以参考:https://www.cnblogs.com/sammyliu/p/5931383.html 一图简介上层镜像生成及删除过程中的分层文件处理原理:如图中文字解释,仅仅是调用api删除镜像是不够的,在删除了镜像之后,镜像的非共享分层文件还是会存放在磁盘中继续占用存储空间,因此,需要在删除镜像之后,使用docker registry自带的GC工具来进行垃圾分层(即无绑定镜像的分层)数据清除. 镜像分析在了解以上前提后,开始排查哪些registry repo的历史镜像较多(分层数量多)1.从宿主机进入docker registry容器内部,使用registry GC分析命令查看分层情况: registry garbage-collect --dry-run /etc/docker/registry/config.yml # --dry-run选项为layer层级分析,并不实际进行GC 2.可以便捷使用以下命令对分层数较多的镜像做一个排序: 1234567891011registry garbage-collect --dry-run /etc/docker/registry/config.yml &gt;&gt; res.txt6ac03183e197:~# cat res.txt | awk -F : '&#123;print $1&#125;' | sort | uniq -c | sort -rn -k1 | head -10 134161 zdtest 56101 ordertest 42691 bjdev 35881 zhqtest 13801 systemtest 9601 zddev 9361 bjtest 7411 dsystemtest 505 tooltest 可以看到,如上10个repo历史镜像数量大,需要清理 删除镜像注意:无论是delete方法调用restful接口,还是registry 自带工具的GC清理,都需要registry的配置文件中开启允许删除功能:/etc/docker/registry/config.yml123storage: delete: enabled: true 由于数量较多,因此使用python多线程来调用registry restful api进行删除操作,脚本内容如下,可根据自己的场景修改registry url: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsfrom concurrent.futures import ThreadPoolExecutorclass Docker(object): def __init__(self, hub, repos): self.hub = hub self.repos = repos @staticmethod def get_tag_list(hub, repo): # 获取这个repo的所有tags tag_list_url = '%s/v2/%s/tags/list' % (hub, repo) r1 = requests.get(url=tag_list_url) tag_list = r1.json().get('tags') return tag_list def main(self): thpool = ThreadPoolExecutor(10) for repo in self.repos: thpool.submit(self.delete_images, repo) thpool.shutdown(wait=True) def delete_images(self, repo): hub = self.hub tag_list = self.get_tag_list(hub=hub, repo=repo) num = 0 try: # 保留最后两个版本的镜像 for tag in tag_list[:-2]: # 获取image digest摘要信息 get_info_url = '&#123;&#125;/v2/&#123;&#125;/manifests/&#123;&#125;'.format(hub, repo, tag) header = &#123;"Accept": "application/vnd.docker.distribution.manifest.v2+json"&#125; r2 = requests.get(url=get_info_url, headers=header, timeout=10) digest = r2.headers.get('Docker-Content-Digest') # 删除镜像 delete_url = '%s/v2/%s/manifests/%s' % (hub, repo, digest) r3 = requests.delete(url=delete_url) if r3.status_code == 202: num += 1 except Exception as e: print(str(e)) print('仓库%s 共删除了%i个历史镜像' % (repo, num))if __name__ == '__main__': hub = 'http://registry.xxx.com:5000' repos = ['zdtest', 'ordertest', 'bjdev', 'zhqtest', 'systemtest', 'zddev', 'bjtest', 'dsystemtest', 'tooltest'] d = Docker(hub=hub, repos=repos) d.main() 运行结果: 123456789仓库tooltest 共删除了17个历史镜像仓库dsystemtest 共删除了245个历史镜像仓库bjtest 共删除了310个历史镜像仓库zddev 共删除了318个历史镜像仓库systemtest 共删除了463个历史镜像仓库zdtest 共删除了1574个历史镜像仓库zhqtest 共删除了300个历史镜像仓库bjdev 共删除了1421个历史镜像仓库ordertest 共删除了1868个历史镜像 空间清理回到docker registry容器内,直接运行GC命令,这次不再加 –dry-run选项 1registry garbage-collect /etc/docker/registry/config.yml 查看磁盘,可以发现磁盘容量已经空闲出许多了,镜像清理及存储空间释放完成!]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十三)、企业级docker仓库Harbor在kubernetes上搭建使用]]></title>
    <url>%2F2018%2F10%2F24%2Fk8s(%E5%8D%81%E4%B8%89)%E3%80%81%E4%BC%81%E4%B8%9A%E7%BA%A7docker%E4%BB%93%E5%BA%93Harbor%E5%9C%A8kubernetes%E4%B8%8A%E6%90%AD%E5%BB%BA%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言Harbor是Docker镜像仓库管理系统，用于企业级管理容器镜像，支持对接ldap进行权限管理,由VMVare中国团队开发。项目地址:https://github.com/goharbor/harbor/安装方式:1.Docker Compose方式部署:https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md2.通过kubernetes直接部署:新版本已废弃3.通过helm/kubernetes方式部署:https://github.com/goharbor/harbor-helm/blob/master/README.md推荐使用,本文将采用此方式 Helm安装helm简介Helm是一个用于Kubernetes的包管理工具,每一个安装包构成一个chart基本单位.想象一下,一个完整的部署,可能包含前端\后端\中间件\DB的部署实例,以及k8s管理对应的secret/configmap/svc/ing/pv/pvc等众多资源类型,在部署管理或者卸载之前,如果逐一编辑管理这些资源的yaml文件,工作量比较庞大且容易疏漏.helm的出现很好的解决了这个问题,使用模板以及变量替换的方式,按需生成部署yaml文件包,并且向kubernetes api发送调度请求,虽然额外地引入了一些学习成本,但对于部署包的管理无疑是带来了很大的便利性的. helm组件HelmHelm是一个cli客户端，可以完成如下内容： 1.创建/打包/调试Chart2.创建本地Chart仓库,管理本地和远程Chart仓库3.与Tiller交互并完成Chart的安装，升级，删除，回滚，查看等操作 Tiller1.监听helm客户端的请求,根据chart生成相应的release2.将release的资源信息发送给kubernetes api,实现资源增删改查.3.追踪release的状态,实现资源的更新/回滚操作 安装:安装较为简单,参考官方文档,不做复述:https://docs.helm.sh/using_helm/#installing-helm Harbor安装这里采用次新版本的v1.5.3版本 123root@h1:# wget https://github.com/goharbor/harbor/archive/v1.5.3.tar.gzroot@h1:# tar -xf https://github.com/goharbor/harbor/archive/v1.5.3.tar.gz root@h1:# cd harbor-1.5.3/contrib/helm/harbor/ helm方式安装的harbor默认是https的,因此需要k8s集群ingress网关开启https协议,traefik开启https参考此前文章:k8s（二）、对外服务 更新helm dependency:harbor的helm部署依赖了postgresql的helm，在官方的安装文档没有明确说明，直接按照官方文档说明安装，就会缺失postgresql的部署，导致helm install failed 1helm dependency update 修改value.yaml文件,添加pvc持久存储并将相应组件上下文中的volume部分注释打开: 12345678910111213141516171819mysql: volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnce adminserver: volumes: config: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 1Gregistry: volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 5G 完整value.yaml文件如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327# Configure persisten Volumes per application## Applications that require storage have a `volumes` definition which will be used## when `persistence.enabled` is set to true.## example# mysql:# volumes:# data:## Persistent Volume Storage Class## If defined, storageClassName: &lt;storageClass&gt;## If set to "-", storageClassName: "", which disables dynamic provisioning## If undefined (the default) or set to null, no storageClassName spec is## set, choosing the default provisioner. (gp2 on AWS, standard on## GKE, AWS &amp; OpenStack)### storageClass: "-"# accessMode: ReadWriteOnce# size: 1Gimysql: volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnceadminserver: volumes: config: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 1Gregistry: volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 5G## Configure resource requests and limits per application## ref: http://kubernetes.io/docs/user-guide/compute-resources/### mysql:# resources:# requests:# memory: 256Mi# cpu: 100mpersistence: enabled: true# The tag for Harbor docker images.harborImageTag: &amp;harbor_image_tag v1.4.0# The FQDN for Harbor service.externalDomain: harbor.my.domain# If set to true, you don't need to set tlsCrt/tlsKey/caCrt, but must add# Harbor FQDN as insecure-registries for your docker client.insecureRegistry: false#insecureRegistry: true# The TLS certificate for Harbor. The common name of tlsCrt must match the externalDomain above.tlsCrt: #tlsCrt: root/traefik/ssl/kokoerp.crttlsKey:#tlsKey: root/traefik/ssl/kokoerp.keycaCrt:# The secret key used for encryption. Must be a string of 16 chars.secretKey: not-a-secure-key# These annotations allow the registry to work behind the nginx# ingress controller.ingress: annotations: ingress.kubernetes.io/ssl-redirect: "true" ingress.kubernetes.io/body-size: "0" ingress.kubernetes.io/proxy-body-size: "0"adminserver: image: repository: vmware/harbor-adminserver tag: *harbor_image_tag pullPolicy: IfNotPresent emailHost: "smtp.mydomain.com" emailPort: "25" emailUser: "sample_admin@mydomain.com" emailSsl: "false" emailFrom: "admin &lt;sample_admin@mydomain.com&gt;" emailIdentity: "" emailInsecure: "False" emailPwd: not-a-secure-password adminPassword: Harbor12345 authenticationMode: "db_auth" selfRegistration: "on" ldap: url: "ldaps://ldapserver" searchDN: "" baseDN: "" filter: "(objectClass=person)" uid: "uid" scope: "2" timeout: "5" verifyCert: "True" ## Persist data to a persistent volume volumes: config: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 1Gi # resources: # requests: # memory: 256Mi # cpu: 100m## jobservice#jobservice: image: repository: vmware/harbor-jobservice tag: *harbor_image_tag pullPolicy: IfNotPresent secret: not-a-secure-secret# resources:# requests:# memory: 256Mi# cpu: 100m## UI#ui: image: repository: vmware/harbor-ui tag: *harbor_image_tag pullPolicy: IfNotPresent secret: not-a-secure-secret privateKeyPem: | -----BEGIN RSA PRIVATE KEY----- MIIJKAIBAAKCAgEA4WYbxdrFGG6RnfyYKlHYML3lEqtA9cYWWOynE9BeaEr/cMnM bBr1dd91/Nm6RiYhQvTDU2Kc6NejqjdliW5B9xUoVKayri8OU81a8ViXeNgKwCPR AiTTla1zoX5DnvoxpO9G3lxyNvTKXc0cw8NjQDAXpaDbzJYLkshCeuyD9bco8R96 /zrpBEX8tADN3+3yA3fMcZzVXsBm4BTpHJRk/qBpHYEPSHzxyH3iGMNKk3vMUBZz e0EYkK8NCA2CuEKMnC3acx9IdRwkx10abGvHQCLRCVY7rGoak+b0oZ99RJIRQ9Iq YXsn8fsMBQly6xxvSeY5XuSP7Xb6JKDt3y8Spi4gR1M/5aEzhuOyu201rMna7Rs/ GPfaKjBlbX0jiLDa7v4zjsBPsPaf/c4uooz3ICLsdukaom+E538R0EiOkXt/wyw2 2YmaWNCsYlEpke7cVC33e/0dPBq4IHsVflawSF9OWS23ikVAs/n+76KjuucEDmbT aKUYAJjvAmZL14j+EKc/CoplhCe6pKhavjmNIOfCSdlreIPBhOVbf1f817wKoSIZ qVyCA1AYNkI9RYS00axtJGBGMlKbdQqCNpLL58c6To2awmckIZCEcATKOp++NoGm Ib0bhdSasdGB5VCtwZVluN8bLl13zBKoxTGjNlEatUGDRnDAnLdZbXXffjsCAwEA AQKCAgBEUigO8/4UJse6xKr3APHv7E94NjKtjMqPT8RhDCLhqAH/lRuClTVb8k0Y RILi6oHggsKGDvkS1vJEESCU5LfYBjDAX/r/M0I7gp6TU1AukAXKMdETvkfoMbg/ 9j7W/G152hF4KztvjwmcHyUd7aay+SDh0n1taPm/FzaXfgONwmQFmo40uQ2SfwhX I3tD6iMWjASLV4eRfe5w88WpJQ3r5IGYMNuKFF1RcV7MNL3xMHBAwl1kudmRWY4w p6+83Gc0m+2AQbY70TkQuRbeUFkIBsWn99yEqXC+7h2us+JLm57iGN1ByQvVnEwL Zs7Pl0Hge4leSxeZWhv+aE1R/jm/VdG4dglInuhED0ug8WAJg58IkDYfMKOOALHx +0CNHE02XqqUIFwboZJSYTjMYvFL1i14L30FWnqH/0kDs4whXHbnGWhVustsMSK9 iyIGepuGhMnvtUF1wa/SrBd12qfDj68QHDXsKKbs6eTNYHfn3QL9uisrfMIa5HAt nX2YOsAVxg+yvxkWD6n1DU+a/+pAu6iAgiwyxSZiyn6vJUE2zO6pJNbk1kJW6jU3 A69srtbO4jQn4EM859XYSqdqwXgJL+XJEYNbBcHalmiIOvRg9CCvDSKS7M5rJ0M1 L7oCzl6EW+zUb4JHkSO7V5uxIZu2sEduw5gofQ3OT9L/qDhDIQKCAQEA8T/8okF2 Q7SOj3su6KKX6H/ab31SvHECf/oeJtH8ZfLBYL55Yof0pZwq8iXQ26d8cH7FPKBo hz0RZ9i2S3bYkzEVCPv9ISFg1NACxL3dU0PMBnmbmg2vPhMzEuQI2JOUu6ILOXEN mImvfjZXps/b8OjQgzicH0skBBcbUlXT3a4fF52ktC8FiXgBG9JYg5CsXmfPRxci ITa4w4ZLEuECmtJieS0MdKXPLwUVv3e2BlNS6c1JzXyp6EyX/euJ8cCe3n/GHbTY 2j1OO+xTQfQJVf6S9f2mSzjdHe9KZwWKgyxQ9dZ9Qtho2z/gUN9/UkL52fdljjlw ++b/z9Ppcl9K0QKCAQEA7y4Fv8dPFLLnr0R/S7eoAKa0S95xVe97EJHVUhWyOI09 K9VdZHp6be8W0Yd9h/Ks8Zi4EPRiTTaF3yA3iADwdKFeZt49jGzeM+Gl7Q2Ll98W I5gOdJkHSVAP2uK7qSjZ8lPCu4iUYRsae+Psam7Yd6X17RP0M966PlUFj1nnrJjQ EN4zeh/m01q9vqebB9C1W/ZiJ6rpt6VVHAcOQQ69F/lKdTif4XCvbMIhIXTYNifk 1oIv2qTDnfzzv+bgrlvpBJPpPYR0Oc7WoEpyd1Y9IzienLZi8RnujV//FXEmJ45E F9GE1HOmoERdEWA1bMYhOO5OfRY1HSMuFMA4+5ojSwKCAQEAmwubio/1uMemw3HQ kPRGGsdolDR/4tniWGtfy2UzCDY+r7Vaf8eOpIy8UQmatEBsykO+8RrKcvf9Yrc1 WUSVJevqb+67HPq9p6fTz6uSPXwZ+KNZLGXVFVjzfxWM1dvrP7eB7TXKHhmG7t9v 76Yw3SBTObI9LCN3jyVmisDcO+E23E+VVbPOpC260K2b81ocXUPsQ+0LIztu/UIm p4hyyxug6+3WznTttXNYKch+9IvCgr5Ly0NuUvw+xpMFAZjgwXBu3BKpN4Ek8YAN dhqnkVveCTguErQF78IlGBbIkUr+8TAbKsW4hggEWxV4V17yAnJsEz65bTtldqTj qHyzsQKCAQBGhv6g/2d9Rgf1cbBLpns+vel6Wbx3x6c1SptpmgY0kMlR7JeeclM5 qX/EBzzn4pJGp27XaQi3lfVBxyE41HYTHiZVFQF3L/8Rs18XGKBqBxljI4pXrWwt nRMfyy3lAqvJvhM082A1hiV4FMx40fi4x1JON00SIoIusSlzjOI4zdLEtpDdWRza g+5hktCvLEbeODfXVJmYUoNXQWldm7f8osDm8eyLMIw5+MCGOgsrZPYgnsD3qxAX vSgvFSh5oZaDiA4F2tHe3fQBzhIUyHQ8t4xlz447ZBcozv7L1tKWZWgE0f5mGzgu GBqNbh4y1fWj8Plp/ytoTSBgdBIZdukjAoIBAELJPSVFnlf/gv6OWRCHyKxquGjv fEn/E8bw5WSqMcj/7wiSJozr0Y8oyWjtWXObliLRQXcEhC8w3lLMjNqnFzQOAI7s Oa6BQPigqyXZPXG5GK+V0TlUYvZQn9sfCq4YCxUBNtQ4GHbKKl3FGQL3rJiuFr6G fVcetuDFNCiIGYbUF+giJ2cEN3a/Q+7fR6V4xC7VDdL+BqM09wZ6R98G48XzCKKp ekNpEfmvJiuk9tFFQwDPWcQ6uyHqesK/Wiweo5nh5y2ZPipwcb0uBoYOQH60NqEL 6MXRVNdtKujjl1XZkG053Nvcz/YfF6lFjDekwgfd9m49b/s0EGTrl7z9z8Y= -----END RSA PRIVATE KEY-----# resources:# requests:# memory: 256Mi# cpu: 100m## MySQL Settings. Currently Harbor does not support an external## MySQL server, only their own image. Until this is fixed, do not## Change the settings below.#mysql: image: repository: vmware/harbor-db tag: *harbor_image_tag pullPolicy: IfNotPresent # If left blank will use the included mysql service name. host: ~ port: 3306 user: "root" pass: "registry" database: "registry" volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 1Gi # resources: # requests: # memory: 256Mi # cpu: 100mregistry: image: repository: vmware/registry-photon tag: v2.6.2-v1.4.0 pullPolicy: IfNotPresent httpSecret: not-a-secure-secret logLevel:# comment out one of the below to use your cloud's object storage.# objectStorage:# gcs:# keyfile: ""# bucket: ""# chunksize: "5242880"# s3:# region: ""# accesskey: ""# secretkey: ""# bucket: ""# encrypt: "true"# azure:# accountname: ""# accountkey: ""# container: "" rootCrt: | -----BEGIN CERTIFICATE----- MIIE0zCCArugAwIBAgIJAIgs3S+hsjhmMA0GCSqGSIb3DQEBCwUAMAAwHhcNMTcx MTA5MTcyNzQ5WhcNMjcxMTA3MTcyNzQ5WjAAMIICIjANBgkqhkiG9w0BAQEFAAOC Ag8AMIICCgKCAgEA4WYbxdrFGG6RnfyYKlHYML3lEqtA9cYWWOynE9BeaEr/cMnM bBr1dd91/Nm6RiYhQvTDU2Kc6NejqjdliW5B9xUoVKayri8OU81a8ViXeNgKwCPR AiTTla1zoX5DnvoxpO9G3lxyNvTKXc0cw8NjQDAXpaDbzJYLkshCeuyD9bco8R96 /zrpBEX8tADN3+3yA3fMcZzVXsBm4BTpHJRk/qBpHYEPSHzxyH3iGMNKk3vMUBZz e0EYkK8NCA2CuEKMnC3acx9IdRwkx10abGvHQCLRCVY7rGoak+b0oZ99RJIRQ9Iq YXsn8fsMBQly6xxvSeY5XuSP7Xb6JKDt3y8Spi4gR1M/5aEzhuOyu201rMna7Rs/ GPfaKjBlbX0jiLDa7v4zjsBPsPaf/c4uooz3ICLsdukaom+E538R0EiOkXt/wyw2 2YmaWNCsYlEpke7cVC33e/0dPBq4IHsVflawSF9OWS23ikVAs/n+76KjuucEDmbT aKUYAJjvAmZL14j+EKc/CoplhCe6pKhavjmNIOfCSdlreIPBhOVbf1f817wKoSIZ qVyCA1AYNkI9RYS00axtJGBGMlKbdQqCNpLL58c6To2awmckIZCEcATKOp++NoGm Ib0bhdSasdGB5VCtwZVluN8bLl13zBKoxTGjNlEatUGDRnDAnLdZbXXffjsCAwEA AaNQME4wHQYDVR0OBBYEFCMYYMOL0E/Uyj5wseDfIl7o4ELsMB8GA1UdIwQYMBaA FCMYYMOL0E/Uyj5wseDfIl7o4ELsMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQEL BQADggIBABG8fPvrrR+erpwQFuB/56j2i6sO+qoOJPpAMYwkzICrT0eerWAavwoy f0UAKN7cUeEJXjIR7s7CogGFijWdaWaQsXUD0zJq5aotLYZLimEc1O0uAmJEsfYC v7mG07eU6ge22sSo5hxhVplGt52hnXnT0DdgSRZpq2mvgd9lcopAidM+KHlaasXk IecHKM99KX9D8smr0AcQ6M/Ygbf2qjO9YRmpBIjyQWEake4y/4LWm+3+v08ecg4B g+iMC0Rw1QcPqgwaGaWu71RtYhyTg7SnAknb5nBcHIbLb0hdLgQTa3ZdtXgqchIi GuFlEBmHFZP6bLJORRUQ0ari5wpXIsYfrB4T8PybTzva3OCMlEsMjuysFr9ewhzM 9UGLiSQNDyKA10J8WwlzbeD0AAW944hW4Dbg6SWv4gAo51T+6AukRdup5y6lfQ5a h4Lbo6pzaA369IsJBntvKvia6hUf/SghnbG7pCHX/AEilcgTb13HndF/G+7aZgKR mi9qvNRSDsE/BrgZawovp81+j6aL4y6UtXYspHr+SuWsKYsaH7pl5HspNCyJ5vV6 dpJAwosFBqSEnI333wAunpMYmi/jKHH/j4WqjLnCInp0/wouzYu42l8Pmz591BSp Jag500bEBxqI2RLELgMt/bUdjp4N2M7mrxdrN+2579HTzb6Hviu9 -----END CERTIFICATE----- ## Persist data to a persistent volume volumes: data: storageClass: "cephrbd" accessMode: ReadWriteOnce size: 5Gi # resources: # requests: # memory: 256Mi # cpu: 100mclair: enabled: true image: repository: vmware/clair-photon tag: v2.0.1-v1.4.0 pullPolicy: IfNotPresent## The following needs to match the credentials## in the `postgresql` configuration under the## `postgresql` namespace below. postgresPassword: not-a-secure-password postgresUser: clair postgresDatabase: clair# resources:# requests:# memory: 256Mi# cpu: 100m# pgResources:# requests:# memory: 256Mi# cpu: 100m# volumes:# pgData:# storageClass: "cephrbd"# accessMode: ReadWriteOnce# size: 1Gi # resources: # requests: # memory: 256Mi # cpu: 100m## Notary support is not yet fully implemented in the Helm Charts## Enabling it will just break things.#notary: enabled: false## Settings for postgresql dependency.## see https://github.com/kubernetes/charts/tree/master/stable/postgresql## for further configurables.postgresql: postgresUser: clair postgresPassword: not-a-secure-password postgresDatabase: clair persistence: # enabled: false enabled: true storageClass: "cephrbd" accessMode: ReadWriteOnce size: 8Gi 部署:1helm install . --debug --name hub --set externalDomain=hub.test.com 等待一段时间的镜像拉取后,查看pod:1234567hub-harbor-adminserver-0 1/1 Running 1 3hhub-harbor-clair-6c7d9dcdb7-q4lv4 1/1 Running 2 3hhub-harbor-jobservice-75f7fbcc9c-ggwp4 1/1 Running 2 3hhub-harbor-mysql-0 1/1 Running 0 3hhub-harbor-registry-0 1/1 Running 1 3hhub-harbor-ui-57b4674ff9-kcfq6 1/1 Running 0 3hhub-postgresql-ccf8d56d5-jg4wq 1/1 Running 1 3h 添加dns,指向ingress gateway(即traefik的node ip): 1echo "192.168.1.238 hub.test.com" &gt;&gt; /etc/hosts 浏览器打开测试,默认登录口令为admin/Harbor12345可以看到默认有一个公开项目library,尝试往这里推送镜像 Docker推送测试找一台安装了docker的机器,修改docker服务脚本:1234~# cat /lib/systemd/system/docker.service#ExecStart=/usr/bin/dockerd -H fd://ExecStart=/usr/bin/dockerd --insecure-registry hub.kokoerp.com 重启docker并登录测试:1234567~# systemctl daemon-reload~# systemctl restart dockerroot@h2:~# docker login hub.test.comUsername (admin): adminPassword: Login Succeeded 推送镜像测试: 12345root@h2:~# docker tag busybox hub.test.com/library/busybox:test1root@h2:~# docker push hub.test.com/library/busybox:test1The push refers to a repository [hub.test.com/library/busybox]8a788232037e: Pushed test1: digest: sha256:915f390a8912e16d4beb8689720a17348f3f6d1a7b659697df850ab625ea29d5 size: 527 可以看到推送成功 ldap配置: harbor可以基于企业内部的ldap来获取人员信息,可以在ui上创建非公开项目,将ldap中获取的人员加入项目并赋予相应pull push权限,ui操作界面比较友好,这里就不展示了.]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cephfs & Ceph RBD 在k8s中的适用场景讨论及数据库性能压测]]></title>
    <url>%2F2018%2F09%2F29%2FCephfs-CephRBD%E5%9C%A8k8s%E4%B8%AD%E7%9A%84%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%E8%AE%A8%E8%AE%BA%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%BA%93%E6%80%A7%E8%83%BD%E5%8E%8B%E6%B5%8B.html</url>
    <content type="text"><![CDATA[## 前言在此前的文章中,已经针对cephfs进行过I/O性能压力测试:cephfs调优 &amp; 性能测试 &amp; 监控 &amp; 常用命令在上一篇文章中,完成了k8s 对接ceph rbd的测试:k8s(十二)、分布式存储Ceph RBD使用 测试发现cephfs的小文件读写性能一般,且写入延迟偏高,性能不甚满意,但是满足于日常应用环境的读写是没有问题的,但是在面对数据库的应用场景,是否能满足性能要求呢?本篇主要结合kubernetes,针对数据库应用场景,对cephfs 和 ceph rbd这两种ceph存储接口来进行性能对比测试. 在对比之前,再贴一下k8s对于cephfs和cephrbd的接入模式的支持: Ceph RBD调优在测试之前,首先对ceph rbd进行针对性的配置优化ceph.conf:123456789[client]rbd cache size = 268435456 #默认32Mrbd cache max dirty = 134217728 #最大未落盘脏数据大小,默认16M,此值小于cache sizerbd cache max dirty age = 5 #最大未落盘脏数据存放时间,默认1,单位srbd cache max dirty object = 64 #最大未落盘脏数据对象数量,默认object大小为4M,因此建议值为rbd cache max dirty / 4#osd段增加线程数,默认为1,不要过大,防止线程锁[osd]rbd op threads = 4 配置完成后逐个重启ceph节点 测试工具1.sysbench项目地址:akopytov/sysbenchsysbench是跨平台的基准测试工具，使用预定义或自定义的lua脚本灵活地进行性能测试,支持多线程，支持多种数据库；主要包括以下几种方面的性能：cpu性能磁盘io性能调度程序性能内存分配及传输速度POSIX线程性能数据库性能(OLTP基准测试) 使用参考:https://www.cnblogs.com/kismetv/archive/2017/09/30/7615738.html 本文将进行数据库 事务/非事务 只读(查询)/混合读写基准性能测试。 2.TPCCTPC-C是专门针对联机交易处理系统（OLTP系统）的规范Tpcc-mysql由percona根据tpcc规范实现以订单为中心的多表联表操作性能测试,更贴切于业务场景.工具内置业务场景模拟逻辑图:使用参考:https://www.cnblogs.com/Aiapple/p/5705146.html 测试为方便部署,全部使用容器测试,容器运行起来后本质即是一组进程,cpu/内存/磁盘 性能损耗可以说比虚拟化还小,因此可以排除性能损耗的干扰 配置规格:内存: 64GCPU: E5-2683 v4 *2网卡: 10GE存储介质分为4种场景:1.机械硬盘 raid5 16 * ssd raid 5 cephfs(ssd) ceph rbd(ssd) 测试方法可参考上方链接中的文章,这里直接贴结果: 结果分析: ssd raid性能毫无疑问是最好的 ceph rbd 数据库qps/tps可达ssd raid的60%-70%, cephfs因为写入延迟不稳定的原因,压测过程中极小比例的操作响应时间非常漫长,导致qps/tps值整体表现不佳 hdd测试得到的qps/tps值中规中矩,操作最低响应时间较其他三者要高,但最高响应时间值也不会很高.然而机械硬盘介质决定了随着它的负载增高寻址时间会随之加长,性能将会呈线性下降. 适用场景讨论Cephfs: 优点: 1.读取延迟低,I/O带宽表现良好,尤其是block size较大一些的文件 2.灵活度高,支持k8s的所有接入模式 缺点: 1.写入延迟相对较高且延迟时间不稳定适用场景:适用于要求灵活度高(支持k8s多节点挂载特性),对I/O延迟不甚敏感的文件读写操作,以及非海量的小文件存储支持.例如作为常用的应用/中间件挂载存储后端. Ceph RBD:优点:1.I/O带宽表现良好2.读写延迟都很低3.支持镜像快照,镜像转储缺点:1.不支持多节点挂载适用场景:对I/O带宽和延迟要求都较高,且无多个节点同时读写数据需求的应用,例如数据库. 欢迎补充]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十二)、分布式存储Ceph RBD使用]]></title>
    <url>%2F2018%2F09%2F29%2Fk8s(%E5%8D%81%E4%BA%8C)%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8Ceph%20RBD%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言上篇文章介绍了k8s使用pv/pvc 的方式使用cephfs,k8s(十一)、分布式存储Cephfs使用 Ceph存储有三种存储接口,分别是:对象存储 Ceph Object Gateway块设备 RBD文件系统 CEPHFS Kubernetes支持后两种存储接口,支持的接入模式如下图: 在本篇将测试使用ceph rbd作持久化存储后端 RBD创建测试rbd的使用分为3个步骤:1.服务端/客户端创建块设备image2.客户端将image映射进linux系统内核,内核识别出该块设备后生成dev下的文件标识3.格式化块设备并挂载使用 12345678910111213141516171819202122232425# 创建[root@h020112 ~]# rbd create --size 10240000 rbd/test1 #创建指定大小的node[root@h020112 ~]# rbd info test1rbd image 'test1': size 10000 GB in 2560000 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.31e3b6b8b4567 format: 2 features: layering, exclusive-lock,object-map, fast-diff, deep-flatten flags: #映射进内核操作之前,首先查看内核版本,jw版本的ceph默认format为2,默认开启5种特性,2.x及之前的内核版本需手动调整format为1,4.x之前要关闭object-map, fast-diff, deep-flatten功能才能成功映射到内核,这里使用的是centos7.4,内核版本3.10[root@h020112 ~]# rbd feature disable test1 object-map fast-diff deep-flatten# 映射进内核[root@h020112 ~]# rbd map test1[root@h020112 ~]# ls /dev/rbd0/dev/rbd0#挂载使用[root@h020112 ~]# mkdir /mnt/cephrbd[root@h020112 ~]# mkfs.ext4 -m0 /dev/rbd0[root@h020112 ~]# mount /dev/rbd0 /mnt/cephrbd/ Kubernetes Dynamic Pv 使用首先回顾一下上篇cephfs在k8s中的使用方式:绑定使用流程: pv指定存储类型和接口,pvc绑定pv,pod挂载指定pvc.这种一一对应的方式,即为静态pv例如cephfs的使用流程: 那么什么是动态pv呢?在现有PV不满足PVC的请求时，可以使用存储分类(StorageClass)，pvc对接sc后,k8s会动态地创建pv,并且向相应的存储后端发出资源申请.描述具体过程为：首先创建sc分类，PVC请求已创建的sc的资源，来自动创建pv,这样就达到动态配置的效果。即通过一个叫 Storage Class的对象由存储系统根据PVC的要求自动创建pv并使用存储资源 Kubernetes的pv支持rbd,从第一部分测试使用rbd的过程可以看出,存储实际使用的单位是image,首先必须在ceph端创建image,才能创建pv/pvc,再在pod里面挂载使用.这是一个标准的静态pv的绑定使用流程.但是这样的流程就带来了一个弊端,即一个完整的存储资源挂载使用的操作被分割成了两段,一段在ceph端划分rbd image,一段由k8s端绑定pv,这样使得管理的复杂性提高,不利于自动化运维. 因此,动态pv是一个很好的选择,ceph rbd接口的动态pv的实现流程是这样的:根据pvc及sc的声明,自动生成pv,且向sc指定的ceph后端发出申请自动创建相应大小的rbd image,最终绑定使用 工作流程图: 下面开始演示1.创建secretkey获取方式参考上篇文章12345678~/mytest/ceph/rbd# cat ceph-secret-rbd.yaml apiVersion: v1kind: Secretmetadata: name: ceph-secret-rbdtype: "kubernetes.io/rbd"data: key: QVFCL3E1ZGIvWFdxS1JBQTUyV0ZCUkxldnRjQzNidTFHZXlVYnc9PQ== 2.创建sc1234567891011121314151617~/mytest/ceph/rbd# cat ceph-secret-rbd.yamlkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: cephrbd-test2provisioner: kubernetes.io/rbdreclaimPolicy: Retainparameters: monitors: 192.168.20.112:6789,192.168.20.113:6789,192.168.20.114:6789 adminId: admin adminSecretName: ceph-secret-rbd pool: rbd userId: admin userSecretName: ceph-secret-rbd fsType: ext4 imageFormat: "2" imageFeatures: "layering" 创建pvc123456789101112~/mytest/ceph/rbd# cat cephrbd-pvc.yaml kind: PersistentVolumeClaimapiVersion: v1metadata: name: cephrbd-pvc2spec: storageClassName: cephrbd-test2 accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 创建deploy,容器挂载使用创建的pvc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253:~/mytest/ceph/rbd# cat deptest13dbdm.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: deptest13dbdm name: deptest13dbdm namespace: defaultspec: replicas: 1 selector: matchLabels: app: deptest13dbdm strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: deptest13dbdm spec: containers: image: registry.xxx.com:5000/mysql:56v6 imagePullPolicy: Always name: deptest13dbdm ports: - containerPort: 3306 protocol: TCP resources: limits: memory: 16Gi requests: memory: 512Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/mysql name: datadir subPath: deptest13dbdm resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - name: datadir persistentVolumeClaim: claimName: cephrbd-pvc2 这里会遇到一个大坑: 123~# kubectl describe pvc cephrbd-pvc2 ...Error creating rbd image: executable file not found in $PATH 这是因为kube-controller-manager需要调用rbd客户端api来创建rbd image,集群的每个节点都已经安装ceph-common,但是集群是使用kubeadm部署的,kube-controller-manager组件是以容器方式工作的,内部不包含rbd的执行文件,后面在这个issue里,找到了解决的办法:Github issue使用外部的rbd-provisioner,提供给kube-controller-manager以rbd的执行入口,注意,issue内的demo指定的image版本有bug,不能指定rbd挂载后使用的文件系统格式,需要使用最新的版本demo,项目地址:rbd-provisioner完整demo yaml文件:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990root@h009027:~/mytest/ceph/rbd/rbd-provisioner# cat rbd-provisioner.yaml kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["services"] resourceNames: ["kube-dns","coredns"] verbs: ["list", "get"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionersubjects: - kind: ServiceAccount name: rbd-provisioner namespace: defaultroleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: rbd-provisionerrules:- apiGroups: [""] resources: ["secrets"] verbs: ["get"]- apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbd-provisionerroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisionersubjects:- kind: ServiceAccount name: rbd-provisioner namespace: default---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: rbd-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: quay.io/external_storage/rbd-provisioner:latest env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisioner---apiVersion: v1kind: ServiceAccountmetadata: name: rbd-provisioner 部署完成后,重新创建上方的sc/pv,这个时候,pod就正常运行起来了,查看pvc,可以看到此pvc触发自动生成了一个pv且绑定了此pv,pv绑定的StorageClass则是刚创建的sc:123456789101112131415161718# pod~/mytest/ceph/rbd/rbd-provisioner# kubectl get pods | grep deptest13deptest13dbdm-c9d5bfb7c-rzzv6 2/2 Running 0 6h# pvc~/mytest/ceph/rbd/rbd-provisioner# kubectl get pvc cephrbd-pvc2NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEcephrbd-pvc2 Bound pvc-d976c2bf-c2fb-11e8-878f-141877468256 20Gi RWO cephrbd-test2 23h# 自动创建的pv~/mytest/ceph/rbd/rbd-provisioner# kubectl get pv pvc-d976c2bf-c2fb-11e8-878f-141877468256NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-d976c2bf-c2fb-11e8-878f-141877468256 20Gi RWO Retain Bound default/cephrbd-pvc2 cephrbd-test2 23h# sc~/mytest/ceph/rbd/rbd-provisioner# kubectl get sc cephrbd-test2NAME PROVISIONER AGEcephrbd-test2 ceph.com/rbd 23h 再回到ceph节点上看看是否自动创建了rbd image123456789101112~# rbd lskubernetes-dynamic-pvc-d988cfb1-c2fb-11e8-b2df-0a58ac1a084etest1~# rbd info kubernetes-dynamic-pvc-d988cfb1-c2fb-11e8-b2df-0a58ac1a084erbd image 'kubernetes-dynamic-pvc-d988cfb1-c2fb-11e8-b2df-0a58ac1a084e': size 20480 MB in 5120 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.47d0b6b8b4567 format: 2 features: layering flags: 可以发现,动态创建pv/rbd成功 挂载测试回顾最上方的kubernetes支持的接入模式截图,rbd是不支持ReadWriteMany的,而k8s的理念是像牲口一样管理你的容器,容器节点有可能随时横向扩展,横向缩减,在节点间漂移,因此,不支持多点挂载的RBD模式,显然不适合做多应用节点之间的共享文件存储,这里进行一次进一步的验证 多点挂载测试创建一个新的deploy,并挂载同一个pvc:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879root@h009027:~/mytest/ceph/rbd# cat deptest14dbdm.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: "4" creationTimestamp: 2018-09-26T07:32:41Z generation: 4 labels: app: deptest14dbdm domain: dev.deptest14dbm.kokoerp.com nginx_server: "" name: deptest14dbdm namespace: default resourceVersion: "26647927" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/deptest14dbdm uid: 5a1a62fe-c15e-11e8-878f-141877468256spec: replicas: 1 selector: matchLabels: app: deptest14dbdm strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: deptest14dbdm domain: dev.deptest14dbm.kokoerp.com nginx_server: "" spec: containers: - env: - name: DB_NAME value: deptest14dbdm image: registry.youkeshu.com:5000/mysql:56v6 imagePullPolicy: Always name: deptest14dbdm ports: - containerPort: 3306 protocol: TCP resources: limits: memory: 16Gi requests: memory: 512Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/mysql name: datadir subPath: deptest14dbdm - env: - name: DATA_SOURCE_NAME value: exporter:6qQidF3F5wNhrLC0@(127.0.0.1:3306)/ image: registry.youkeshu.com:5000/mysqld-exporter:v0.10 imagePullPolicy: Always name: mysql-exporter ports: - containerPort: 9104 protocol: TCP resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst nodeSelector: 192.168.20.109: "" restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - name: datadir persistentVolumeClaim: claimName: cephrbd-pvc2 使用nodeselector将此deptest13和deptest14限制在同一个node中:123root@h009027:~/mytest/ceph/rbd# kubectl get pods -o wide | grep deptest1[3-4]deptest13dbdm-c9d5bfb7c-rzzv6 2/2 Running 0 6h 172.26.8.82 h020109deptest14dbdm-78b8bb7995-q84dk 2/2 Running 0 2m 172.26.8.84 h020109 不难发现在同一个节点中,是可以支持多个pod共享rbd image运行的,在容器的工作节点上查看mount条目也可以证实这一点: 1234192.168.20.109:~# mount | grep rbd/dev/rbd0 on /var/lib/kubelet/plugins/kubernetes.io/rbd/rbd/rbd-image-kubernetes-dynamic-pvc-d988cfb1-c2fb-11e8-b2df-0a58ac1a084e type ext4 (rw,relatime,stripe=1024,data=ordered)/dev/rbd0 on /var/lib/kubelet/pods/68b8e3c5-c38d-11e8-878f-141877468256/volumes/kubernetes.io~rbd/pvc-d976c2bf-c2fb-11e8-878f-141877468256 type ext4 (rw,relatime,stripe=1024,data=ordered)/dev/rbd0 on /var/lib/kubelet/pods/1e9b04b1-c3c3-11e8-878f-141877468256/volumes/kubernetes.io~rbd/pvc-d976c2bf-c2fb-11e8-878f-141877468256 type ext4 (rw,relatime,stripe=1024,data=ordered) 修改新deploy的nodeselector指向其他的node:1234567~/mytest/ceph/rbd# kubectl get pods -o wide| grep deptest14dbdeptest14dbdm-78b8bb7995-q84dk 0/2 Terminating 0 7m 172.26.8.84 h020109deptest14dbdm-79fd495579-rkl92 0/2 ContainerCreating 0 49s &lt;none&gt; h009028# kubectl describe pod deptest14dbdm-79fd495579-rkl92... Warning FailedAttachVolume 1m attachdetach-controller Multi-Attach error for volume "pvc-d976c2bf-c2fb-11e8-878f-141877468256" Volume is already exclusively attached to one node and can't be attached to another pod将阻塞在ContainerCreating状态,描述报错很清晰地指出控制器不支持多点挂载此pv.进行下一步测试前先删除此临时新增的deploy 滚动更新测试在deptest13dbdm的deploy文件中指定的更新方式为滚动更新:12345strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate 这里我们尝试一下,将正在运行的pod,从一个节点调度到另一个节点,rbd 是否能迁移到新的节点上挂载: 123456789101112131415161718192021222324252627282930313233# 修改nodeselectorroot@h009027:~/mytest/ceph/rbd# kubectl edit deploy deptest13dbdmdeployment "deptest13dbdm" edited# 观察pod调度迁移过程root@h009027:~/mytest/ceph/rbd# kubectl get pods -o wide| grep deptest13deptest13dbdm-6999f5757c-fk9dp 0/2 ContainerCreating 0 4s &lt;none&gt; h009028deptest13dbdm-c9d5bfb7c-rzzv6 2/2 Terminating 0 6h 172.26.8.82 h020109root@h009027:~/mytest/ceph/rbd# kubectl describe pod deptest13dbdm-6999f5757c-fk9dp ...Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m default-scheduler Successfully assigned deptest13dbdm-6999f5757c-fk9dp to h009028 Normal SuccessfulMountVolume 2m kubelet, h009028 MountVolume.SetUp succeeded for volume "default-token-7q8pq" Warning FailedMount 45s kubelet, h009028 Unable to mount volumes for pod "deptest13dbdm-6999f5757c-fk9dp_default(28e9cdd6-c3c5-11e8-878f-141877468256)": timeout expired waiting for volumes to attach/mount for pod "default"/"deptest13dbdm-6999f5757c-fk9dp". list of unattached/unmounted volumes=[datadir] Normal SuccessfulMountVolume 35s (x2 over 36s) kubelet, h009028 MountVolume.SetUp succeeded for volume "pvc-d976c2bf-c2fb-11e8-878f-141877468256" Normal Pulling 27s kubelet, h009028 pulling image "registry.youkeshu.com:5000/mysql:56v6" Normal Pulled 27s kubelet, h009028 Successfully pulled image "registry.youkeshu.com:5000/mysql:56v6" Normal Pulling 27s kubelet, h009028 pulling image "registry.youkeshu.com:5000/mysqld-exporter:v0.10" Normal Pulled 26s kubelet, h009028 Successfully pulled image "registry.youkeshu.com:5000/mysqld-exporter:v0.10" Normal Created 26s kubelet, h009028 Created container Normal Started 26s kubelet, h009028 Started containerroot@h009027:~/mytest/ceph/rbd# kubectl get pods -o wide| grep deptest13deptest13dbdm-6999f5757c-fk9dp 2/2 Running 0 3m 172.26.0.87 h009028#在原node上查看内核中映射的rbd image块设备描述文件已消失,挂载已取消root@h020109:~/tpcc-mysql# ls /dev/rbd*ls: cannot access '/dev/rbd*': No such file or directoryroot@h020109:~/tpcc-mysql# mount | grep rbdroot@h020109:~/tpcc-mysql# 可以看出,是可以自动完成在节点间迁移的滚动更新的,整个调度的过程有些漫长,接近3分钟,这是因为其中至少涉及如下几个步骤:1.原pod终结,释放资源2.原node取消挂载rbd,释放内核映射3.新node内核映射rbd image,挂载rbd到文件目录4.创建pod,挂载指定目录 结论:ceph rbd不支持多节点挂载,支持滚动更新. 后续在完成了使用cephfs和ceph rbd这两种k8s分布式存储的对接方式测试之后,下一篇文章将针对这两种方式进行数据库性能测试及适用场景讨论]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十一)、分布式存储Cephfs使用]]></title>
    <url>%2F2018%2F09%2F16%2Fk8s(%E5%8D%81%E4%B8%80)%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8Cephfs%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言在前篇部署测试完cephfs的基础上:Ceph集群生产环境安装部署cephfs调优 &amp; 性能测试 &amp; 常用命令本篇介绍k8s使用 pv/pvc的方式使用cephfs分布式文件系统 使用首先解释一下pv/pvc的相关概念:PV:PersistentVolumes,是k8s抽象化的存储资源，主要包括存储能力、访问模式、存储类型、回收策略等关键信息.PV是k8s实际对接存储后端的真正入口[Access Mode] (接入模式):ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 [persistentVolumeReclaimPolicy]（回收策略:Retain（保留）- 保留数据，不会再分配给pvc,需要管理员手工清理数据Recycle（回收）- 清除 PV 中的数据，保留pv资源,可以留供其他pvc使用Delete（删除）- 删除整个pv资源及内部的数据。 PVC:PersistentVolumeClaims,是对PV资源的一种声明,pvc绑定实体资源pv后,pod通过绑定pvc来使用pv资源.PVC是k8s的一种抽象化的声明式绑定存储资源的管理模式的中间层,pod无法直接通过pv使用存储资源,必须经过pvc,而pvc必须要绑定pv实体后,才可被pod使用. 依赖安装:每个node需要安装ceph-common包,才能正常使用cephfs:1yum -y install ceph-common 创建secret:在创建pv前,由于ceph是开启了cephx认证的,于是首先需要创建secret资源,k8s的secret资源采用的是base64加密在ceph monitor上提取key:123456789101112# ceph auth get-key client.admin |base64QVFCL3E1ZGIvWFdxS1JBQTUyV0ZCUkxldnRjQzNidTFHZXlVYnc9PQ==# cat ceph-secret.yaml apiVersion: v1kind: Secretmetadata: name: ceph-secretdata: key: QVFCL3E1ZGIvWFdxS1JBQTUyV0ZCUkxldnRjQzNidTFHZXlVYnc9PQ== ~/ceph# kubectl apply -f ceph-secret.yaml 创建测试pv: 12345678910111213141516171819202122# cat cephfs-pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: cephfs-pv labels: pv: cephfs-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteMany cephfs: monitors: - 192.168.20.112:6789 - 192.168.20.113:6789 - 192.168.20.114:6789 user: admin secretRef: name: ceph-secret readOnly: false persistentVolumeReclaimPolicy: Delete 查看: 123# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEcephfs-pv 1Gi RWX Delete Available 6s 现在pvc是空闲Available状态 创建测试pvc:1234567891011121314# cat cephfs-pvc.yaml kind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-pvcspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi selector: matchLabels: pv: cephfs-pv 查看: 1234567# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEcephfs-pvc Bound cephfs-pv 1Gi RWX 24s# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEcephfs-pv 1Gi RWX Delete Bound default/cephfs-pvc 8m 可以看到pv和pvc都已经自动变为Bound状态,但是我们这里没有使用任何关联标签,使它们两者进行关联,pvc是怎么自动绑定到pv的呢?其实pvc在创建伊始就会主动去寻找符合requets条件的pv资源,如果寻找到了,便会自动进行绑定,无需做标签匹配. 小Tips:1.当pv的容量大于pvc的需求时,pvc可以成功自动绑定pv;2.当pv的容量小于pvc的需求时,pvc无法绑定该pv;3.pv和pvc的绑定关系是一一对应的.4.pv/pvc的创建顺序是:pv -&gt; pvc -&gt; pod5.pv/pvc的销毁顺序是:pod -&gt; pvc -&gt; pv,顺序一定不要错例如:2G的pv可以被1G的pvc绑定,并且绑定成功后,pvc的实际空间也是2G而不是1G;1G的pv无法被2G的pvc绑定 创建pod实例将pod调度到某一个node上,挂载pvc12345678910111213141516171819202122232425262728293031323334353637383940414243root@h009027:~/ceph# cat testpod1-deploy.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: testpod1 name: testpod1 namespace: defaultspec: replicas: 1 selector: matchLabels: app: testpod1 template: metadata: labels: app: testpod1 spec: containers: - args: - /bin/bash - -c - /run.sh image: nginx imagePullPolicy: Always name: testpod1 ports: - containerPort: 80 protocol: TCP terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /var/www name: cephfs-testpod1 dnsPolicy: ClusterFirst nodeSelector: DEVNODE: "" restartPolicy: Always securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - name: cephfs-testpod1 persistentVolumeClaim: claimName: cephfs-pvc 查看pod: 12~# kubectl get pods | grep testpod1testpod1-7847858587-2rbsq 1/1 Running 0 10s 进入pod并给nginx添加一个主页index.html: 12~# kubectl exec -it testpod1-7847858587-2rbsq bash~# echo "Just for pv/pvc test" &gt; /var/www/index.html 获取pod ip打开浏览器测试:(本篇测试场景外部与集群内部网段路由都已互通,如果pod内部路由未通,需要使用svc的方式暴露服务,可查看此前文章,这里不再赘述): 到这里,文件系统挂载已经测试完毕,但选用cephfs作k8s分布式文件系统的最大优势是因为其支持ReadWriteMany多节点挂载的灵活性,非常符合k8s像管理牲口一样管理你的容器的理念,下面开始跨节点测试 跨节点测试再创建一个部署文件,让其在另外的node上运行,与上方pod共用一个pvc: 1234567891011121314151617181920212223242526272829303132333435363738394041424344~/ceph# cat testpod2-deploy.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: testpod2 name: testpod2 namespace: defaultspec: replicas: 1 selector: matchLabels: app: testpod2 template: metadata: labels: app: testpod2 spec: containers: - args: - /bin/bash - -c - /run.sh image: registry.youkeshu.com:5000/nginx:php7.1v1 imagePullPolicy: Always name: testpod2 ports: - containerPort: 80 protocol: TCP terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /var/www name: cephfs-testpod2 dnsPolicy: ClusterFirst nodeSelector: PRODNODE: "" restartPolicy: Always securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - name: cephfs-testpod2 persistentVolumeClaim: claimName: cephfs-pvc 查看pod:123~/ceph# kubectl get pods -o wide | grep testpodtestpod1-7847858587-2rbsq 1/1 Running 0 17m 172.26.2.172 h009029testpod2-6d4b8d5db9-wkpx6 1/1 Running 0 9s 172.26.0.52 h009028 浏览器打开testpod2主页: 结论:cephfs可以很好的实现k8s多节点调度容器时对持久化文件的挂载支持 注意以上方式使用时,会将容器内的指定路径直接挂载到cephfs的根目录下,前往上篇文章中挂载mount cephfs的节点挂载目录就可以发现此问题:1234[root@h020112 cephfs]# lsindex.html testfile[root@h020112 cephfs]# cat index.html Just for pv/pvc test 如果所有的文件都这样直接放在整个cephfs文件系统的根目录里,无疑是极不好管理的,这个时候我们可以在 Pod 中使用一个新的属性： subPath，该属性可以来解决这个问题，我们只需要更改上面的 Pod 的 YAML 文件即可 修改后的yaml文件: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546root@h009027:~/ceph# cat testpod1-deploy.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: testpod1 name: testpod1 namespace: default selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/amazondevspec: replicas: 1 selector: matchLabels: app: testpod1 template: metadata: labels: app: testpod1 spec: containers: - args: - /bin/bash - -c - /run.sh image: registry.youkeshu.com:5000/nginx:php7.1v1 imagePullPolicy: Always name: testpod1 ports: - containerPort: 80 protocol: TCP terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /var/www name: cephfs-testpod1 # 仅添加这一行即可 subPath: testpod1 dnsPolicy: ClusterFirst nodeSelector: DEVNODE: "" restartPolicy: Always securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 volumes: - name: cephfs-testpod1 persistentVolumeClaim: claimName: cephfs-pvc 重新部署后,再回到挂载点查看: 123456[root@h020112 cephfs]# lsindex.html testfile testpod1[root@h020112 cephfs]# ls testpod1/[root@h020112 cephfs]# [root@h020112 cephfs]# ll testpod1/total 0 可以看到,这里创建了一个subPath名的目录,但是目录里面是空的,也即是说,通过k8s的抽象层pvc,容器对cephfs的根目录拥有了指定容量的使用权限,subPath仅仅是在cephfs根目录下创建了一个子目录,无论是否创建子目录,容器对cephfs的根目录都拥有使用权限,因此,为了避免多个容器同名文件冲突,便于管理维护,请在实际使用中一定不要忘记在部署文件中添加subPath项]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cephfs调优 & 性能测试 & 监控 & 常用命令]]></title>
    <url>%2F2018%2F09%2F15%2Fcephfs%E8%B0%83%E4%BC%98-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95-%E7%9B%91%E6%8E%A7-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
    <content type="text"><![CDATA[前言k8s对ceph rbd模式不支持ReadWriteMany（RWX）,为了满足k8s的灵活性需求,采用支持多点挂载的cephfs工作模式.网上少有针对cephfs调优的文章,因此在本篇,将针对cephfs进行一些参数调优及性能测试 硬件规划 配置调优在进行配置调优之前首先理解一下ceph几种基础配置项名词的定义名词解析:12345object: ceph数据存储的基本单位,可以是普通data也可以是journal dataosd: ceph数据存储的目标载体,一般一个osd实例对应一块磁盘pg(Placement Group): 存储位组,object对象的存储时的分组,与osd关联pgp( Placement Group for Placement purpose):值应与pg保持一致,当此值修改时,pg内的数据对象才会进行rebalancepool: 数据的逻辑隔离与管理单位,由多个pg构成 关系图: 经过多次的对参数反复调整,将ceph.conf修改成如下配置,比较适合我们的使用场景:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# cat /etc/ceph/ceph.conf [global]fsid = 05bd836f-f307-47fa-ae95-34e302fbd9e8mon_initial_members = h020112, h020113, h020114mon_host = 192.168.20.112,192.168.20.113,192.168.20.114auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxpublic_network= 192.168.20.0/24osd pool default size = 2osd pool default pg num = 1024osd pool default pgp num = 1024osd data = /var/lib/ceph/osd/ceph-$idosd journal size = 81920osd mkfs type = xfsosd mkfs options xfs = -fosd max write size = 512 #默认值90 #OSD一次可写入的最大值(MB)osd client message size cap = 2147483648 #默认值100 #客户端允许在内存中的最大数据(bytes)osd deep scrub stride = 131072 #默认值524288 #在Deep Scrub时候允许读取的字节数(bytes)osd op threads = 16 #默认值2 #并发文件系统操作数osd disk threads = 4 #默认值1 #OSD密集型操作例如恢复和Scrubbing时的线程osd map cache size = 1024 #默认值500 #保留OSD Map的缓存(MB)osd map cache bl size = 128 #默认值50 #OSD进程在内存中的OSD Map缓存(MB)osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier" #默认值rw,noatime,inode64 #Ceph OSD xfs Mount选项osd recovery op priority = 2 #默认值10 #恢复操作优先级，取值1-63，值越高占用资源越高osd recovery max active = 10 #默认值15 #同一时间内活跃的恢复请求数osd max backfills = 4 #默认值10 #一个OSD允许的最大backfills数osd min pg log entries = 30000 #默认值3000 #修建PGLog是保留的最大PGLog数osd max pg log entries = 100000 #默认值10000 #修建PGLog是保留的最大PGLog数osd mon heartbeat interval = 40 #默认值30 #OSD ping一个monitor的时间间隔（默认30s）osd op log threshold = 50 #默认值5 #一次显示多少操作的logfilestore xattr use omap = truefilestore min sync interval = 10 #默认0.1#从日志到数据盘最小同步间隔(seconds)filestore max sync interval = 15 #默认5#从日志到数据盘最大同步间隔(seconds)filestore queue max ops = 25000 #默认500#数据盘最大接受的操作数filestore queue max bytes = 1048576000 #默认100 #数据盘一次操作最大字节数(bytesfilestore queue committing max ops = 50000 #默认500 #数据盘能够commit的操作数filestore queue committing max bytes = 10485760000 #默认100 #数据盘能够commit的最大字节数(bytes)filestore split multiple = 1600 #默认值2 #前一个子目录分裂成子目录中的文件的最大数量filestore merge threshold = 40 #默认值10 #前一个子类目录中的文件合并到父类的最小数量filestore fd cache size = 204800 #默认值128 #对象文件句柄缓存大小filestore omap header cache size = 204800filestore fiemap = true #默认值false #读写分离filestore ondisk finisher threads = 2 #默认值为1filestore apply finisher threads = 2 #默认值为1mds cache size = 102400000mds beacon grace = 120mds session timeout = 15mds session autoclose = 60mds reconnect timeout = 15mds decay halflife = 10rbd op threads = 4rbd cache size = 268435456rbd cache max dirty = 134217728rbd cache max dirty age = 5rbd cache max dirty object = 64journal max write bytes = 1073714824journal max write entries = 10000journal queue max ops = 50000journal queue max bytes = 10485760000ms_async_op_threads = 5 #默认值3ms dispatch throttle bytes = 1048576000 #默认值 104857600 #等待派遣的最大消息数objecter inflight ops = 819200 #默认值1024 #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限client_caps_release_delay = 10 创建mds12345678910111213141516171819ceph-deploy mds create h020112 h020113 h020114ceph osd pool create yks_cephfs_dataceph osd pool create yks_cephfs_metadata# cat /etc/ceph/ceph.client.admin.keyring[client.admin] key = AQB/q5db/XWqKRAA52WFBxRLevtcC3bus1GeyUbw==### 准备在客户端挂载ceph #### 首先客户端必须先安装ceph-common$ yum -y install ceph-common# 将key写入文件/etc/ceph/admin.secret中$ echo "AQB/q5db/XWqKRAA52WFBxRLevtcC3bus1GeyUbw==" &gt; /etc/ceph/admin.secret# 挂载ceph$ mkdir /mnt/cephfs$ mount -t ceph 192.168.20.114:6789:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret$ echo "192.168.20.114:6789:/ /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/admin.secret,noatime 0 2" &gt;&gt; /etc/fstab# 挂载成功后,即可使用cephfs文件系统 性能测试使用fio命令测试小块(16k)\大块(512k)文件读写\随机读写性能,测试结果如下: 稳定性测试使用多线程压力测试脚本运行一天两夜往cephfs内写入了几千万个小文件,压力测试脚本如下: 123456789101112131415161718#!/bin/bashfor dir1 in &#123;1..100&#125;;do mkdir /mnt/cephfs/$&#123;dir1&#125; cd /mnt/cephfs/$&#123;dir1&#125; for i in &#123;1..20000&#125;;do dd if=/dev/zero of=file$&#123;i&#125; bs=1024k count=1 done dir2s="a b c d e f g j" for dir2 in $&#123;dir2s&#125;;do &#123; mkdir /mnt/cephfs/$&#123;dir1&#125;/$&#123;dir2&#125; cd /mnt/cephfs/$&#123;dir1&#125;/$&#123;dir2&#125; for i in &#123;1..100000&#125;;do dd if=/dev/zero of=file$&#123;i&#125; bs=128k count=1 done &#125; &amp; done done 压测的过程出现过几次客户端响应阻塞的意外,但基本都自动恢复,稳定性尚可.在写入了5T 4k的小文件后,查看文件大小\目录大小\文件详情等常规操作延迟可以接受,测试基本通过 Ceph监控ceph性能指标及容量监控,prometheus采集+grafana展示,这里使用digitalocean提供的ceph_exporter prometheus插件.项目主页:安装ceph_exporter 123456789101112131415161718192021yum install golangyum install librados2-devel librbd1-devel# 配置go PATH环境变量cat &gt; /etc/profile.d/go.sh &lt;&lt; EOFexport GOROOT=/usr/lib/golangexport GOBIN=$GOROOT/binexport GOPATH=/home/golangexport PATH=$PATH:$GOROOT/bin:$GOPATH/binEOFsource /etc/profile.d/go.sh# 安装.过程没有输出,请耐心等待go get -u github.com/digitalocean/ceph_exporter# 进入目录可以查看到已安装的ceph_exportercd /usr/lib/golang/bin/ls# 启动程序nohup ./ceph_exporter &amp; prometheus配置job 12345- job_name: 'ceph-exporter' static_configs: - targets: ['192.168.20.114:9128'] labels: alias: ceph-exporter 重启prometheus,web ui查看target: prometheus/grafana安装配置步骤见此前的文章 grafana添加图形模板导入179号模板: 监控告警配置可以自定义阈值,参考此前文章触发告警接口 Ceph常用命令12345678910111213141516171819202122232425262728293031323334353637383940414243# 服务相关:- systemctl status ceph\*.service ceph\*.target #查看所有服务- systemctl stop ceph\*.service ceph\*.target #关闭所有服务- systemctl start ceph.target #启动服务- systemctl stop ceph-osd\*.service # 关闭所有osd服务- systemctl stop ceph-mon\*.service #关闭所有mon服务- sudo systemctl start ceph-osd@&#123;id&#125;- sudo systemctl start ceph-mon@&#123;hostname&#125;- sudo systemctl start ceph-mds@&#123;hostname&#125;# 查看- ceph -help #查看命令帮助- ceph -s #查看状态- ceph osd pool set rbd pg_num 1024 # 修改pg_num数量- ceph osd pool set rbd pg_num 1024 # 修改pgp_num数量- ceph osd tree #查看osd树- ceph osd pool ls #查看所有的osd池- ceph --admin-daemon /var/run/ceph/ceph-osd.11.asok config show # 查看指定的osd运行中的所有参数- rados df #查看储存池使用情况- rados -p rbd ls |sort - ceph osd pool get rbd pg_num - ceph osd pool get rbd pgp_num - ceph osd pool set rbd pg_num 1024- ceph osd pool set rbd pgp_num 1024# rbd相关- rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125;- rbd list- rbd info RBD_NAME- rbd feature disable RBD_NAME FEATURE1 FEATURE1 ..- rbd map RBD_NAME #映射到系统内核- rbd showmapped #查看rbd映射条目- rbd unmap /dev/rbd0 # 取消内核映射- rbd resize --size 2048 RBD_NAME # to increase- rbd resize --size 2048 foo --allow-shrink #to decrease- rbd du &#123;RBD_NAME&#125; -p rbd #查看某个或所有Image的容量,-p 指定pool名- rbd diff RBD_NAME | awk '&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024/1024 " GB" &#125;' #查看rbd image当前占用大小# 修改- ceph tell # 使用tell命令手动临时修改组件的配置[例如:集群状态恢复涉及数据回填时,加速回填速度]- ceph tell 'osd.*' injectargs '--osd-max-backfills 16' #默认为1- ceph tell 'osd.*' injectargs '--osd-recovery-max-active 8' #默认为4 最后Ceph是一套庞大的存储系统,工作原理及涉及组件相当复杂,存储更是系统架构中的重中之重,一定要慎之又慎,同时做好知识储备,多查官方文档,另外再提一个坑,那就是官方的中文文档除了丰富度不如英文文档外,很多参数的默认值与英文文档不一致,英文文档更加准确,因此推荐查看英文文档.链接:ceph官方文档]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph集群生产环境安装部署]]></title>
    <url>%2F2018%2F09%2F11%2FCeph%E9%9B%86%E7%BE%A4%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.html</url>
    <content type="text"><![CDATA[前言ceph的组件以及工作流程非常的复杂,是一个庞大的系统,在尝试ceph之前尽量多查阅官方的文档,理解ceph的mon/osd/mds/pg/pool等各组件/Unit的协同工作方式Ceph官方文档 一、配置规划： 二、部署1. ntp-server开启ntp服务： 123apt-get install ntp ntpdate ntp-docsystemctl enable ntpsystemctl start ntp 2. ceph node 三台node全部执行如下操作：磁盘分区规划如顶部表格，按照规划写的磁盘划分脚本，分别在3台node上执行脚本： 1234567891011121314151617181920212223242526272829303132# cat ~/parted.sh #!/bin/bashset -eif [ ! -x "/usr/sbin/parted" ]; thenecho "This script requires /sbin/parted to run!" &gt;&amp;2exit 1fiDISKS="d e f g h i j k l m n o p"for i in $&#123;DISKS&#125;; do echo "Creating partitions on /dev/sd$&#123;i&#125; ..." parted -a optimal --script /dev/sd$&#123;i&#125; -- mktable gpt parted -a optimal --script /dev/sd$&#123;i&#125; -- mkpart primary xfs 0% 100% sleep 1 #echo "Formatting /dev/sd$&#123;i&#125;1 ..." mkfs.xfs -f /dev/sd$&#123;i&#125;1 &amp;doneSSDS="b c"for i in $&#123;SSDS&#125;; do parted -s /dev/sd$&#123;i&#125; mklabel gpt parted -s /dev/sd$&#123;i&#125; mkpart primary 0% 10% parted -s /dev/sd$&#123;i&#125; mkpart primary 11% 20% parted -s /dev/sd$&#123;i&#125; mkpart primary 21% 30% parted -s /dev/sd$&#123;i&#125; mkpart primary 31% 40% parted -s /dev/sd$&#123;i&#125; mkpart primary 41% 50% parted -s /dev/sd$&#123;i&#125; mkpart primary 51% 60% parted -s /dev/sd$&#123;i&#125; mkpart primary 61% 70%donechown -R ceph:ceph /dev/sdb[1-7]chown -R ceph:ceph /dev/sdc[1-7] 添加 /etc/hosts解析，并scp到3台node上:123192.168.20.112 h020112192.168.20.113 h020113192.168.20.114 h020114 关闭防火墙、selinux，添加定时同步时间计划任务： 12345678sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsetenforce 0systemctl stop firewalldsystemctl disable firewalldcat &gt;&gt;/etc/crontab&lt;&lt;EOF× */1 * * * root ntpdate 192.168.9.28 &amp;&amp; --systohcEOF 12345cd /etc/yum.repos.dmv CentOS-Base.repo CentOS-Base.repo.bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum -y install ntp ntpdateyum -y install ceph 在node1(192.168.20.112)上执行: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354wget https://download.ceph.com/rpm-kraken/el7/noarch/ceph-deploy-1.5.37-0.noarch.rpmyum -y install ceph-deploy-1.5.37-0.noarch.rpm# 创建秘钥，传送到各节点，实现无秘钥登录ssh-keygenssh-copy-id h020112ssh-copy-id h020113ssh-copy-id h020114# 新建集群，生成配置文件mkdir ceph-cluster &amp;&amp; cd ceph-clusterceph-deploy new h020112 h020113 h020114## 修改默认生成的ceph.conf，增加如下配置段：# 80G志盘osd_journal_size = 81920 public_network= 192.168.20.0/24 # 副本pg数为2，默认为3，最小工作size为默认size - (默认size/2) osd pool default size = 2# 官方建议平均每个osd 的pg数量不小于30，即pg num &gt; (osd_num) * 30 / 2(副本数)osd pool default pg num = 1024osd pool default pgp num = 1024# 传送ceph.confceph-deploy --overwrite-conf config push h020112 h020113 h020114# 若有RuntimeError: bootstrap-mds keyring not found; run 'gatherkeys'报错，则执行如下命令传送keyceph-deploy gatherkeys yh020112 h020113 h020114# 初始化mon节点ceph-deploy mon create-initialceph -s # 查看mon是否添加成功# ceph 集群加入OSDcd /etc/ceph # 一定要进入这个目录下# 执行如下脚本，志盘和数据盘的对应关系要再三确认[root@h20112 ceph]# cat add_osd.sh #!/bin/bashfor ip in $(cat ~/ceph-cluster/cephosd.txt) doecho ----$ip-----------;ceph-deploy --overwrite-conf osd prepare $ip:sdd1:/dev/sdb1 $ip:sde1:/dev/sdb2 $ip:sdf1:/dev/sdb3 $ip:sdg1:/dev/sdb4 $ip:sdh1:/dev/sdb5 $ip:sdi1:/dev/sdb6 \ $ip:sdj1:/dev/sdc1 $ip:sdk1:/dev/sdc2 $ip:sdl1:/dev/sdc3 $ip:sdm1:/dev/sdc4 $ip:sdn1:/dev/sdc5 $ip:sdo1:/dev/sdc6 $ip:sdp1:/dev/sdc7donefor ip in $(cat ~/ceph-cluster/cephosd.txt)doecho ----$ip-----------;ceph-deploy osd activate $ip:sdd1:/dev/sdb1 $ip:sde1:/dev/sdb2 $ip:sdf1:/dev/sdb3 $ip:sdg1:/dev/sdb4 $ip:sdh1:/dev/sdb5 $ip:sdi1:/dev/sdb6 \ $ip:sdj1:/dev/sdc1 $ip:sdk1:/dev/sdc2 $ip:sdl1:/dev/sdc3 $ip:sdm1:/dev/sdc4 $ip:sdn1:/dev/sdc5 $ip:sdo1:/dev/sdc6 $ip:sdp1:/dev/sdc7done 查看结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@h20112 ceph]# ceph osd treeID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 34.04288 root default -2 11.34763 host h020112 0 0.87289 osd.0 up 1.00000 1.00000 3 0.87289 osd.3 up 1.00000 1.00000 4 0.87289 osd.4 up 1.00000 1.00000 5 0.87289 osd.5 up 1.00000 1.00000 6 0.87289 osd.6 up 1.00000 1.00000 7 0.87289 osd.7 up 1.00000 1.00000 8 0.87289 osd.8 up 1.00000 1.00000 9 0.87289 osd.9 up 1.00000 1.00000 10 0.87289 osd.10 up 1.00000 1.00000 11 0.87289 osd.11 up 1.00000 1.00000 12 0.87289 osd.12 up 1.00000 1.00000 13 0.87289 osd.13 up 1.00000 1.00000 14 0.87289 osd.14 up 1.00000 1.00000 -3 11.34763 host h020113 1 0.87289 osd.1 up 1.00000 1.00000 15 0.87289 osd.15 up 1.00000 1.00000 16 0.87289 osd.16 up 1.00000 1.00000 17 0.87289 osd.17 up 1.00000 1.00000 18 0.87289 osd.18 up 1.00000 1.00000 19 0.87289 osd.19 up 1.00000 1.00000 20 0.87289 osd.20 up 1.00000 1.00000 21 0.87289 osd.21 up 1.00000 1.00000 22 0.87289 osd.22 up 1.00000 1.00000 23 0.87289 osd.23 up 1.00000 1.00000 24 0.87289 osd.24 up 1.00000 1.00000 25 0.87289 osd.25 up 1.00000 1.00000 26 0.87289 osd.26 up 1.00000 1.00000 -4 11.34763 host h020114 2 0.87289 osd.2 up 1.00000 1.00000 27 0.87289 osd.27 up 1.00000 1.00000 28 0.87289 osd.28 up 1.00000 1.00000 29 0.87289 osd.29 up 1.00000 1.00000 30 0.87289 osd.30 up 1.00000 1.00000 31 0.87289 osd.31 up 1.00000 1.00000 32 0.87289 osd.32 up 1.00000 1.00000 33 0.87289 osd.33 up 1.00000 1.00000 34 0.87289 osd.34 up 1.00000 1.00000 35 0.87289 osd.35 up 1.00000 1.00000 36 0.87289 osd.36 up 1.00000 1.00000 37 0.87289 osd.37 up 1.00000 1.00000 38 0.87289 osd.38 up 1.00000 1.00000 [root@h20112 ceph]# ceph -s cluster 6661d89d-5895-4bcb-9b11-9400638afc85 health HEALTH_OK monmap e1: 3 mons at &#123;h020112=192.168.20.112:6789/0,h020113=192.168.20.113:6789/0,h020114=192.168.20.114:6789/0&#125; election epoch 6, quorum 0,1,2 h020112,h020113,h020114 osdmap e199: 39 osds: 39 up, 39 in flags sortbitwise,require_jewel_osds pgmap v497: 1024 pgs, 1 pools, 0 bytes data, 0 objects 4385 MB used, 34854 GB / 34858 GB avail 1024 active+clean# 若配置文件里指定的pg_num 和 php_num未生效，使用命令指定：sudo ceph osd pool set rbd pg_num 1024sudo ceph osd pool set rbd pgp_num 1024 自定义crush分布式调度规则：ceph一共有如下层级的管理单位，从上到下层级依次提升，可以灵活地按照物理的逻辑粒度，将osd关联到不同的主机、机位、机架、pdu、机房、区域等管理单位，每一层级的整体权重值等于该层级下所有OSD的权重之和。 1234567891011type 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 regiontype 10 root 自定义crush视图:这里只写操作方法，不作实施，本次操作的环境硬盘数目、硬盘规格、主机位置均一致，暂时不作crush调整 12345678910111213141516ceph osd crush add-bucket hnc root #添加root层级名为hnc的bucketceph osd crush add-bucket rack0 rack #添加rack层级名为rack0的bucketceph osd crush add-bucket rack1 rack #添加rack层级名为rack1的bucketceph osd crush add-bucket rack2 rack #添加rack层级名为rack2的bucketceph osd crush move rack0 root=hnc #将rack0移入hnc之下ceph osd crush move rack0 root=hnc #将rack1移入hnc之下ceph osd crush move rack0 root=hnc #将rack2移入hnc之下ceph ods crush move h020112 rack=rack0 #将host h020112移入rack0层级下ceph ods crush move h020113 rack=rack1 #将host h020113移入rack1层级下ceph ods crush move h020114 rack=rack2 #将host h020114移入rack2层级下ceph osd getcrushmap -o map_old 导出mapcrushtool -d map_old -o map_old.txt 转化成可编辑格式crushtool -c map_new.txt -o map_new 还原为mapceph osd setcrushmap -i map_new 将map导入ceph 修改配置文件,防止ceph自动更新crushmap12echo ‘osd_crush_update_on_start = false‘ &gt;&gt; /etc/ceph/ceph.conf/etc/init.d/ceph restart 使用如下样例crash（map_new.txt）配置: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117--------------------------------------# begin crush maptunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable straw_calc_version 1# devicesdevice 0 osd.0device 1 osd.1device 2 osd.2device 3 osd.3device 4 osd.4device 5 osd.5device 6 osd.6device 7 osd.7device 8 osd.8device 9 osd.9device 10 osd.10device 11 osd.11# typestype 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 regiontype 10 root# bucketsroot default &#123; id -1 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1&#125;# 权重值一般根据磁盘容量(基数)与性能(倍率)调整，例如设置1T为1.00，2T为2.00，hdd倍数为1，ssd倍数为2host h020112 &#123; id -2 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.0 weight 2.000 item osd.1 weight 2.000 item osd.2 weight 2.000 item osd.3 weight 2.000 # 需写出全部osd，这里省略不写这么多了&#125;host h020113 &#123; id -3 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.4 weight 2.000 item osd.5 weight 2.000 item osd.6 weight 2.000 item osd.7 weight 2.000&#125;host h020114 &#123; id -4 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item osd.8 weight 2.000 item osd.9 weight 2.000 item osd.10 weight 2.000 item osd.11 weight 2.000 &#125;rack rack0 &#123; id -6 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item h020112 weight 8.000&#125;rack rack1 &#123; id -7 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item h020113 weight 8.000&#125;rack rack2 &#123; id -8 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item h020114 weight 8.000&#125;root hnc &#123; id -5 # do not change unnecessarily # weight 0.000 alg straw hash 0 # rjenkins1 item rack0 weight 8.000 item rack1 weight 8.000 item rack2 weight 8.000&#125;# rulesrule replicated_ruleset &#123; #规则集的命名，创建pool时可以指定rule集 ruleset 0 #rules集的编号，顺序编即可 type replicated #定义pool类型为replicated(还有esurecode模式) min_size 1 #pool中最小指定的副本数量不能小1 max_size 10 #pool中最大指定的副本数量不能大于10 step take hnc #定义pg查找副本的入口点 step chooseleaf firstn 0 type rack #选叶子节点、深度优先、隔离rack step emit&#125;# end crush map 将修改后的crushmap编译并且注入集群中 123456crushtool -c map_new.txt -o map_newceph osd setcrushmap -i map_newceph osd treeceph osd crush rm default # 删除默认crush mapsystemctl stop ceph\*.service ceph\*.target #关闭所有服务systemctl start ceph.target #启动服务]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置根据参数转发]]></title>
    <url>%2F2018%2F08%2F23%2Fnginx%E9%85%8D%E7%BD%AE%E6%A0%B9%E6%8D%AE%E5%8F%82%E6%95%B0%E8%BD%AC%E5%8F%91.html</url>
    <content type="text"><![CDATA[需求：因浏览器安全策略，在reference为https类型时，无法跳转获取http协议链接的数据。因此，设计解决方案为：由程序将需要跳转的完整url作为参数pic_url的值，加入整个url内，所以需要在代理中实现中转，将请求转发给pic_url url：原：https://n1-test.xxx.com/angentImg/?type=image&amp;pic_url=http://www.wipo.int/branddb/jsp/data.jsp?SOURCE=EMTM&amp;TYPE=jpg&amp;KEY=014835301&amp;qi=2-FbonCURiPwm/abnckAV 希望转发给：http://www.wipo.int/branddb/jsp/data.jsp?SOURCE=EMTM&amp;TYPE=jpg&amp;KEY=014835301&amp;qi=2-FbonCURiPwm/abnckAV 实现方法： 1234567location ~/angentImg/ &#123; if ($query_string ~* ^(.*)type=image&amp;pic_url=(.*)$)&#123; set $pic_url $2; # set $pic_url $arg_pic_url proxy_pass $pic_url; &#125;&#125; 12345解释： 1.set $1 $2 将$2变量的值赋值给$1变量2.arg_(VAR_NAME)用来获取url后指定参数对应的值,比如arg_pic_url可以获取到连接中pic_url这个参数的值3.$query_string 可以获取路径之后包含所有参数kv的string4.但是如果要获取的参数本身也是一个url，且该url包含有自带的参数，例如在这个例子里，直接使用$pic_url只能捕获http://www.wipo.int/branddb/jsp/data.jsp?SOURCE=EMTM这一段值，它后方的&amp;之后的参数是无法捕捉到。因此使用正则匹配，将pic_url=后方的字符串全部赋予给了变量$2，将流量转发向$2值的url 配置完上面这些后，发现链接还是无法打开，报错502： 怀疑目标url做了防盗链处理，但经过测试，这是个cdn地址，并没有配置防盗链，继续检查，发现nginx志有异常记录： 域名无法解析？dns已经配置了呀，本地也是可以浏览器正常访问该图片url，尝试在nginx服务器上添加/etc/hosts 固定dns记录，发现依然有同样的报错，求助万能的google，找到了问题的原因： 在Ngnix中如果用变量作为反向代理的地址时，可能会出现“no resolver defined to resolve xxx.xxx”的问题，原因是 Nginx 0.6.18以后的版本中启用了一个resolver指令，在使用变量来构造某个server地址的时候一定要用resolver指令来指定DNS服务器的地址，所以解决这个问题的方法很简单：在nginx的配置文件中的http{}部分添加一行DNS解析即可，注意，要写在nginx配置的http{}内： 1resolver 8.8.8.8 ipv6=off; 再次尝试，图片可以正常访问，检查浏览器控制台可以看到各项url参数均正常携带：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s部署zookeeper/kakfa集群]]></title>
    <url>%2F2018%2F08%2F16%2Fk8s%E9%83%A8%E7%BD%B2zookeeper_kakfa%E9%9B%86%E7%BE%A4.html</url>
    <content type="text"><![CDATA[一、部署ZK集群问题：在制作好zk的docker镜像后，测试docker直接运行起3个实例，zk集群选举建立都是正常的，但是，通过k8s部署后，发现zk集群选举无论如何都不能成功，各种google都无法解决，弃用自制镜像，改用docker官方的镜像，问题依旧。最终通过headless service这一方式完美解决，在此记录一下解决过程。 首先，贴一下manifest.yaml文件，整合成了一个完整的yaml文件，基于官方的zk docker镜像： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykszktest-n1spec: replicas: 1 selector: matchLabels: app: ykszktest-n1 template: metadata: labels: app: ykszktest-n1 spec: hostname: ykszktest-n1 volumes: - name: ykszktest-data hostPath: path: /data/ykszktest-cluster/ykszktest-data-n1 - name: ykszktest-logs hostPath: path: /data/ykszktest-cluster/ykszktest-logs-n1 dnsPolicy: ClusterFirst containers: - name: ykszktest-n1 image: zookeeper:3.4.10 imagePullPolicy: Always volumeMounts: - name: ykszktest-data readOnly: false mountPath: "/data/ykszktest-data" - name: ykszktest-logs readOnly: false mountPath: "/data/ykszktest-logs" ports: - containerPort: 2181 - containerPort: 2888 - containerPort: 3888 #command: ['tail', '-f', '/etc/hosts'] env: - name: ZOO_MY_ID value: "1" - name: ZOO_SERVERS value: server.1=ykszktest-n1:2888:3888 server.2=ykszktest-n2:2888:3888 server.3=ykszktest-n3:2888:3888 - name: ZOO_DATA_DIR value: '/data/ykszktest-data' - name: ZOO_DATA_LOG_DIR value: '/data/ykszktest-logs' ---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykszktest-n2spec: replicas: 1 selector: matchLabels: app: ykszktest-n2 template: metadata: labels: app: ykszktest-n2 spec: hostname: ykszktest-n2 volumes: - name: ykszktest-data hostPath: path: /data/ykszktest-cluster/ykszktest-data-n2 - name: ykszktest-logs hostPath: path: /data/ykszktest-cluster/ykszktest-logs-n2 dnsPolicy: ClusterFirst containers: - name: ykszktest-n2 image: zookeeper:3.4.10 imagePullPolicy: Always volumeMounts: - name: ykszktest-data readOnly: false mountPath: "/data/ykszktest-data" - name: ykszktest-logs readOnly: false mountPath: "/data/ykszktest-logs" ports: - containerPort: 2181 - containerPort: 2888 - containerPort: 3888 #command: ['tail', '-f', '/etc/hosts'] env: - name: ZOO_MY_ID value: "2" - name: ZOO_SERVERS value: server.1=ykszktest-n1:2888:3888 server.2=ykszktest-n2:2888:3888 server.3=ykszktest-n3:2888:3888 - name: ZOO_DATA_DIR value: '/data/ykszktest-data' - name: ZOO_DATA_LOG_DIR value: '/data/ykszktest-logs' ---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykszktest-n3spec: replicas: 1 selector: matchLabels: app: ykszktest-n3 template: metadata: labels: app: ykszktest-n3 spec: hostname: ykszktest-n3 volumes: - name: ykszktest-data hostPath: path: /data/ykszktest-cluster/ykszktest-data-n3 - name: ykszktest-logs hostPath: path: /data/ykszktest-cluster/ykszktest-logs-n3 dnsPolicy: ClusterFirst containers: - name: ykszktest-n3 image: zookeeper:3.4.10 imagePullPolicy: Always volumeMounts: - name: ykszktest-data readOnly: false mountPath: "/data/ykszktest-data" - name: ykszktest-logs readOnly: false mountPath: "/data/ykszktest-logs" ports: - containerPort: 2181 - containerPort: 2888 - containerPort: 3888 #command: ['tail', '-f', '/etc/hosts'] env: - name: ZOO_MY_ID value: "3" - name: ZOO_SERVERS value: server.1=ykszktest-n1:2888:3888 server.2=ykszktest-n2:2888:3888 server.3=ykszktest-n3:2888:3888 - name: ZOO_DATA_DIR value: '/data/ykszktest-data' - name: ZOO_DATA_LOG_DIR value: '/data/ykszktest-logs' ---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n1 name: ykszktest-n1 namespace: defaultspec: ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n1 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n2 name: ykszktest-n2 namespace: defaultspec: ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n2 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n3 name: ykszktest-n3 namespace: defaultspec: ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n3 sessionAffinity: None type: ClusterIP 报错1： 1Socket connection established to localhost/127.0.0.1:2181, initiating session ClientCnxn$SendThread@1158] - Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket conn 原因：zk peer不通，查看发现是svc未关联上endpoint，导致流量无法到达zk peer。 报错2：1234562018-08-16 11:46:38,158 [myid:2] - WARN [SendWorker:1:QuorumCnxManager$SendWorker@732] - Exception when using channel: for id 1 my id = 2 error = java.net.SocketException: Broken pipe2018-08-16 11:46:38,159 [myid:2] - WARN [RecvWorker:1:QuorumCnxManager$RecvWorker@813] - Interrupting SendWorker2018-08-16 11:46:38,166 [myid:2] - WARN [SendWorker:1:QuorumCnxManager$SendWorker@736] - Send worker leaving thread2018-08-16 11:47:38,157 [myid:2] - INFO [QuorumPeer[myid=2]/0.0.0.0:2181:FastLeaderElection@852] - Notification time out: 600002018-08-16 11:47:38,260 [myid:2] - WARN [RecvWorker:1:QuorumCnxManager$RecvWorker@810] - Connection broken for id 1, my id = 2, error = java.io.EOFException google出来通常的排查思路：1.检查myid内的序号与本地的server.{ID}是否匹配2.检查data目录是否有权限，是否正常生成pid文件3.将本地对应的service.{ID}的IP配置为0.0.0.04./etc/hosts修改本地dns 条目localhost指向0.0.0.0 以上方法全部无效。 测试发现，如果直接以docker的形式运行，并配置好相应的环境变量后，依次启动zk服务，选举很快就完成，集群建立成功，为什么用k8s部署就会出现net.SocketException的报错呢，若为网络问题，可是在pod内通过svc ClusterIP检查zk peer的工作端口、选举端口、leader端口都是通的，难道是因为zk的选举交互过程不能使用经过kube-proxy转换过的svc IP而必须使用pod的local IP？ 抱着以上猜测，手动将zoo.cfg内的配置文件的server实例的名称从svc-name改成了当前的POD IP，其余保持不变： 12345678910111213141516171819###修改前tickTime=2000initLimit=10syncLimit=5dataDir=/opt/zookeeper-3.4.9/dataclientPort=2181server.1=ykszktest-n1:2888:3888server.2=ykszktest-n1:2888:3888server.3=ykszktest-n1:2888:3888###修改后tickTime=2000initLimit=10syncLimit=5dataDir=/opt/zookeeper-3.4.9/dataclientPort=2181server.1=10.100.2.65:2888:3888server.2=10.100.0.71:2888:3888server.3=10.100.0.70:2888:3888 再次重启zk服务，果然，选举完成，zk集群建立成功。 123456~/mytest/zk# kubectl exec -it ykszktest-n2-566c5cd4db-zhvvr bashbash-4.3# bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /conf/zoo.cfgMode: follower 思考：svc的意义本身是为了防止Pod IP变化带来的不稳定性，因此通过svc在控制层面以标签的形式，在数据层面以ipvs (新版) | iptables (旧版)的形式，将流量转发给endpoint pod实例，以实现服务依赖对于pod IP的解耦，但是如果zk的集群建立只能以pod IP的方式进行，那将带来诸多不确定性与不稳定的因素。有没有什么办法可以既使用上层的svc，又不经过转换流量直接转发的pod呢？ 答案是有。google了一圈，发现了service的一种独特的工作方式：headless service配置这种工作方式很简单，将svc的ClusterIP指定为None即可： 1Service.spec.clusterIP: None 经过这样配置后，kube-proxy只是单纯地转发流量，不再进行转换，同时集群dns的A记录不再指向svc IP，而直接指向后端的pod IP，但endpoints controller仍然会在API中创建Endpoints的记录。当然，不再代理流量的同时，svc的负载均衡的功能也同时失去了。在本处场景下，zk本身就是以集群的方式工作，有自己内部的负载均衡算法，因此也无需svc这一层来做LB. 修改完成之后的3个svc实例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n1 name: ykszktest-n1 namespace: defaultspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n1 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n2 name: ykszktest-n2 namespace: defaultspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n2 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykszktest-n3 name: ykszktest-n3 namespace: defaultspec: clusterIP: None ports: - port: 2181 protocol: TCP targetPort: 2181 name: client - port: 2888 protocol: TCP targetPort: 2888 name: leader - port: 3888 protocol: TCP targetPort: 3888 name: leader-election selector: app: ykszktest-n3 sessionAffinity: None type: ClusterIP 最后，重新部署yaml文件，进入pod内检查结果，部署成功： 1234bash-4.3# bin/zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /conf/zoo.cfgMode: leader 二、部署kafka集群使用star数量最高的镜像wurstmeister/kafka:1.0.1： 123root@yksv001238:~/test/kafka# docker search kafkaNAME DESCRIPTION STARS OFFICIAL AUTOMATEDwurstmeister/kafka Multi-Broker Apache Kafka Image 654 [OK] 需要提供的环境变量如下，根据节点修改相应变量:12345678910111213141516171819202122env: # broker ID,必须要有,各节点不能一致- name: KAFKA_BROKER_ID value: "1" # 必须要有,zk集群- name: KAFKA_ZOOKEEPER_CONNECT value: ykszktest-n1:2181,ykszktest-n2:2181,ykszktest-n3:2181/kafka # 必须要有,kafka工作端口- name: KAFKA_ADVERTISED_PORT value: "9092" # 可选- name: KAFKA_ADVERTISED_HOST_NAME value: ykskafkatest-n1 # 可选 - name: KAFKA_HEAP_OPTS value: "-Xmx4G -Xms4G" # JMX相关，可选- name: KAFKA_JMX_OPTS value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=1099" # JMX相关，可选- name: JMX_PORT value: "1099" 数据和志通过hostpath方式挂载出来。完整manifest yaml文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykskafkatest-n1spec: replicas: 1 selector: matchLabels: app: ykskafkatest-n1 template: metadata: labels: app: ykskafkatest-n1 spec: hostname: ykskafkatest-n1 volumes: - name: ykskafkatest-data hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-data-n1 - name: ykskafkatest-logs hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-logs-n1 dnsPolicy: ClusterFirst containers: - name: ykskafkatest-n1 image: wurstmeister/kafka:1.0.1 imagePullPolicy: Always volumeMounts: - name: ykskafkatest-data readOnly: false mountPath: "/kafka" - name: ykskafkatest-logs readOnly: false mountPath: "/opt/kafka/logs" ports: - containerPort: 9092 - containerPort: 1099 env: # broker ID,必须要有,各节点不能一致 - name: KAFKA_BROKER_ID value: "1" # 必须要有,zk集群 - name: KAFKA_ZOOKEEPER_CONNECT value: ykszktest-n1:2181,ykszktest-n2:2181,ykszktest-n3:2181/kafka # 必须要有,kafka工作端口 - name: KAFKA_ADVERTISED_PORT value: "9092" # 可选 - name: KAFKA_ADVERTISED_HOST_NAME value: ykskafkatest-n1 # 可选 - name: KAFKA_HEAP_OPTS value: "-Xmx4G -Xms4G" # JMX相关，可选 - name: KAFKA_JMX_OPTS value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=1099" # JMX相关，可选 - name: JMX_PORT value: "1099"---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykskafkatest-n2spec: replicas: 1 selector: matchLabels: app: ykskafkatest-n2 template: metadata: labels: app: ykskafkatest-n2 spec: hostname: ykskafkatest-n2 volumes: - name: ykskafkatest-data hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-data-n2 - name: ykskafkatest-logs hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-logs-n2 dnsPolicy: ClusterFirst containers: - name: ykskafkatest-n1 image: wurstmeister/kafka:1.0.1 imagePullPolicy: Always volumeMounts: - name: ykskafkatest-data readOnly: false mountPath: "/kafka" - name: ykskafkatest-logs readOnly: false mountPath: "/opt/kafka/logs" ports: - containerPort: 9092 - containerPort: 1099 env: - name: KAFKA_BROKER_ID value: "2" - name: KAFKA_ZOOKEEPER_CONNECT value: ykszktest-n1:2181,ykszktest-n2:2181,ykszktest-n3:2181/kafka - name: KAFKA_ADVERTISED_PORT value: "9092" - name: KAFKA_ADVERTISED_HOST_NAME value: ykskafkatest-n2 - name: KAFKA_HEAP_OPTS value: "-Xmx4G -Xms4G" - name: KAFKA_JMX_OPTS value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=1099" - name: JMX_PORT value: "1099"---kind: DeploymentapiVersion: extensions/v1beta1metadata: name: ykskafkatest-n3spec: replicas: 1 selector: matchLabels: app: ykskafkatest-n3 template: metadata: labels: app: ykskafkatest-n3 spec: hostname: ykskafkatest-n3 volumes: - name: ykskafkatest-data hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-data-n3 - name: ykskafkatest-logs hostPath: path: /data/ykskafkatest-cluster/ykskafkatest-logs-n3 dnsPolicy: ClusterFirst containers: - name: ykskafkatest-n1 image: wurstmeister/kafka:1.0.1 imagePullPolicy: Always volumeMounts: - name: ykskafkatest-data readOnly: false mountPath: "/kafka" - name: ykskafkatest-logs readOnly: false mountPath: "/opt/kafka/logs" ports: - containerPort: 9092 - containerPort: 1099 env: - name: KAFKA_BROKER_ID value: "3" - name: KAFKA_ZOOKEEPER_CONNECT value: ykszktest-n1:2181,ykszktest-n2:2181,ykszktest-n3:2181/kafka - name: KAFKA_ADVERTISED_PORT value: "9092" - name: KAFKA_ADVERTISED_HOST_NAME value: ykskafkatest-n3 - name: KAFKA_HEAP_OPTS value: "-Xmx4G -Xms4G" - name: KAFKA_JMX_OPTS value: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.rmi.port=1099" - name: JMX_PORT value: "1099"---apiVersion: v1kind: Servicemetadata: labels: app: ykskafkatest-n1 name: ykskafkatest-n1 namespace: defaultspec: clusterIP: None ports: - port: 9092 protocol: TCP targetPort: 9092 name: kafka - port: 1099 protocol: TCP targetPort: 1099 name: jmx selector: app: ykskafkatest-n1 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykskafkatest-n2 name: ykskafkatest-n2 namespace: defaultspec: clusterIP: None ports: - port: 9092 protocol: TCP targetPort: 9092 name: kafka - port: 1099 protocol: TCP targetPort: 1099 name: jmx selector: app: ykskafkatest-n2 sessionAffinity: None type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: app: ykskafkatest-n3 name: ykskafkatest-n3 namespace: defaultspec: clusterIP: None selector: ports: - port: 9092 protocol: TCP targetPort: 9092 name: kafka - port: 1099 protocol: TCP targetPort: 1099 name: jmx selector: app: ykskafkatest-n3 sessionAffinity: None type: ClusterIP 部署完成后，查看pod状态： 1234root@yksv001238:~/test/kafka# kubectl get pods -o wide | grep kafkaykskafkatest-n1-5b78b89fb-srnft 1/1 Running 0 2h 10.100.1.45 yksv001239ykskafkatest-n2-5f57ccb9c4-9ghsd 1/1 Running 0 2h 10.100.0.93 yksv001238ykskafkatest-n3-66ccfcbd96-dg2ch 1/1 Running 0 6m 10.100.1.49 yksv001239 分别进入两台kafka容器内，分别使用消费者/生产者脚本进行测试，能正常生产/消费消息，则kafka集群部署成功： 1234567891011121314151617# n1bash-4.4# kafka-topics.sh --create \&gt; --topic test \&gt; --zookeeper ykszktest-n1:2181,ykszktest-n2:2181,ykszktest-n3:2181/kafka \&gt; --partitions 3 \&gt; --replication-factor 2Created topic "test2".# 在n1 开启生产者，输入数据，可以在消费者端接收到数据bash-4.4# kafka-console-producer.sh --topic test2 --broker-list localhost:9092&gt;hello world?&gt;hello world!# n2 开启消费者，接收到数据bash-4.4# kafka-console-consumer.sh --topic test2 --bootstrap-server localhost:9092&gt;hello world?&gt;hello world! 注意：直接进入容器内运行kafka-console-consumer.sh或者kafka-console-producer.sh会报错： 1Error: JMX connector server communication error: service:jmx:rmi://9dcb21ce1644:1099 这是因为这两个脚本在运行时会执行/opt/kafka_2.12-1.0.1/bin/kafka-run-class.sh脚本，这个脚本在启动时会获取JMX_PORT环境变量并运行一个带有jmx的jvm，这时就会与系统当前的jmx端口冲突，因此，可以使用unset JMX_PORT取消环境变量解决这个问题。 详情参考：https://github.com/wurstmeister/kafka-docker/wiki#why-do-kafka-tools-fail-when-jmx-is-enabled Problem: Tools such as kafka-topics.sh and kafka-console-producer.sh fail when JMX is enabled. This is caused because of the JMX_PORT environment variable. The Kafka helper script /opt/kafka/bin/kafka-run-class.sh will try to invoke the required command in a new JVM with JMX bound to the specified port. As the broker JVM that is already running in the container has this port bound, the process fails and exits with error. Solution: Although we’d recommend not running operation tools inside of your running brokers, this may sometimes be desirable when performing local development and testing.]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(十)、微服务--istio1.0抢鲜测试]]></title>
    <url>%2F2018%2F08%2F02%2Fk8s(%E5%8D%81)%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1--istio1.0%E6%8A%A2%E9%B2%9C%E6%B5%8B%E8%AF%95.html</url>
    <content type="text"><![CDATA[前言此前写了三篇文章，介绍了istio的工作原理、流量调度策略、服务可视化以及监控：k8s（四）、微服务框架istio安装测试k8s（五）、微服务框架istio流量策略控制k8s（六）、微服务框架istio服务可视化与监控istio项目成立1/多以来，鲜有生产上的用例，但这次最新的1.0版本，在官网介绍上赫然写着: All of our core features are now ready for production use. 既然宣称生产可用，那么不妨来试用一番昨晚刚发布热气腾腾的1.0版本。 一、安装首先环境还是基于此前的k8s v1.9集群1.下载安装istio官方包： 1234curl -L https://git.io/getLatestIstio | sh -mv istio-1.0.0/ /usr/localln -sv /usr/local/istio-1.0.0/ /usr/local/istiocd /usr/local/istio/install/kubernetes/ 由于集群使用traefik对外服务，因此对官方的istio-demo.yaml部署文件中名为istio-ingressgateway的Service部分稍作修改，修改部分未删除已注释，大约在文件的第2482行开始：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354### vim istio-demo.yaml'''apiVersion: v1kind: Servicemetadata: name: istio-ingressgateway namespace: istio-system annotations: labels: chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgatewayspec: #type: LoadBalancer type: ClusterIP selector: app: istio-ingressgateway istio: ingressgateway ports: - name: http2 #nodePort: 31380 port: 80 #targetPort: 80 - name: https #nodePort: 31390 port: 443 - name: tcp #nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 #targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 #targetPort: 8060 - name: http2-prometheus port: 15030 #targetPort: 15030 - name: http2-grafana port: 15031 #targetPort: 15031''' 修改完毕，kubectl apply -f istio-demo.yaml出现了一些类似如下的报错:1error: unable to recognize "install/kubernetes/istio-demo.yaml": no matches for authentication.istio.io/, Kind=Policy WTF?第一步就出错？去Github查了一下，找到一个issue下的答案说再次kubectl apply -f istio-demo.yaml就不会有报错了，果然，再次apply之后确实没有报错，推测可能是某些资源之间的依赖关系导致的，几千行的yaml文件，就不深入寻找原因了，继续往下。 稍后查看pod和svc是否正常部署： 123456789101112131415root@yksv001238:/usr/local/istio/install/kubernetes# kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEgrafana-6995b4fbd7-kpfsz 1/1 Running 0 34mistio-citadel-54f4678f86-bfv72 1/1 Running 0 34mistio-egressgateway-5d7f8fcc7b-fpfbr 1/1 Running 0 34mistio-galley-7bd8b5f88f-gtq2x 1/1 Running 0 34mistio-ingressgateway-6f58fdc8d7-pwwrz 1/1 Running 0 34mistio-pilot-d99689994-rfk84 2/2 Running 0 34mistio-policy-766bf4bd6d-gscqs 2/2 Running 0 34mistio-sidecar-injector-85ccf84984-m9v99 1/1 Running 0 34mistio-statsd-prom-bridge-55965ff9c8-8fv4m 1/1 Running 0 34mistio-telemetry-55b6b5bbc7-9ptrx 2/2 Running 0 34mistio-tracing-77f9f94b98-5ghc5 1/1 Running 0 34mprometheus-7456f56c96-qrfpg 1/1 Running 0 34mservicegraph-684c85ffb9-6ctmc 1/1 Running 0 34m 12345678910111213141516171819root@yksv001238:/usr/local/istio/install/kubernetes# kubectl get svc -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgrafana ClusterIP 10.99.131.246 &lt;none&gt; 3000/TCP 34mistio-citadel ClusterIP 10.96.83.131 &lt;none&gt; 8060/TCP,9093/TCP 34mistio-egressgateway ClusterIP 10.111.29.59 &lt;none&gt; 80/TCP,443/TCP 34mistio-galley ClusterIP 10.101.207.154 &lt;none&gt; 443/TCP,9093/TCP 34mistio-ingressgateway ClusterIP 10.96.171.51 &lt;none&gt; 80/TCP,443/TCP,31400/TCP,15011/TCP,8060/TCP,15030/TCP,15031/TCP 34mistio-pilot ClusterIP 10.103.244.35 &lt;none&gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 34mistio-policy ClusterIP 10.108.241.232 &lt;none&gt; 9091/TCP,15004/TCP,9093/TCP 34mistio-sidecar-injector ClusterIP 10.103.39.16 &lt;none&gt; 443/TCP 34mistio-statsd-prom-bridge ClusterIP 10.100.205.153 &lt;none&gt; 9102/TCP,9125/UDP 34mistio-telemetry ClusterIP 10.106.175.49 &lt;none&gt; 9091/TCP,15004/TCP,9093/TCP,42422/TCP 34mjaeger-agent ClusterIP None &lt;none&gt; 5775/UDP,6831/UDP,6832/UDP 34mjaeger-collector ClusterIP 10.107.200.30 &lt;none&gt; 14267/TCP,14268/TCP 34mjaeger-query ClusterIP 10.104.201.226 &lt;none&gt; 16686/TCP 34mprometheus ClusterIP 10.103.110.167 &lt;none&gt; 9090/TCP 34mservicegraph ClusterIP 10.108.243.227 &lt;none&gt; 8088/TCP 34mtracing ClusterIP 10.96.208.110 &lt;none&gt; 80/TCP 34mzipkin ClusterIP 10.98.50.58 &lt;none&gt; 9411/TCP 34m 可以看到，istio-sidecar-injector已经部署成功，在此前的0.x的版本中，开启注入的钩子步骤如下，稍显繁琐，在这里已经全部集成在这一个istio-demo.yaml文件中了，省去许多步骤。0.x版本开启注入的步骤： 开启指定命名空间的自动注入： 1kubectl label namespace default istio-injection=enabled 二、demo测试继续沿用此前的测试demo，对此demo拆分结构不了解的可以先看前面的文章： 1git clone https://github.com/yinwenqin/istio-test.git 部署服务： 1234567891011cd istio-testkubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 暴露服务 123kubectl apply -f istio/ingress-python.ymlkubectl apply -f istio/ingress-js.yml# 这两个ingress内的host字段均为http://istio-test.will，根据path路由到不同的服务上 查看部署结果： 123456789101112131415161718192021222324252627root@yksv001238:~/istio-test/service# kubectl get podsservice-go-v1-84498949c8-4cpks 2/2 Running 1 5hservice-go-v1-84498949c8-cjnwq 2/2 Running 0 5hservice-go-v2-f7c7fb8d-46xq5 2/2 Running 0 5hservice-go-v2-f7c7fb8d-7t7fm 2/2 Running 0 5hservice-js-v1-6454464f4c-czqsh 2/2 Running 1 5hservice-js-v1-6454464f4c-v6l58 2/2 Running 0 5hservice-lua-v1-65bb7c6b8d-kk4xm 2/2 Running 0 5hservice-lua-v1-65bb7c6b8d-pbkng 2/2 Running 0 5hservice-lua-v2-864d7954f4-fd454 2/2 Running 0 5hservice-lua-v2-864d7954f4-h89h8 2/2 Running 0 5hservice-node-v1-6c87fdc44d-mf7ch 2/2 Running 0 5hservice-node-v1-6c87fdc44d-zlbsj 2/2 Running 1 5hservice-node-v2-7698dc75cd-qhckq 2/2 Running 0 5hservice-node-v2-7698dc75cd-r4qs8 2/2 Running 0 5hservice-python-v1-c5bf498b-7s47j 2/2 Running 0 5hservice-python-v1-c5bf498b-cb7wr 2/2 Running 0 5hservice-python-v2-7c8b865568-6wzhk 2/2 Running 0 5hservice-python-v2-7c8b865568-8bf7h 2/2 Running 0 5h###root@yksv001238:~/istio-test/service# kubectl describe pod service-go-v1-84498949c8-4cpks...Image: gcr.io/istio-release/proxy_init:1.0.0Image: registry.cn-shanghai.aliyuncs.com/istio-test/service-go:v1##可以看到初始化步骤中生成了两个容器，自动注入成功！ 获取到istio-ingressgateway的ClusterIP： 12root@yksv001238:~/istio-test/istio# kubectl get svc -n istio-system | grep istio-ingressgatewayistio-ingressgateway ClusterIP 10.96.171.51 &lt;none&gt; 80/TCP,443/TCP,31400/TCP,15011/TCP,8060/TCP,15030/TCP,15031/TCP 19h 在本机添加一条dns记录，关联ingress host和istio-ingressgateway： 110.96.171.51 istio-test.will 打开浏览器测试：打不开？哭脸一张，莫不是新版资源间的交互方式有什么变化，导致此前的方式不再适用，联想到1.0版本新增了gateway这种资源，去官网查官网的用例，果然发现了不一样的地方 官网的新版bookinfo用例说明：https://istio.io/docs/examples/bookinfo/#confirm-the-app-is-running查看官方包中用例的服务暴露文件，发现已经不再适用ingress这种资源了，现版用的是由istio申明的Gateway和VirtualService这两种资源结合工作，代替之前的ingress。 cat istio/samples/bookinfo/networking/bookinfo-gateway.yaml123456789101112131415161718192021222324252627282930313233343536373839apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: bookinfo-gatewayspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - "*"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: bookinfospec: hosts: - "*" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 看来要针对ingress进行修改了，原来的ingress-js.yaml和ingress-python.yaml文件内容: 1234567891011121314151617181920212223242526272829303132333435# service-js ingress配置apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-js annotations: kubernetes.io/ingress.class: istiospec: rules: - host: istio-test.will http: paths: - path: /.* backend: serviceName: service-js servicePort: 80 --- # service-python ingress配置apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-python annotations: kubernetes.io/ingress.class: istiospec: rules: - host: istio-test.will http: paths: - path: /env backend: serviceName: service-python servicePort: 80 修改之后（几个注意的点会在后方说明）： 123456789101112131415161718192021222324252627282930313233343536373839404142root@yksv001238:~/istio-test/istio# cat gateway-demo.yml apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: demo-gatewayspec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - "*"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: test-demospec: hosts: - istio-test.will gateways: - demo-gateway http: - match: - uri: exact: "/" route: - destination: host: service-js port: number: 80 - match: - uri: prefix: "/env" route: - destination: host: service-python port: number: 80 查看资源是否生成：123456789root@yksv001238:~/istio-test/istio# kubectl apply -f gateway-demo.yml gateway "demo-gateway" configuredvirtualservice "test-demo" configuredroot@yksv001238:~/istio-test/istio# kubectl get gatewayNAME AGEdemo-gateway 2mroot@yksv001238:~/istio-test/istio# kubectl get virtualserviceNAME AGEtest-demo 2m 浏览器查看效果： 如此前一样，多次点击发射按钮，前端js服务在调用后端的服务时，k8s会根据svc的ep负载均衡策略将流量均衡调度，因此可以看到每次点击按钮，后端服务的版本号都不一致。 服务暴露注意事项：一、.gateway是推荐作为一个项目的所有微服务节点的通用网关来使用的，建议所有VirtualService共用此gateway 二、.gateway.spec.selector 必须有istio: ingressgateway标签，才能加入istio-ingress-gateway这个统一的入口 三、.如果gateway之下的存在多个VirtualService，且VirtualService.spec.hosts如果存在域名冲突的情况，后者会覆盖前者的配置，因此，同一域名下的服务，建议放入同一VirtualService下。 四（重要）、.VirtualService.spec.http中的uri匹配规则有三种： 1.不写match规则，从根路径起包含所有子路径完全匹配到destination.host指向的svc 2.match匹配方式为exact，此方式为路径完全绝对匹配，例如上方的exact: “/“只能匹配根路径 3.match匹配方式为prefix，此方式为路径前缀匹配，例如上方的prefix: “/env”则会匹配以/env开头的所有路径 补充说明：根据路径做路由匹配时，如果写为模糊匹配的方式，且存在某一匹配规则的匹配路径覆盖了其他规则的匹配路径，此时被覆盖的规则对应的服务如果存在前端js与后端交互，则会出现304跳转以及随之而来的同源策略造成的跨域问题，因此这里的路径规则匹配要避免不必要的路径重叠以避免出现此类问题。(见下方举例) 举例： 将上方的VirtualService的第一段稍作修改，exact方式注释掉改为prefix，其余保持不变，此时匹配前缀”/”路径会覆盖其余所有的子路径，因此会产生304跳转：12345http:- match: - uri: #exact: "/" prefix: "/" kubectl apply之后再次打开浏览器查看效果：无论点击多少次“发射”按钮，结果都是undefined，浏览器提示js未运行，原因如上方说明，请求在处理的过程中先从&quot;/&quot;在转到&quot;/env&quot;过程中出现了304跳转，此时js跨域将被禁，因此，一定要避免不必要的路径设计重叠。 简单测试使用先到这里，随后将继续体验关于流量策略以及安全策略的一些新特性，待补充。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(九)、监控--Prometheus扩展篇（mysqld-exporter、服务发现、监控项、联邦、relabel）]]></title>
    <url>%2F2018%2F06%2F28%2Fk8s(%E4%B9%9D)%E3%80%81%E7%9B%91%E6%8E%A7--Prometheus%E6%89%A9%E5%B1%95%E7%AF%87%EF%BC%88mysqld-exporter%E3%80%81%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E3%80%81%E7%9B%91%E6%8E%A7%E9%A1%B9%E3%80%81%E8%81%94%E9%82%A6%E3%80%81relabel%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言上一篇主要介绍prometheus基础告警相关，本篇再进行扩展，加入mysql监控，整理出一些监控告警表达式及配置文件，以及部署prometheus 2.0版之后支持的联邦job，多prometheus分支数据聚合，在多集群环境中很适用。 mysqld-exporter部署：在公司的环境中，大部分的DB已迁移至K8S内运行，因此再以常见的节点二进制安装部署mysqld-exporter的方式不再适用。在本地k8s环境中，采取单pod包含多容器镜像的方式，为原本的DB pod实例多注入一个mysql-exporter的方式，对两者进行整合为一体部署，类似此前文章里istio的注入。注入后的deployment文件的spec.template.spec.containers变更如下： 123456789101112131415161718192021222324252627282930313233343536373839spec: containers: - name: mydbtm env: - name: DB_NAME value: mydbtm image: registry.xxx.com:5000/mysql:56v6 imagePullPolicy: Always livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 3306 timeoutSeconds: 30 ports: - containerPort: 3306 protocol: TCP resources: limits: memory: 4Gi requests: memory: 512Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /var/lib/mysql name: datadir - name: mysql-exporter env: - name: DATA_SOURCE_NAME value: $username:$passwrod@(127.0.0.1:3306)/ image: prom/mysqld-exporter imagePullPolicy: Always name: mysql-exporter ports: - containerPort: 9104 protocol: TCP 额外多增加一个mysql-exporter的容器，并提供环境变量DATA_SOURCE_NAME，注意，pod运行起来后，需要保证DATA_SOURCE_NAME的账号有数据库的读写权限。否则权限问题会导致无法获取数据。 进入pod查看端口3306/9104是否监听正常： 部署完mysql实例以及mysqld-exporter后，需要在prometheus的配置文件中增加相应的job。因为容器内POD IP是可变的，为了保证连接的持续可靠性，这里为该deployment创建了一个svc，通过svc的cluster IP与prometheus进行绑定，这样就基本不用担心POD IP变化了。 首先创建svc，捆绑2个服务端口：123456789101112131415161718192021apiVersion: v1kind: Servicemetadata: labels: app: mydbtm name: mydbtm namespace: defaultspec: ports: - port: 3306 protocol: TCP targetPort: 3306 name: mysql - port: 9104 protocol: TCP targetPort: 9104 name: mysql-exporter selector: app: mydbtm sessionAffinity: None type: ClusterIP kubectl get svc 获取刚创建的svc 的cluster IP，绑定至prometheus的confimap：在prometheus-configmap.yaml下添加一个job，targets对应的列表内填写刚获取到的cluster IP和9104端口，后续增加实例可直接在列表中添加元素：1234567891011- job_name: 'mysql performance' scrape_interval: 1m static_configs: - targets: ['10.102.20.179:9104'] params: collect[]: - global_status - perf_schema.tableiowaits - perf_schema.indexiowaits - perf_schema.tablelocks 更新配置文件后滚动更新pod加载配置文件：1kubectl patch deployment prometheus --patch '&#123;"spec": &#123;"template": &#123;"metadata": &#123;"annotations": &#123;"update-time": "2018-06-25 17:50" &#125;&#125;&#125;&#125;&#125;' -n kube-system 查看prometheus的target，可以看到mysql-exporter已经up了，如果有异常，检查pod志： 打开grafana查看mysql模板里的监控图： 服务发现在k8s环境中，部署的应用可能是海量的，如果像上方一样手动在static_configs.targets中增加对象，然后再触发prometheus实例的更新，未免显得太过麻烦和笨重，好在promethues提供SD(service discovery)功能，支持基于k8s的自动发现服务，可以自动发现k8s的node/service/pod/endpoints/ingress这些类型的资源。 其实在第一部分部署时，已经指定了kubernetes-service的target job，但是可以看到在web /target 路径页面上没有看到任何相关的job，连kubernetes-service的标题都没有： 在查阅了官方文档后，发现其中的说明并不详尽，没有具体的配置样例，网上关于此类场景的文章也没有找到，为此，特意去github上提交了issue向项目member请教，最后得知了不生效的原因为标签不匹配。 github issue链接：https://github.com/prometheus/prometheus/issues/4326官方文档配置说明链接：prometheus配置 作出如下修改后，配置生效，web上可见：1.job: 12345678910111213141516171819- job_name: 'kube_svc_mysql-exporter' kubernetes_sd_configs: - role: service metrics_path: /metrics relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true # 匹配svc.metadata.annotation字段包含key/value(prometheus.io/probe: "true") - source_labels: [__meta_kubernetes_service_port_name] action: keep regex: "mysql-exporter" # 匹配svc.spec.ports.name 是否包含mysql-exporter - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name svc样例： 12345678910111213141516171819202122232425apiVersion: v1kind: Servicemetadata: labels: app: testdbtm annotations: # 必须包含这个kv以匹配上方job规则 prometheus.io/probe: true name: testdbtm namespace: defaultspec: ports: - port: 3306 protocol: TCP targetPort: 3306 name: mysql - port: 9104 protocol: TCP targetPort: 9104 # 必须包含这个name以匹配job规则 name: mysql-exporter selector: app: orderdealdbtm sessionAffinity: None type: ClusterIP 更新configmap以及prometheus实例后，即可在web /targets页面中查看到自动发现的包含mysql-exporter的svc，通过其集群FQDN域名+9104端口获取mysql-exporter metrics，实现自动发现。其他的各种应用，也可以类似定制。 监控项经过这几天的验证，整理了一些基础的监控项表达式，根据exporter进行了分类，当然这还不够，需要根据环境持续补充，调整.注意，metric的名称在不同的版本有些许不同，我们的环境中有两套prometheus，其中一套对应的exporter版本较低，注意点已注释出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495targets: cadvisor: #包含pod(container)相关指标 url: http://$NODE_IP:10255 metric expr: ### 容器内存使用率超过limit值80%,部分pod没有limit限制，所以值为+Inf，需要排除 - container_memory_usage_bytes&#123;kubernetes_container_name!=""&#125; / container_spec_memory_limit_bytes&#123;kubernetes_container_name!=""&#125; *100 != +Inf &gt; 80 #低版本 - container_memory_usage_bytes&#123;container_name!=""&#125; / container_spec_memory_limit_bytes&#123;container_name!=""&#125; *100 != +Inf &gt; 80 #新版本 ### 计算pod的内存使用情况,单位为MB，可以按需设定阈值，有两个表达式都可以实现，感觉第一个结果更清晰。 - sum (container_memory_working_set_bytes&#123;image!="",name=~"^k8s_.*"&#125;) by (pod_name) /1024 /1024 #低版本pod_name改成kubernetes_pod_name - sort_desc(sum(container_memory_usage_bytes&#123;image!=""&#125;) by (io_kubernetes_container_name, image)) / 1024 / 1024 #通用 ### 根据相同的pod_name来计算过去一分钟内pod 的 cpu使用率,metric名称版本不同也有些不一样： - sum by (pod_name)( rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m] ) ) * 100 &gt; 70 #高版本 - sum by (kubernetes_pod_name)( rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m] ) ) * 100 &gt; 70 #低版本 ### 计算pod的网络IO情况，单位为Mbps # rx方向 - sort_desc(sum by (kubernetes_pod_name) (rate (container_network_receive_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 * 8 &gt; 100 #低版本 - sort_desc(sum by (pod_name) (rate (container_network_receive_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 *8 &gt; 100 #高版本 # tx方向 - sort_desc(sum by (kubernetes_pod_name) (rate (container_network_transmit_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 * 8 &gt; 100 #低版本 - sort_desc(sum by (pod_name) (rate (container_network_transmit_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 *8 &gt; 100 #高版本 node-exporter: ### 主要包含node相关的指标 # url: http://$NODE_IP:31672/metrics metric expr: ### node磁盘使用率 - (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 &gt; 80 ### node内存使用率 - (node_memory_MemTotal_bytes - node_memory_MemFree_bytes) / node_memory_MemTotal_bytes * 100 &gt; 80 ### 计算node cpu使用率，关于此项计算，参考了许多个模板的表达式，但感觉都不太精确，下面两个是模板里找到的： # 这些模板里计算node cpu使用率的表达式，都是拿容器cpu使用情况除以节点cpu总数，这样计算出来的结果感觉有些偏差，因为node本身 \ # 还有一些额外开销，虽然在大集群中，node本身的开销相比较于这么多容器开销来说占比较小，但是还是不可忽视： - sum(sum by (io_kubernetes_container_name)( rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m] ) )) / count(node_cpu&#123;mode="system"&#125;) * 100 - sum (rate (container_cpu_usage_seconds_total&#123;id="/"&#125;[1m])) / sum (machine_cpu_cores) * 100 # 因此经过对集群cpu使用率的实际观察，我觉得下面这个表达式计算出的数值更为准确： - (sum(rate(node_cpu_seconds_total[1m])) - sum(rate(node_cpu_seconds_total&#123;mode="idle"&#125;[1m]))) / sum(rate(node_cpu_seconds_total[1m])) * 100 &gt; 80 # 在老版本的exporter中`node_cpu_seconds_total`这个metric值叫`node_cpu`,因此老版本使用这个表达式： - (sum(rate(node_cpu[1m])) - sum(rate(node_cpu&#123;mode="idle"&#125;[1m]))) / sum(rate(node_cpu[1m])) * 100 &gt; 80 mysql-exporter: ### 主要包含mysql相关指标 ## url: http://$Mysql_IP:9104/metrics # innodb各类型缓存缓存大小： - InnoDB Buffer Pool Data size : mysql_global_status_innodb_page_size * on (instance) mysql_global_status_buffer_pool_pages&#123;state="data"&#125; - InnoDB Log Buffer Size: mysql_global_variables_innodb_log_buffer_size - InnoDB Additional Memory Pool Size: mysql_global_variables_innodb_additional_mem_pool_size - InnoDB Dictionary Size: mysql_global_status_innodb_mem_dictionary - Key Buffer Size: mysql_global_variables_key_buffer_size - Query Cache Size: mysql_global_variables_query_cache_size - Adaptive Hash Index Size: mysql_global_status_innodb_mem_adaptive_hash metric expr: # 实例启动时间,单位s，三分钟内有重启记录则告警 - mysql_global_status_uptime &lt; 180 # 每秒查询次数指标 - rate(mysql_global_status_questions[5m]) &gt; 500 # 连接数指标 - rate(mysql_global_status_connections[5m]) &gt; 200 # mysql接收速率,单位Mbps - rate(mysql_global_status_bytes_received[3m]) * 1024 * 1024 * 8 &gt; 50 # mysql传输速率,单位Mbps - rate(mysql_global_status_bytes_sent[3m]) * 1024 * 1024 * 8 &gt; 100 # 慢查询 - rate(mysql_global_status_slow_queries[30m]) &gt; 3 # 死锁 - rate(mysql_global_status_innodb_deadlocks[3m]) &gt; 1 # 活跃线程小于30% - mysql_global_status_threads_running / mysql_global_status_threads_connected * 100 &lt; 30 # innodb缓存占用缓存池大小超过80% - (mysql_global_status_innodb_page_size * on (instance) mysql_global_status_buffer_pool_pages&#123;state="data"&#125; + on (instance) mysql_global_variables_innodb_log_buffer_size + on (instance) mysql_global_variables_innodb_additional_mem_pool_size + on (instance) mysql_global_status_innodb_mem_dictionary + on (instance) mysql_global_variables_key_buffer_size + on (instance) mysql_global_variables_query_cache_size + on (instance) mysql_global_status_innodb_mem_adaptive_hash ) / on (instance) mysql_global_variables_innodb_buffer_pool_size * 100 &gt; 80 根据上方的监控项expr，整理出如下rules规则模板： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176rules.yml: | groups: - name: alert-rule rules: ### Node监控 - alert: NodeFilesystemUsage-high expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 &gt; 80 for: 2m labels: team: node severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Node Filesystem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Node Filesystem usage is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NodeMemoryUsage expr: (node_memory_MemTotal_bytes - node_memory_MemFree_bytes) / node_memory_MemTotal_bytes * 100 &gt; 80 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Node Memory usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Node Memory usage is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NodeCPUUsage expr: (sum(rate(node_cpu_seconds_total[1m])) - sum(rate(node_cpu_seconds_total&#123;mode="idle"&#125;[1m]))) / sum(rate(node_cpu_seconds_total[1m])) * 100 &gt; 80 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Node High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Node CPU usage is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NodeCPUUsage_ expr: (sum(rate(node_cpu[1m])) - sum(rate(node_cpu&#123;mode="idle"&#125;[1m]))) / sum(rate(node_cpu[1m])) * 100 &gt; 80 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Node High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Node CPU usage is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" ### Pod监控 - alert: PodMemUsage expr: container_memory_usage_bytes&#123;container_name!=""&#125; / container_spec_memory_limit_bytes&#123;container_name!=""&#125; *100 != +Inf &gt; 80 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High Mem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod Mem is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: PodMemUsage_ expr: container_memory_usage_bytes&#123;kubernetes_container_name!=""&#125; / container_spec_memory_limit_bytes&#123;kubernetes_container_name!=""&#125; *100 != +Inf &gt; 90 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High Mem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod Mem is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: PodCpuUsage expr: sum by (pod_name)( rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m] ) ) * 100 &gt; 70 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod CPU is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: PodCpuUsage_ expr: sum by (kubernetes_pod_name)( rate(container_cpu_usage_seconds_total&#123;image!=""&#125;[1m] ) ) * 100 &gt; 70 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod CPU is above 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NetI/O_RX expr: sort_desc(sum by (kubernetes_pod_name) (rate (container_network_receive_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 * 8 &gt; 500 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High NetI/O_RX detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod NetI/O_RX is more than 500Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NetI/O_RX_ expr: sort_desc(sum by (pod_name) (rate (container_network_receive_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 *8 &gt; 500 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High NetI/O_RX detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod NetI/O_RX is more than 500Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NetI/O_TX expr: sort_desc(sum by (kubernetes_pod_name) (rate (container_network_transmit_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 * 8 &gt; 500 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High NetI/O_TX detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod NetI/O_TX is more than 100Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: NetI/O_TX_ expr: sort_desc(sum by (pod_name) (rate (container_network_transmit_bytes_total&#123;name!=""&#125;[1m]) )) /1024 /1024 /60 *8 &gt; 500 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High NetI/O_TX detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod NetI/O_TX is more than 100Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" ### Mysql监控 - alert: Mysql_Instance_Reboot expr: mysql_global_status_uptime &lt; 180 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Instance_Reboot detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Instance_Reboot in 3 minute (up to now is: &#123;&#123; $value &#125;&#125; seconds" - alert: Mysql_High_QPS expr: rate(mysql_global_status_questions[5m]) &gt; 500 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_High_QPS detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql opreation is more than 500 per second ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_Too_Many_Connections expr: rate(mysql_global_status_connections[5m]) &gt; 100 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql Too Many Connections detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql Connections is more than 100 per second ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_High_Recv_Rate expr: rate(mysql_global_status_bytes_received[3m]) * 1024 * 1024 * 8 &gt; 100 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_High_Recv_Rate detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Receive_Rate is more than 100Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_High_Send_Rate expr: rate(mysql_global_status_bytes_sent[3m]) * 1024 * 1024 * 8 &gt; 100 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_High_Send_Rate detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql data Send Rate is more than 100Mbps ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_Too_Many_Slow_Query expr: rate(mysql_global_status_slow_queries[30m]) &gt; 3 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Too_Many_Slow_Query detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql current Slow_Query Sql is more than 3 ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_Deadlock expr: mysql_global_status_innodb_deadlocks &gt; 0 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Deadlock detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql Deadlock was found ,(current value is: &#123;&#123; $value &#125;&#125;)" - alert: Mysql_Too_Many_sleep_threads expr: mysql_global_status_threads_running / mysql_global_status_threads_connected * 100 &lt; 30 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_Too_Many_sleep_threads detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_sleep_threads percent is more than &#123;&#123; $value &#125;&#125;, please clean the sleeping threads" - alert: Mysql_innodb_Cache_insufficient expr: (mysql_global_status_innodb_page_size * on (instance) mysql_global_status_buffer_pool_pages&#123;state="data"&#125; + on (instance) mysql_global_variables_innodb_log_buffer_size + on (instance) mysql_global_variables_innodb_additional_mem_pool_size + on (instance) mysql_global_status_innodb_mem_dictionary + on (instance) mysql_global_variables_key_buffer_size + on (instance) mysql_global_variables_query_cache_size + on (instance) mysql_global_status_innodb_mem_adaptive_hash ) / on (instance) mysql_global_variables_innodb_buffer_pool_size * 100 &gt; 80 for: 2m labels: severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Mysql_innodb_Cache_insufficient detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Mysql innodb_Cache was used more than 80% ,(current value is: &#123;&#123; $value &#125;&#125;)" 将这些规则更新进prometheus的configmap中，重新apply config，更新prometheus,查看prometheus的告警页面，可以查看多项告警选项，则配置成功。 联邦prometheus 2.0以上版本的新特性，联邦机制，支持从其他的prometheus扩展合并数据，比较类似zabbix的server端与proxy端，适用于跨集群/机房使用，但不同的是prometheus联邦配置非常简单，简单的增加job指定其他prometheus实例服务IP端口，即可获取数据。我们的环境中有两套集群，两个prometheus server实例，在prometheus的configmap配置增加一个如下job配置即可合并数据统一展示： 12345678910- job_name: 'federate' scrape_interval: 15s honor_labels: true metrics_path: '/federate' params: 'match[]': - '&#123;job=~"kubernetes-.*"&#125;' static_configs: - targets: - '192.168.9.26:9090' 更新配置，更新prometheus pod，重新打开grafana的集群监控模板页面，可以看到数据已经进行了整合： 整合前： 整合后： RelabelRelabel机制可以在Prometheus采集数据存入TSDB之前，通过Target实例的Metadata信息，动态地为Label赋值，可以是metadata中的label，也可以赋值给新label。除此之外，还能根据Target实例的Metadata信息选择是否采集或者忽略该Target实例。例如这里自定义了如下target： 1234567- job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 在web上查看target的原始标签：可以看到instance标签的值为主机名，鼠标悬停展开的黑色框中的标签为metadata标签，form表单里的标签是放入TSDB最终展示的标签 下面给这段配置增加一个自定义标签： 12345678910111213- job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__address__] regex: (.*):(.*) action: replace replacement: $1 target_label: instance 可以看到instance标签的值变成了host的IP地址relabel配置解析： 12345source_labels: [__address__] #值的来源标签（metadata），__address__是标签名regex: (.*):(.*) #正则匹配表达式action: replace #动作为替换，默认动作replacement: $1 #替换的字段，即将正则表达式第一段匹配到的值赋值给target_labeltarget_label: instance #被赋值的标签，可以为metadata中的标签，也可以是自定义的新标签名 通过relabel，可以做一些自定义过滤规则，实现特殊的需求订制 配置热更新监控配置项调整是非常常见的操作，如果每次调整配置后，都需要重建pod，过程还是比较繁琐漫长的，好在prometheus提供热重载的api，访问方式为： 12curl -X POST http://$&#123;IP/DOMAIN&#125;/-/reload# 注意，2.0版本以后默认重载api没有开启，需要在prometheus的启动参数里添加指定--web.enable-lifecycle 参数 总结：以上就是在上一篇的基础上对prometheus适应生产环境做的一些扩展，当然，需要做的监控项、应用还很多，配置文件、修改后的grafana模板文件随后打包附上，欢迎交流讨论，一起扩展。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(八)、监控--Prometheus告警篇（告警消息对接钉钉接口）]]></title>
    <url>%2F2018%2F06%2F26%2Fk8s(%E5%85%AB)%E3%80%81%E7%9B%91%E6%8E%A7--Prometheus%E5%91%8A%E8%AD%A6%E7%AF%87%EF%BC%88%E5%91%8A%E8%AD%A6%E6%B6%88%E6%81%AF%E5%AF%B9%E6%8E%A5%E9%92%89%E9%92%89%E6%8E%A5%E5%8F%A3%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言承接上章k8s(七)、Prometheus部署篇，在上章的基础上，本章介绍Prometheus告警相关配置。 一、Querying expr &amp; PromQL在了解告警规则之前，首先得了解Prometheus的数据查询表达式，来获取metric数据是否到达告警阈(ps:这个字儿念yu，第四声，不念第二声的fa)值。 OverviewPrometheus提供了一种功能性表达式语言，能够让用户实时的选择和聚合时间序列的数据。表达式返回的结果可以被显示为曲线图，也可以在prometheus浏览器中显示为表格，或者通过HTTP API经由外部系统处理。 表达式语言类型Prometheus表达式或子表达式可以评估为一下四种类型之一：即时向量（Instant vector） - 包含每个时间序列单个样品的一组时间序列，共享相同的时间戳范围向量（Range vector） - 包含一个范围内数据点的一组时间序列标量（Scalar） - 一个简单的数字浮点值字符串（String） - 一个简单的字符串值；当前未使用根据使用情况（例如画图或者显示表达式的输出），只有某些类型是合法的，例如，即时向量表达式是可以画图的唯一类型。 时间序列选择器 即时向量选择即时向量选择器允许选择一组时间序列，或者某个给定的时间戳的样本数据。下面这个例子选择了具有时间序列的http_requests_total metric对象： 1http_requests_total 你可以通过附加一组标签，并用{}括起来，来进一步筛选这些时间序列。下面这个例子只选择有http_requests_total名称的、有prometheus工作标签的、有canary组标签的时间序列： 1http_requests_total&#123;job="prometheus",group="canary"&#125; 另外，也可以也可以将标签值反向匹配，或者对正则表达式匹配标签值。下面列举匹配操作符： =：选择正好相等的字符串标签!=：选择不相等的字符串标签=~：选择匹配正则表达式的标签（或子标签）!=：选择不匹配正则表达式的标签（或子标签）例如，选择staging、testing、development环境下的，GET之外的HTTP方法的http_requests_total的时间序列： 1http_requests_total&#123;environment=~"staging|testing|development",method!="GET"&#125; 范围向量选择范围向量表达式正如即时向量表达式一样运行，但是前者返回从当前时刻开始的一定时间范围的时间序列集合回来。语法是，在一个向量表达式之后添加[]来表示时间范围，持续时间用数字表示，后接下面单元之一： s：secondsm：minutesh：hoursd：daysw：weeksy：years在下面这个例子中，我们选择最后5分钟的记录，metric名称为http_requests_total、作业标签为prometheus的时间序列的所有值： 1http_requests_total&#123;job="prometheus"&#125;[5m] 操作符Prometheus支持多种二元和聚合的操作符。 1234567891011121314151617+ (addition)- (subtraction)* (multiplication)/ (division)% (modulo)^ (power/exponentiation)== (equal)!= (not-equal)&gt; (greater-than)&lt; (less-than)&gt;= (greater-or-equal)&lt;= (less-or-equal)and (intersection)or (union)unless (complement) 函数Prometheus支持多种函数，来对数据进行操作，参考官网：https://prometheus.io/docs/prometheus/latest/querying/functions/ 了解了metric查询表达式的基础上，下面开始部署告警器组件。 二、告警媒介prometheus支持多种类型的告警媒介，国内可以用的例如mail/webchat，也可以使用webhook自定义接口，公司一般使用的钉钉，prometheus没有定制的钉钉接口，因此在这里使用自定义的接口来中转，将消息发送至钉钉接口，接收钉钉告警消息。首先看下告警配置，各项参数已添加中文注释，直接看注释：alertmanager-config.yaml: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061kind: ConfigMapapiVersion: v1metadata: name: alertmanager namespace: kube-systemdata: config.yml: |- global: resolve_timeout: 5m templates: - '/etc/alertmanager-templates/*.tmpl' route: group_by: ['alertname', 'cluster', 'service'] #根据['alertname', 'cluster', 'service']，在初始告警发送前，等待30秒，将这段时间的多个告警进行分组 # When a new group of alerts is created by an incoming alert, wait at # least 'group_wait' to send the initial notification. # This way ensures that you get multiple alerts for the same group that start # firing shortly after another are batched together on the first # notification. group_wait: 30s # When the first notification was sent, wait 'group_interval' to send a batch # of new alerts that started firing for that group. #按上面分组的消息，同一组消息，间隔5m才发送下一个。这个是为了尽量避免由一个问题带来的批量告警重复发送。 group_interval: 5m # If an alert has successfully been sent, wait 'repeat_interval' to # resend them. #完全相同的一个告警消息，间隔repeat_interval时间才下一次发送，这里为了测试效果设置1m，一般这个值设置时间都较长 #repeat_interval: 1m repeat_interval: 1m # A default receiver # If an alert isn't caught by a route, send it to default. #默认接收器为webhook_alert receiver: webhook_alert # All the above attributes are inherited by all child routes and can # overwritten on each. # The child route trees. routes: #给webhook_alert接收器定义匹配标签规则 # Send severity=slack alerts to slack. - match: severity: info receiver: webhook_alert - match: severity: warning receiver: webhook_alert # 接收器定义，这里是webhook类型，webhook_configs写上自定义api接口的url。 # 注意，这里的webhook钩子接口是我自己写的接收prometheus告警的接口，api接收到prometheus的消息后会提取信息，通过钉钉的接口发给相应的人员。 # 也可以使用wechat/mail的接口，配置方式参考官网：https://prometheus.io/docs/alerting/configuration/ receivers: - name: webhook_alert webhook_configs: - url: 'http://192.168.88.26:8080/api/1.0/utils/alert/?token=ADW82115yn7YEWXCEW88WEW6' send_resolved: true prometheus通过webhook发送的告警消息样例： 1234567891011121314151617181920212223242526272829303132333435&#123; "receiver":"webhook_alert", "status":"firing", "alerts":[&#123; "status":"firing", "labels":&#123; "DEVDB":"","alertname":"NodeMemoryUsage-low", "beta_kubernetes_io_arch":"amd64", "beta_kubernetes_io_os":"linux", "instance":"hv001238", "job":"kubernetes-node-exporter","kubernetes_io_hostname":"hv001238","node_role_kubernetes_io_master":"","severity":"info","team":"node" &#125;, "annotations":&#123; "description":"hv001238: Memory usage is above 80% (current value is: 52.18385396227273", "summary":"hv001238: High Memory usage detected" &#125;, "startsAt":"2018-06-23T03:47:10.408935848Z", "endsAt":"0001-01-01T00:00:00Z", "generatorURL":"http://prometheus-785fc5bbf4-ztlrd:9090/graph?g0.expr=%28node_memory_MemTotal_bytes+-+%28node_memory_MemFree_bytes+%2B+node_memory_Buffers_bytes+%2B+node_memory_Cached_bytes%29%29+%2F+node_memory_MemTotal_bytes+%2A+100+%3E+1\\u0026g0.tab=1" &#125;,], "groupLabels":&#123;"alertname":"NodeMemoryUsage-low"&#125;, "commonLabels":&#123; "DEVDB":"", "alertname":"NodeMemoryUsage-low", "beta_kubernetes_io_arch":"amd64", "beta_kubernetes_io_os":"linux", "job":"kubernetes-node-exporter", "node_role_kubernetes_io_master":"", "severity":"info","team":"node"&#125;, "commonAnnotations":&#123;&#125;, "externalURL":"http://alertmanager-7cffc68878-xb2m6:9093", "version":"4", "groupKey":"&#123;&#125;/&#123;severity=\\"info\\"&#125;:&#123;alertname=\\"NodeMemoryUsage-low\\"&#125;"&#125; 消息太多，不够精简，我webhook端在接收之后将它精简提取为了如下格式：123456789&#123; 'description': 'hv001238: Memory usage is above 80% (current value is: 39.42384016274741', 'title': 'hv001238: High Memory usage detected', 'notice_user': ['xxx', 'xxx', 'xxx'], 'start_time': '2018-06-23T03:47:10.408935848Z', 'end_time': '0001-01-01T00:00:00Z', 'status': 'firing', 'graph_link': 'http://http://prometheusv19.xxx.com/graph?g0.expr=%28node_memory_MemTotal_bytes+-+%28node_memory_MemFree_bytes+%2B+node_memory_Buffers_bytes+%2B+node_memory_Cached_bytes%29%29+%2F+node_memory_MemTotal_bytes+%2A+100+%3E+1&amp;g0.tab=1'&#125; 中转api接口python(django + rest-framework框架)代码样例如下，钉钉消息的接口可以自己去官网查：钉钉官网 123456789101112131415161718192021222324252627282930313233343536373839from rest_framework.views import APIViewfrom rest_framework.response import Responseimport collectionsfrom account.tasks.message import send_dingding_messageclass AlertView(APIView): def post(self,request): token = request.GET.get('token') message = collections.OrderedDict() if token == 'ADW82115yn7YEWXCEW88WEW6xxwetrcX': # ops = ['xxx', 'xxx', 'xxx'] ops = ['xxx'] content = request.data message['title'] = content['alerts'][0]['annotations']['summary'] message['status'] = content['status'] message['description'] = content['alerts'][0]['annotations']['description'] message['start_time'] = content['alerts'][0]['startsAt'] message['end_time'] = content['alerts'][0]['endsAt'] #截图有效的url段，并将hostname从podname替换成自己真实环境的域名，这样接收到的消息点击链接可以直接查看告警数据及图形 valid_link = content['alerts'][0]['generatorURL'].split("\\")[0] uri = valid_link.split(':9090')[1] host = "http://prometheus.xxx.com" message['graph_link'] = host + uri messages = "" for k,v in message.items(): messages = messages + '[' + k + ']:' + v + "\n" print(messages) res = send_dingding_message(user=ops,message=messages) return Response(res)def send_dingding_message(user,message) #自己去获取钉钉的api接口，调用接口 return True 三、Alertmanager部署Alertmanager相关部署yaml文件：1234alertmanager-templates.yaml #各类常见告警模板configmap.yaml #配置文件deployment.yaml #部署文件service.yaml #服务文件 alertmanager-templates.yaml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205apiVersion: v1data: default.tmpl: | &#123;&#123; define "__alertmanager" &#125;&#125;AlertManager&#123;&#123; end &#125;&#125; &#123;&#123; define "__alertmanagerURL" &#125;&#125;&#123;&#123; .ExternalURL &#125;&#125;/#/alerts?receiver=&#123;&#123; .Receiver &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__subject" &#125;&#125;[&#123;&#123; .Status | toUpper &#125;&#125;&#123;&#123; if eq .Status "firing" &#125;&#125;:&#123;&#123; .Alerts.Firing | len &#125;&#125;&#123;&#123; end &#125;&#125;] &#123;&#123; .GroupLabels.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .CommonLabels) (len .GroupLabels) &#125;&#125;(&#123;&#123; with .CommonLabels.Remove .GroupLabels.Names &#125;&#125;&#123;&#123; .Values | join " " &#125;&#125;&#123;&#123; end &#125;&#125;)&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__description" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "__text_alert_list" &#125;&#125;&#123;&#123; range . &#125;&#125;Labels: &#123;&#123; range .Labels.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125;Annotations: &#123;&#123; range .Annotations.SortedPairs &#125;&#125; - &#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125;Source: &#123;&#123; .GeneratorURL &#125;&#125; &#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.username" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.fallback" &#125;&#125;&#123;&#123; template "slack.default.title" . &#125;&#125; | &#123;&#123; template "slack.default.titlelink" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.pretext" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.titlelink" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.iconemoji" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.iconurl" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "slack.default.text" &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "hipchat.default.from" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "hipchat.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.description" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.client" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.clientURL" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pagerduty.default.instances" &#125;&#125;&#123;&#123; template "__text_alert_list" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "opsgenie.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "opsgenie.default.description" &#125;&#125;&#123;&#123; .CommonAnnotations.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 -&#125;&#125; Alerts Firing: &#123;&#123; template "__text_alert_list" .Alerts.Firing &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 -&#125;&#125; Alerts Resolved: &#123;&#123; template "__text_alert_list" .Alerts.Resolved &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123; define "opsgenie.default.source" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "victorops.default.message" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125; | &#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "victorops.default.from" &#125;&#125;&#123;&#123; template "__alertmanager" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "email.default.subject" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "email.default.html" &#125;&#125; &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt; &lt;!-- Style and HTML derived from https://github.com/mailgun/transactional-email-templates The MIT License (MIT) Copyright (c) 2014 Mailgun Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --&gt; &lt;html xmlns="http://www.w3.org/1999/xhtml" xmlns="http://www.w3.org/1999/xhtml" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;head style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;meta name="viewport" content="width=device-width" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;title style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;&#123;&#123; template "__subject" . &#125;&#125;&lt;/title&gt; &lt;/head&gt; &lt;body itemscope="" itemtype="http://schema.org/EmailMessage" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; -webkit-font-smoothing: antialiased; -webkit-text-size-adjust: none; height: 100%; line-height: 1.6em; width: 100% !important; background-color: #f6f6f6; margin: 0; padding: 0;" bgcolor="#f6f6f6"&gt; &lt;table style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; width: 100%; background-color: #f6f6f6; margin: 0;" bgcolor="#f6f6f6"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"&gt;&lt;/td&gt; &lt;td width="600" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; display: block !important; max-width: 600px !important; clear: both !important; width: 100% !important; margin: 0 auto; padding: 0;" valign="top"&gt; &lt;div style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; max-width: 600px; display: block; margin: 0 auto; padding: 0;"&gt; &lt;table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; border-radius: 3px; background-color: #fff; margin: 0; border: 1px solid #e9e9e9;" bgcolor="#fff"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 16px; vertical-align: top; color: #fff; font-weight: 500; text-align: center; border-radius: 3px 3px 0 0; background-color: #E6522C; margin: 0; padding: 20px;" align="center" bgcolor="#E6522C" valign="top"&gt; &#123;&#123; .Alerts | len &#125;&#125; alert&#123;&#123; if gt (len .Alerts) 1 &#125;&#125;s&#123;&#123; end &#125;&#125; for &#123;&#123; range .GroupLabels.SortedPairs &#125;&#125; &#123;&#123; .Name &#125;&#125;=&#123;&#123; .Value &#125;&#125; &#123;&#123; end &#125;&#125; &lt;/td&gt; &lt;/tr&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 10px;" valign="top"&gt; &lt;table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;a href="&#123;&#123; template "__alertmanagerURL" . &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #FFF; text-decoration: none; line-height: 2em; font-weight: bold; text-align: center; cursor: pointer; display: inline-block; border-radius: 5px; text-transform: capitalize; background-color: #348eda; margin: 0; border-color: #348eda; border-style: solid; border-width: 10px 20px;"&gt;View in &#123;&#123; template "__alertmanager" . &#125;&#125;&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;[&#123;&#123; .Alerts.Firing | len &#125;&#125;] Firing&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; range .Alerts.Firing &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Labels&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Annotations) 0 &#125;&#125;&lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Annotations&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &lt;a href="&#123;&#123; .GeneratorURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #348eda; text-decoration: underline; margin: 0;"&gt;Source&lt;/a&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;hr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;[&#123;&#123; .Alerts.Resolved | len &#125;&#125;] Resolved&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &#123;&#123; range .Alerts.Resolved &#125;&#125; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top"&gt; &lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Labels&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &#123;&#123; range .Labels.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Annotations) 0 &#125;&#125;&lt;strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt;Annotations&lt;/strong&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &#123;&#123; range .Annotations.SortedPairs &#125;&#125;&#123;&#123; .Name &#125;&#125; = &#123;&#123; .Value &#125;&#125;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt;&#123;&#123; end &#125;&#125; &lt;a href="&#123;&#123; .GeneratorURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; color: #348eda; text-decoration: underline; margin: 0;"&gt;Source&lt;/a&gt;&lt;br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" /&gt; &lt;/td&gt; &lt;/tr&gt; &#123;&#123; end &#125;&#125; &lt;/table&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;div style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; width: 100%; clear: both; color: #999; margin: 0; padding: 20px;"&gt; &lt;table width="100%" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;"&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 12px; vertical-align: top; text-align: center; color: #999; margin: 0; padding: 0 0 20px;" align="center" valign="top"&gt;&lt;a href="&#123;&#123; .ExternalURL &#125;&#125;" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 12px; color: #999; text-decoration: underline; margin: 0;"&gt;Sent by &#123;&#123; template "__alertmanager" . &#125;&#125;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/div&gt; &lt;/td&gt; &lt;td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; &#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.title" &#125;&#125;&#123;&#123; template "__subject" . &#125;&#125;&#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.message" &#125;&#125;&#123;&#123; .CommonAnnotations.SortedPairs.Values | join " " &#125;&#125; &#123;&#123; if gt (len .Alerts.Firing) 0 &#125;&#125; Alerts Firing: &#123;&#123; template "__text_alert_list" .Alerts.Firing &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; if gt (len .Alerts.Resolved) 0 &#125;&#125; Alerts Resolved: &#123;&#123; template "__text_alert_list" .Alerts.Resolved &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; end &#125;&#125; &#123;&#123; define "pushover.default.url" &#125;&#125;&#123;&#123; template "__alertmanagerURL" . &#125;&#125;&#123;&#123; end &#125;&#125; slack.tmpl: | &#123;&#123; define "slack.devops.text" &#125;&#125; &#123;&#123;range .Alerts&#125;&#125;&#123;&#123;.Annotations.DESCRIPTION&#125;&#125; &#123;&#123;end&#125;&#125; &#123;&#123; end &#125;&#125;kind: ConfigMapmetadata: creationTimestamp: null name: alertmanager-templates namespace: kube-system configmap.yaml： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869kind: ConfigMapapiVersion: v1metadata: name: alertmanager namespace: kube-systemdata: config.yml: |- global: # ResolveTimeout is the time after which an alert is declared resolved # if it has not been updated. resolve_timeout: 5m # The smarthost and SMTP sender used for mail notifications. # The API URL to use for Slack notifications. # # The directory from which notification templates are read. templates: - '/etc/alertmanager-templates/*.tmpl' # The root route on which each incoming alert enters. route: # The labels by which incoming alerts are grouped together. For example, # multiple alerts coming in for cluster=A and alertname=LatencyHigh would # be batched into a single group. group_by: ['alertname', 'cluster', 'service'] # When a new group of alerts is created by an incoming alert, wait at # least 'group_wait' to send the initial notification. # This way ensures that you get multiple alerts for the same group that start # firing shortly after another are batched together on the first # notification. group_wait: 30s # When the first notification was sent, wait 'group_interval' to send a batch # of new alerts that started firing for that group. group_interval: 5m # If an alert has successfully been sent, wait 'repeat_interval' to # resend them. #repeat_interval: 1m repeat_interval: 1m # A default receiver # If an alert isn't caught by a route, send it to default. receiver: webhook_alert # All the above attributes are inherited by all child routes and can # overwritten on each. # The child route trees. routes: # Send severity=slack alerts to slack. - match: severity: info receiver: webhook_alert - match: severity: warning receiver: webhook_alert receivers: - name: webhook_alert webhook_configs: - url: 'http://192.168.88.26:8080/api/1.0/utils/alert/?token=ADW82115yn7YEWXCEW88WEW6' send_resolved: true deployment.yaml: 123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: alertmanager namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: alertmanager template: metadata: name: alertmanager labels: app: alertmanager spec: containers: - name: alertmanager image: quay.io/prometheus/alertmanager:v0.15.0 args: - '--config.file=/etc/alertmanager/config.yml' - '--storage.path=/alertmanager' ports: - name: alertmanager containerPort: 9093 volumeMounts: - name: config-volume mountPath: /etc/alertmanager - name: templates-volume mountPath: /etc/alertmanager-templates - name: alertmanager mountPath: /alertmanager serviceAccountName: prometheus volumes: - name: config-volume configMap: name: alertmanager - name: templates-volume configMap: name: alertmanager-templates - name: alertmanager emptyDir: &#123;&#125; service.yaml: 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' prometheus.io/path: '/metrics' labels: name: alertmanager name: alertmanager namespace: kube-systemspec: selector: app: alertmanager type: NodePort ports: - name: alertmanager protocol: TCP port: 9093 targetPort: 9093 依次部署上方几个yaml文件。在配置完alertmanager之后，需要给prometheus主程序配置文件添加alertmanager相关的配置，即修改prometheus的configmap，添加data.rules.yml，增加一些达到触发告警阈值的规则。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546rules.yml: | groups: - name: test-rule rules: - alert: NodeFilesystemUsage-high expr: (node_filesystem_size&#123;device="rootfs"&#125; - node_filesystem_free&#123;device="rootfs"&#125;) / node_filesystem_size&#123;device="rootfs"&#125; * 100 &gt; 80 for: 2m labels: team: node severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Filesystem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Filesystem usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeMemoryUsage-low expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 &gt; 1 for: 1m labels: team: node severity: info annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu&#123;job="kubernetes-node-exporter",mode="idle"&#125;[5m])) * 100)) &gt; 80 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: PodMemUsage expr: container_memory_usage_bytes&#123;pod_name!="",namespace="default"&#125;/container_spec_memory_limit_bytes&#123;pod_name!="",namespace="default"&#125; *100 != +Inf &gt; 10 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High Mem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod CPU Mem is above 80% (current value is: &#123;&#123; $value &#125;&#125;" 更新之后的prometheus的configmap完整yaml文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s rule_files: - /etc/prometheus/rules.yml alerting: alertmanagers: - static_configs: - targets: ["alertmanager:9093"] scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: 'kubernetes-node-exporter' scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_role] action: replace target_label: kubernetes_role - source_labels: [__address__] regex: '(.*):10250' replacement: '$&#123;1&#125;:31672' target_label: __address__ rules.yml: | groups: - name: test-rule rules: - alert: NodeFilesystemUsage-high expr: (node_filesystem_size&#123;device="rootfs"&#125; - node_filesystem_free&#123;device="rootfs"&#125;) / node_filesystem_size&#123;device="rootfs"&#125; * 100 &gt; 80 for: 2m labels: team: node severity: warning annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Filesystem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Filesystem usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeMemoryUsage-low expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 &gt; 1 for: 1m labels: team: node severity: info annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High Memory usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Memory usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu&#123;job="kubernetes-node-exporter",mode="idle"&#125;[5m])) * 100)) &gt; 80 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: High CPU usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: CPU usage is above 80% (current value is: &#123;&#123; $value &#125;&#125;" - alert: PodMemUsage expr: container_memory_usage_bytes&#123;pod_name!="",namespace="default"&#125;/container_spec_memory_limit_bytes&#123;pod_name!="",namespace="default"&#125; *100 != +Inf &gt; 80 for: 2m labels: team: node annotations: summary: "&#123;&#123;$labels.instance&#125;&#125;: Pod High Mem usage detected" description: "&#123;&#123;$labels.instance&#125;&#125;: Pod CPU Mem is above 80% (current value is: &#123;&#123; $value &#125;&#125;" 为了测试使用，增加了一项node内存使用率超过1%则触发告警，这个在使用时可以删掉。 kubectl apply -f config.map更新prometheus配置文件，更新配置文件后，prometheus不会自动重载配置，需重启pod，为了优雅地重载，可以采取更新deployment注解的方式来触发滚动升级，保证服务不中断。使用如下命令： 1kubectl patch deployment prometheus --patch '&#123;"spec": &#123;"template": &#123;"metadata": &#123;"annotations": &#123;"update-time": "2018-06-25 17:50" &#125;&#125;&#125;&#125;&#125;' -n kube-system 四、告警效果查看prometheus告警模块，可以看到激活的告警消息： 钉钉收到告警消息： 告警处于firing激活状态时，只有有效的start_time，endtime为无效值，状态为resolved时会有准确的end-time。 点击告警消息中的链接，查看明细数据： 总结配置告警消息并不难，难点在于各项键控值的定义，metrics类型有很多，同时支持的各类应用也有很多，还有很多可摸索的空间。后期还要不断的扩充、调整监控类型，调整优化通知策略，prometheus非常地灵活，可以基于此不断发掘，打造最适合自己公司环境的容器云监控体系。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s(七)、监控--Prometheus部署篇]]></title>
    <url>%2F2018%2F06%2F26%2Fk8s(%E4%B8%83)%E3%80%81%E7%9B%91%E6%8E%A7--Prometheus%E9%83%A8%E7%BD%B2%E7%AF%87.html</url>
    <content type="text"><![CDATA[## 前言前面几篇文章介绍了k8s的部署、对外服务、集群网络、微服务支持，在生产环境中使用，离不开运行状态监控，本篇开始部署使用prometheus，被各大公司广泛使用的容器监控工具。 工作方式Prometheus工作示意图： 在k8s中，关于集群的资源有metrics度量值的概念，有各种不同的exporter可以通过api接口对外提供各种度量值的及时数据，prometheus在与k8s融合工作的过程，就是通过与这些提供metric值得exporter进行交互，获取数据，整合数据，展示数据，触发告警的过程。一、获取metrics：1.对短暂生命周期的任务，采取拉的形式获取metrics (不常见)2.对于exporter提供的metrics，采取拉的方式获取metrics(通常方式),对接的exporter常见的有：kube-apiserver 、cadvisor、node-exporter，也可根据应用类型部署相应的exporter，获取该应用的状态信息，目前支持的应用有：nginx/haproxy/mysql/redis/memcache等。 二、数据汇总及按需获取：可以按照官方定义的expr表达式格式，以及PromQL语法对相应的指标进程过滤，数据展示及图形展示。不过自带的webui较为简陋，但prometheus同时提供获取数据的api，grafana可通过api获取prometheus数据源，来绘制更精细的图形效果用以展示。 expr书写格式及语法参考官方文档：https://prometheus.io/docs/prometheus/latest/querying/basics/ 三、告警推送prometheus支持多种告警媒介，对满足条件的告警自动触发告警，并可对告警的发送规则进行定制，例如重复间隔、路由等，可以实现非常灵活的告警触发。 部署1.配置configmap，在部署前将Prometheus主程序配置文件准备好，以configmap的形式挂载进deployment中。prometheus-configmap.yaml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: kube-systemdata: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s rule_files: - /etc/prometheus/rules.yml alerting: alertmanagers: - static_configs: - targets: ["alertmanager:9093"] scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: 'kubernetes-node-exporter' scheme: http tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - source_labels: [__meta_kubernetes_role] action: replace target_label: kubernetes_role - source_labels: [__address__] regex: '(.*):10250' replacement: '$&#123;1&#125;:31672' target_label: __address__ 2.部署prometheus工作主程序，注意挂载上面的configmap：prometheus.deploy.yml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748apiVersion: apps/v1beta2kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.0.0 name: prometheus command: - "/bin/prometheus" args: - "--config.file=/etc/prometheus/prometheus.yml" - "--storage.tsdb.path=/prometheus" - "--storage.tsdb.retention=24h" ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: "/prometheus" name: data - mountPath: "/etc/prometheus" name: config-volume resources: requests: cpu: 100m memory: 100Mi limits: cpu: 500m memory: 2500Mi serviceAccountName: prometheus volumes: - name: data emptyDir: &#123;&#125; - name: config-volume configMap: name: prometheus-config 3.部署svc、ingress、rbac授权。注意：在本地是使用traefik做对外服务代理的，因此修改了默认的NodePort的svc.type为ClusterIP的方式，添加ingress后，可以以域名方式直接访问。若不做代理，可以无需部署ingress，svc.type使用默认的NodePort。prometheus.svc.yaml: 123456789101112131415kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: kube-systemspec: type: ClusterIP ports: - port: 80 protocol: TCP targetPort: 9090 selector: app: prometheus prometheus.ing.yaml： 123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus namespace: kube-system selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/prometheusspec: rules: - host: prometheusv19.abc.com http: paths: - backend: serviceName: prometheus servicePort: 80 path: / rbac-setup.yaml： 1234567891011121314151617181920212223242526272829303132333435363738kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: ["get", "list", "watch"]- apiGroups: - extensions resources: - ingresses verbs: ["get", "list", "watch"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-system 依次部署上方几个yaml文件，待初始化完成后，配置好dns记录，即可打开浏览器访问：随便选取一个metric，点击execute，查看是否能正常获取结果输出。点击status—target，可以看到metrics的数据来源，即各exporter，点击相应exporter上的链接可查看这个exporter提供的metrics明细。 为了更好的展示图形效果，需要部署grafana，因此前已经部署有grafana，这里不再部署，贴一个all-in-one.yaml部署文件。grafana-all-in-one.yaml: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana-core namespace: kube-system labels: app: grafana component: corespec: replicas: 1 template: metadata: labels: app: grafana component: core spec: containers: - image: grafana/grafana:4.2.0 name: grafana-core imagePullPolicy: IfNotPresent # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ENABLED value: "false" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: "true" readinessProbe: httpGet: path: /login port: 3000 # initialDelaySeconds: 30 # timeoutSeconds: 1 volumeMounts: - name: grafana-persistent-storage mountPath: /var volumes: - name: grafana-persistent-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-system labels: app: grafana component: corespec: type: NodePort ports: - port: 3000 selector: app: grafana component: core 访问grafana，添加prometheus数据源：默认管理账号密码为admin admin选择资源类型，填入prometheus的服务地址及端口号，点击保存 导入展示模板：点击dashboard，点击import dashboard，在弹出框内填写数字315，会自动加载官方提供的315号模板，然后选择数据源为刚添加的数据源，模板就创建好了，非常easy。 基本部署到这里就结束了，下篇介绍一下prometheus的告警相关规则。 =========================================================================================== 7.19更新：最近发现，采用daemon-set方式部署的node-exporterc采集到的度量值不准确，最后发现需要将host的/proc和/sys目录挂载进node-exporter的容器内。更新后的node-exporter.yaml文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: node-exporter namespace: default labels: k8s-app: node-exporterspec: template: metadata: labels: k8s-app: node-exporter spec: containers: - image: prom/node-exporter name: node-exporter ports: - containerPort: 9100 protocol: TCP name: http hostPort: 9101 volumeMounts: - mountPath: /host/proc name: proc - mountPath: /host/sys name: sys args: - --path.procfs=/host/proc - --path.sysfs=/host/sys - --collector.filesystem.ignored-mount-points - "^/(dev|proc|sys|host|etc|rootfs|docker)($|/)" volumes: - hostPath: path: /proc name: proc - hostPath: path: /sys name: sys---apiVersion: v1kind: Servicemetadata: labels: k8s-app: node-exporter name: node-exporter namespace: defaultspec: ports: - name: http port: 9100 nodePort: 31672 protocol: TCP type```````````` NodePort selector: k8s-app: node-exporter123但是发现，部署完成之后，采集到的node指标依然不准确，非常奇怪，尝试脱离k8s使用docker方式直接部署，结果采集到的node数值就很准确了，有点不明白原因，后续继续排查一下。docker运行命令： docker run -d \ -p 9100:9100 \ –name node-exporter \ -v “/proc:/host/proc” \ -v “/sys:/host/sys” \ -v “/:/rootfs” \ –net=”host” \ prom/node-exporter:v0.14.0 \ -collector.procfs /host/proc \ -collector.sysfs /host/sys \ -collector.filesystem.ignored-mount-points “^/(sys|proc|dev|host|etc)($|/)”` 最后，记得修改configmap内的job相关targets配置。 为什么依附于k8s集群内采集的node指标就不准确，这个问题后续得好好研究，这次先到这里。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（六）、微服务框架istio服务可视化与监控]]></title>
    <url>%2F2018%2F06%2F06%2Fk8s%EF%BC%88%E5%85%AD%EF%BC%89%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6istio%E6%9C%8D%E5%8A%A1%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%9B%91%E6%8E%A7.html</url>
    <content type="text"><![CDATA[## 前言：承接上文k8s（五）、微服务框架istio流量策略控制，测试环境需保留上文中的环境，在本文将重点展示istio微服务调用链关系展示，以及微服务工作状态监测，涉及组件有：prometheus \ grafana \ jaeger等 非常感谢文章作者这几天耐心地解答疑惑：http://www.maogx.win/posts/26/ 微服务调用链追踪安装jaeger 1234567891011121314151617181920212223242526272829# 下载yml文件mkdir jaeger &amp;&amp; cd jaegerwget https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml# 实验环境不支持 LoadBalancer# 可以修改jaeger-all-in-one-template.yml使用nodeport# 也可以不修改，这样的会使用随机的nodeport# 启动kubectl apply -n istio-system -f jaeger-all-in-one-template.yml# 查看kubectl get pods -n istio-systemkubectl get svc -n istio-system# 多次访问之前的vue react界面并点击发射按钮# 访问jaegerNodePort=$(kubectl get svc -n istio-system | grep jaeger-query | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$jaegerNodePort# 浏览器访问http://$nodeIP:"$jaegerNodePort# 选择 istio-ingress 可以方便查看整个调用链# 清理cd jaegerkubectl delete -n istio-system -f jaeger-all-in-one-template.yml 结果展示：在主页可以查看每次调用trace到的svc间的交互细节明细分析，包括每一跳svc的耗时等。 监控指标收集： 1234567891011121314151617181920212223242526272829303132333435363738# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yaml## 配置收集 #cd /root/istio-test：istioctl create -f istio/new_telemetry.yml #志信息istioctl create -f istio/tcp_telemetry.yml #tcp信息收集 #cd /usr/local/istio：kubectl apply -f install/kubernetes/addons/grafana.yaml #grafana可视化监控部署kubectl apply -f install/kubernetes/addons/servicegraph.yaml #服务树展示部署 访问测试，提供数据： 1234567# 访问web测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 加大压力测试kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -t 100s -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -t 100s -loglevel Warning http://service-go/env 查看效果：grafana： serviceTree： 故障注入，模拟服务故障像上一节一样，将50%的包注入504故障： 1234567#注入故障istioctl create -f istio/route-rule-python-timeout.ymlistioctl create -f istio/route-rule-go-delay.yml#再次压测kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 200 -loglevel Warning http://service-python/env# 前端同时多次点击‘发射’ 查看效果grafana： jaeger： 说明：1.在grafana右下角的趋势图中可以看到service-python出现了大量5xx代码2.在jaeger的主页可以看到有Error的追踪，点击一个进去可以查看追踪明细（图2），可以看到在service-python段捕获到了Error code的包，其余调用链段均正常。利用以上调用链分析工具及监控图表，可以快速识别微服务整个调用链中某一跳或者局部故障，便于分布式微服务架构的故障排查，同时监控图标中也有针对服务每一跳处理时间的分析，可以帮助开发人员进行针对性优化。 注意事项写在最后，要实现服务链调用关系展示，还是得对代码进行微改，并不能百分百无入侵代码。要求在调用集群内svc的http api接口时，给http请求头部添加一些字段， 官网样例截图（python）： 在这里测试的例子中，python服务实例使用的是flask框架，给请求头添加字段代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from apistar import App, Route, httpimport platformimport requestsimport loggingdef getForwardHeaders(request): headers = &#123;&#125; incoming_headers = [ 'x-request-id', 'x-b3-traceid', 'x-b3-spanid', 'x-b3-parentspanid', 'x-b3-sampled', 'x-b3-flags', 'x-ot-span-context' ] for ihdr in incoming_headers: val = request.headers.get(ihdr) if val is not None: headers[ihdr] = val # print("incoming: "+ihdr+":"+val) return headersdef env(request: http.Request): forwardHeaders = getForwardHeaders(request) service_lua_url = 'http://' + 'service-lua' + '/env' resp = requests.get(service_lua_url, headers=forwardHeaders) data_lua = resp.json() service_node_url = 'http://' + 'service-node' + '/env' resp = requests.get(service_node_url, headers=forwardHeaders) data_node = resp.json() return &#123; "message": 'Python' + platform.python_version() + '-----&gt;' + data_lua['message'] + ', ' + data_node['message'] &#125;def status(): return 'ok'routes = [ Route('/env', method='GET', handler=env), Route('/status', method='GET', handler=status),]app = App(routes=routes)logging.basicConfig(level=logging.DEBUG)if __name__ == '__main__': app.serve('0.0.0.0', 80, debug=True) 官方说明：https://istio.io/docs/tasks/telemetry/distributed-tracing/]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（五）、微服务框架istio流量策略控制]]></title>
    <url>%2F2018%2F06%2F06%2Fk8s%EF%BC%88%E4%BA%94%EF%BC%89%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6istio%E6%B5%81%E9%87%8F%E7%AD%96%E7%95%A5%E6%8E%A7%E5%88%B6.html</url>
    <content type="text"><![CDATA[## 前言承接上节k8s（四）、微服务框架istio安装测试本节实验通过在k8s上部署istio，实现微服务的基础功能。其中会涉及到服务的限流，超时，熔断，降级，流量分隔，A/B测试等功能。注意本次实验环境已开启istio的自动注入功能。 1234567891011121314151617181920212223**本实验的服务问调用关系如下：**实验采用时下流行的前后端分离模式，每个svc分别对应两个不同语言版本实例的后端pod前端项目基于vue/react实现前端调用python实现的API接口python服务调用后端node实现的服务和lua实现的服务node服务调用go实现的服务—-&gt;service-js—-&gt;service-python—-&gt;service-lua—-&gt;service-node—-&gt;service-go本实验使用的语言技术栈：vue/reactpython2/3node8/10openresty1.11 /1.13go1.10/1.9 数据流调用关系逻辑图如下： 下载实验仓库 1git clone https://github.com/yinwenqin/istio-test.git 部署服务 1234567891011cd istio-testkubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 暴露服务 12345# 使用istio提供的ingress功能# 暴露js和python服务让k8s集群外部访问kubectl apply -f istio/ingress-python.ymlkubectl apply -f istio/ingress-js.yml# 这两个ingress文件里指定的host均为:istio-test.will，按照路径区分后端对应svc 查看1kubectl get ingress 测试访问1234567###配置hosts解析#查询iostio-ingress的svc clusterIP#kubectl get svc -n istio-system | grep istio-ingress | awk '&#123;print $3&#125;' 10.100.0.25 # 10.100.0.25为istio-ingress 的 ip，给ubuntu和windows分别配置dns条目：10.100.0.25 istio-test.will 1234# ubuntu使用curlcurl -I istio-test.willcurl -I istio-test.will/envcurl -s istio-test.will | egrep "vue|React" 1234#windows打开浏览器测试，有一个前提就是测试的windows机器到集群网段的路由要打通，否则无法直接访问集群网段IP。# 此时如果作用浏览器，可能会出会页面显示不正常的情况。# 因为此时请求会轮流分发到后端js服务的v1/v2版本，因此css与js对应关系可能不正确，所以不能正常加载页面# 点击前端的‘发射’按钮，触发对后端的所有服务的调用 多次测试，可以发现，测试的结果各版本之间来回转换。 流量调度使用routeRule路由规则，根据请求的信息，把流量路由到服务的不同版本的pod上。实验过程如果没有达到预期效果，很有可能是因为存在路由规则冲突，而且没有设置优先级，可以先删除之前设置的路由规则或者把优先级设置高一点。 把所有流量导向v1版本123456789101112131415161718# 创建路由规则istioctl create -f istio/route-rule-all-v1.yml# 查看路由规则istioctl get routerule# 访问浏览器测试http://istio-test.will/# 此时你会看到react app的界面# 点击发射按钮，会发送ajax请求到python服务# 由于把所有流量都导向了v1版本# 多次点击发射按钮会得到一样的内容# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# 清除路由规则istioctl delete -f istio/route-rule-all-v1.yml 根据请求把流量导向不同版本（A/B测试） 123456789101112131415# 创建路由规则# 根据客户端浏览器的不同返回不同内容，通过match请求头的agent字段实现istioctl create -f istio/route-rule-js-by-agent.yml# 使用访问浏览器# 如果你用chrome浏览器你会看到react app的界面# 如果你用firefox浏览器你会看到vue app的界面# 多次点击发射按钮，会获取到不同的内容# 根据前端app不同使用不同版本的python服务，通过请求头match来实现istioctl create -f istio/route-rule-python-by-header.yml# 此步骤创建的第一个路由规则保留不删除，为下面做实验提供方便istioctl delete -f istio/route-rule-python-by-header.yml 根据源服务把流量导向不同版本目的服务 1234567891011# 创建路由规则istioctl create -f istio/route-rule-go-by-source.yml# 此时规则如下# 所有chrome浏览器都走v1版本服务# 所有firefox浏览器都走v2版本服务# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# vue-----&gt;Python3.6.5-----&gt;Gogo1.10.2# 清除路由规则istioctl delete -f istio/route-rule-go-by-source.yml 指定权重进行流量分隔 123456789# 指定权重把流量分隔# 25%流量路由到v1版本# 75%流量路由到v2版本# 创建路由规则istioctl create -f istio/route-rule-go-v1-v2.yaml# 清除路由规则istioctl delete -f istio/route-rule-go-v1-v2.yaml 集群内访问公开服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 默认情况下，启用了istio的服务是无法访问外部url的# ！！！一定要注意，pod注入了sidecar(istio-proxy)服务之后，pod若想访问集群外的域名，必须使用EgressRule。此拦截# 只对http(80)和https(443)协议有效# egress同样支持设置路由规则...一个简单的egress示例： # egress 访问外部服务 apiVersion: config.istio.io/v1alpha2 kind: EgressRule metadata: name: httpbin-egress-rule spec: destination: #service后写域名，可以写多条 service: httpbin.org service: www.baidu.com ports: - port: 80 protocol: http #create之前：root@yksv001238:~/istio-test# kubectl exec -it sleep-776b7bcdcd-ssqkn sh# curl -I -m 10 -o /dev/null -s -w %&#123;http_code&#125; www.baidu.com404#create之后root@yksv001238:~/istio-test# kubectl exec -it sleep-776b7bcdcd-ssqkn sh# curl -I -m 10 -o /dev/null -s -w %&#123;http_code&#125; www.baidu.com200...# httpistioctl create -f istio/egress-rule-http-bin.yml# tcpistioctl create -f istio/egress-rule-tcp-wikipedia.yml# 查看istioctl get egressrule# 测试# 进入pod进行测试kubectl apply -f istio/sleep.yamlkubectl get podsexport SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath=&#123;.items..metadata.name&#125;)kubectl exec -it $SOURCE_POD -c sleep bash# http测试curl http://httpbin.org/headers# tcp测试curl -o /dev/null -s -w "%&#123;http_code&#125;\n" https://www.wikipedia.orgcurl -s https://en.wikipedia.org/wiki/Main_Page | grep articlecount | grep 'Special:Statistics'# 清理istioctl delete -f istio/egress-rule-http-bin.ymlistioctl delete -f istio/egress-rule-tcp-wikipedia.ymlkubectl delete -f istio/sleep.yaml 故障管理● 调用超时设置和重试设置● 故障注入，模拟服务故障 设置超时时间与模拟服务超时故障1234567891011121314151617# 设置python服务超时时间istioctl create -f istio/route-rule-python-timeout.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-delay.yml#这两条规则的定义是： 1.对响应时间超过1s的包则定义为请求超时 2.对请求的包50%比例设置添加2s的延迟 因此实现效果：# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到平均50%的请求会返回504超时# 清除路由规则istioctl delete -f istio/route-rule-python-timeout.ymlistioctl delete -f istio/route-rule-go-delay.yml 测试结果图： 设置重试与模拟服务500故障 123456789101112131415161718# 设置python服务超时时间istioctl create -f istio/route-rule-python-retry.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-abort.yml#这两条规则的定义是： 1.请求失败则重试3次 2.将75%的包响应状态码设置为500 因此实现效果：# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 75/3，约25%的请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-python-retry.ymlistioctl delete -f istio/route-rule-go-abort.yml 测试结果图： 超时和服务故障模拟配合使用 1234567891011# 所有请求延迟5秒钟，然后失败其中的10％... route: - labels: version: v1 httpFault: delay: fixedDelay: 5s abort: percent: 10 httpStatus: 400 熔断器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 熔断器规则需要应用到路由规则上# 需要先配置至少一个路由规则# 设置路由规则，为演示效果，将原文件中的并发数量限制从1修改为10istioctl create -f istio/route-rule-go-default.yml# 设置熔断规则istioctl create -f istio/route-rule-go-cb.yml# 查看规则istioctl get destinationpolicy# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject --debug -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 测试 10并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -n 20 -loglevel Warning http://service-go/env# 结果 # target 50% 0.013 # target 75% 0.0225 # target 90% 0.036607 # target 99% 0.0394997 # target 99.9% 0.039789 Sockets used: 10 (for perfect keepalive, would be 10) Code 200 : 20 (100.0 %)# 测试熔断 20并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 20 -qps 0 -n 20 -loglevel Warning http://service-go/env# 结果 # target 50% 0.016 # target 75% 0.026 # target 90% 0.029 # target 99% 0.0335416 # target 99.9% 0.0343385 Sockets used: 20 (for perfect keepalive, would be 20) Code 200 : 11 (55.0 %) Code 503 : 9 (45.0 %)# 增加并发会看到失败的请求占比增高# 查看状态,指定pod中的istio-proxy容器，通过其api接口可以获取请求的包的状态。# upstream_rq_pending_overflow 表示被熔断的请求数kubectl exec -it $FORTIO_POD -c istio-proxy -- sh -c 'curl localhost:15000/stats' | grep service-go | grep pending#结果 cluster.out.service-go.default.svc.cluster.local|http|version=v1.upstream_rq_pending_active: 0 cluster.out.service-go.default.svc.cluster.local|http|version=v1.upstream_rq_pending_failure_eject: 0 cluster.out.service-go.default.svc.cluster.local|http|version=v1.upstream_rq_pending_overflow: 9 cluster.out.service-go.default.svc.cluster.local|http|version=v1.upstream_rq_pending_total: 34# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-cb.yml 限流动态设置服务qps 123456789101112131415161718192021222324252627282930313233# 创建service-python默认路由# 经测试，一定要配置路由规则，否则无法完成限流# 所以极有可能限流是配置在路由规则上的# 在路由时进行限流统计istioctl create -f istio/route-rule-python-default.yml# 配置一个速率限制的memquota适配器# 默认设置500qpsistioctl create -f istio/ratelimit-handler.yaml# 配置速率限制实例和规则istioctl create -f istio/ratelimit-rule-service-go.yaml# 查看kubectl get memquota -n istio-systemkubectl get quota -n istio-systemkubectl get rule -n istio-systemkubectl get quotaspec -n istio-systemkubectl get quotaspecbinding -n istio-system# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 测试# 会出现部分请求不正常# python 返回 code 500# go 返回 code 429kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -n 100 -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 100 -loglevel Warning http://service-go/env 流量镜像使用镜像服务，复制某服务的流量到另一服务，一般用于线上新上服务的测试，将所有原有服务接收到的流量mirror一份打往新版本的服务进行测试。 12345678910111213141516171819202122232425262728# 创建默认策略# 默认所有流量路由到v1istioctl create -f istio/route-rule-go-default.yml# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 查看v1的志kubectl logs -f $(kubectl get pods | grep service-go-v1 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 查看v2的志# 再开一个终端查看志kubectl logs -f $(kubectl get pods | grep service-go-v2 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 创建镜像规则istioctl create -f istio/route-rule-go-mirror.yml# 测试多次访问kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -t 10s -loglevel Warning http://service-go/env# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-mirror.yml 清理 12345678910111213141516# 删除相关deploy和svckubectl delete -f service/go/v1/go-v1.ymlkubectl delete -f service/go/v2/go-v2.ymlkubectl delete -f service/python/v1/python-v1.ymlkubectl delete -f service/python/v2/python-v2.ymlkubectl delete -f service/js/v1/js-v1.ymlkubectl delete -f service/js/v2/js-v2.ymlkubectl delete -f service/node/v1/node-v1.ymlkubectl delete -f service/node/v2/node-v2.ymlkubectl delete -f service/lua/v1/lua-v1.ymlkubectl delete -f service/lua/v2/lua-v2.yml# 清除路由规则kubectl delete -f istio/ingress-python.ymlkubectl delete -f istio/ingress-js.ymlistioctl delete routerule $(istioctl get routerule | grep RouteRule | awk '&#123;print $1&#125;') 参考文档http://istio.doczh.cnhttps://istio.io/docshttps://istio.io/docs/reference/config/istio.networking.v1alpha3.htmlhttps://istio.io/docs/reference/config/istio.routing.v1alpha1.html]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（四）、微服务框架istio安装测试]]></title>
    <url>%2F2018%2F06%2F06%2Fk8s%EF%BC%88%E5%9B%9B%EF%BC%89%E3%80%81%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6istio%E5%AE%89%E8%A3%85%E6%B5%8B%E8%AF%95.html</url>
    <content type="text"><![CDATA[## 一、前言Istio是Google/IBM/Lyft联合开发的开源项目，2017/5/发布第一个release 0.1.0，Service Mesh微服务新秀，采用sidecar的实现方式，有着高性能、极低的资源开销、无代码入侵等优秀特性，扛起微服务大旗，并与当下火热的k8s Paas容器云平台深度整合。官方描述为:1An open platform to connect, manage, and secure microservices Istio提供一种简单的方式来建立已部署的服务的网络，具备负载均衡，服务到服务认证，监控等等功能，与k8s深度整合。各组件工作图： 控制层面组件：1.Pilot：负责将调度的配置下发到各svc endpoint后pod中注入的Envoys2.Mixer：负责流量策略等统一调度3.Citadel：负责安全策略，如TLS证书颁发 数据层面组件：Envroys：将envroys proxy容器，以sidecar的方式注入到svc后端的pod中，与控制层面组件交互。一图助你理解sidecar工作方式（即：边三轮~）： 非常感谢：http://www.maogx.win/posts/25/ 二、部署安装1.部署前准备这里使用较新的istio 0.7版本（目前最新为0.8），要求k8s版本1.9以上，同时对k8s有以下要求12345678910# k8s 1.9 版本以后才能使用# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# kubeadm默认安装的k8s 1.9版本，kube-apiserver启动参数 --admission-control 里没有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook#需要修改vim /etc/kubernetes/manifests/kube-apiserver.yaml,给--admission-control 添加两个参数 MutatingAdmissionWebhook,ValidatingAdmissionWebhook#添加完成保存退出后重启kubelet服务 2.安装istio1234567891011121314151617181920212223242526272829303132333435363738394041424344wget https://github.com/istio/istio/releases/download/0.7.1/istio-0.7.1-linux.tar.gztar xf istio-0.7.1-linux.tar.gz# 安装配置环境变量mv istio-0.7.1 /usr/local/ln -sv /usr/local/istio-0.7.1 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' &gt; /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version# 如果环境不是云环境，不支持LoadBalancer# 在公司生产环境中ingress-controller使用的是traefik，因此我这里是以ClusterIP的方式部署的istio-ingress service# 若没有其他的ingress，可以直接使用istio-ingress，修改 Istio-ingress service type使用 NodePort# 大概在1548-1590行左右cd /usr/local/istiocp install/kubernetes/istio.yaml install/kubernetes/istio.yaml.orivim install/kubernetes/istio.yaml...apiVersion: v1kind: Servicemetadata: name: istio-ingress namespace: istio-system labels: istio: ingressspec: # type: LoadBlance type: ClusterIP ports: - port: 80# nodePort: 32000 name: http - port: 443 name: https selector: istio: ingress...# 安装不使用认证（不使用tls）kubectl apply -f install/kubernetes/istio.yaml 3.启用自动注入sidecar 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#注意上方讲的钩子一定给kube-apiserver要配置上# 生成所需要的证书./install/kubernetes/webhook-create-signed-cert.sh \ --service istio-sidecar-injector \ --namespace istio-system \ --secret sidecar-injector-certs # 创建配置configmapkubectl apply -f install/kubernetes/istio-sidecar-injector-configmap-release.yaml# 生成相关yamlcat install/kubernetes/istio-sidecar-injector.yaml | \ ./install/kubernetes/webhook-patch-ca-bundle.sh &gt; \ install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml # 安装webhookkubectl apply -f install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml# 查看kubectl -n istio-system get deployment -listio=sidecar-injectorkubectl get namespace -L istio-injection# 开启注入前，创建测试用例kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看描述，看看创建pod过程中是否有istio-proxy容器（即sidecar）kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭default命名空间自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 若不开启针对命名空间的自动注入，只想对部分部署使用自动注入，可以使用： 12#不开启自动注入部署应用需要使用如下方式的命令，即给原有的部署yaml文件内容器部分添加一个istio-proxy容器kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 4.访问测试 123GATEWAY_URL=$(kubectl get po -l istio=ingress -n istio-system -o 'jsonpath=&#123;.items[0].status.hostIP&#125;'):$(kubectl get svc istio-ingress -n istio-system -o 'jsonpath=&#123;.spec.ports[0].nodePort&#125;')curl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage 5.清理：k8s创建和清理部署应用非常简单，创建使用apply/create，清理使用delete，文件指向部署yaml即可。 123456# 清理官方用例samples/bookinfo/kube/cleanup.sh# 清理istiokubectl delete -f install/kubernetes/istio.yaml# kubectl delete -f install/kubernetes/istio-auth.yaml]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（三）、kube-router: BGP直通连接网络]]></title>
    <url>%2F2018%2F05%2F13%2Fk8s%EF%BC%88%E4%B8%89%EF%BC%89%E3%80%81kube-router%E6%8F%90%E4%BE%9BBGP%E5%85%A8%E7%9B%B4%E9%80%9A%E7%BD%91%E7%BB%9C.html</url>
    <content type="text"><![CDATA[## 前言上一篇讲到，k8s使用traefik代理集群内部服务，灵活地注入代理配置的方式，提供对外服务（参考：k8s（二）、对外服务）。在本篇，使用gns3思科模拟器IOU，搭建BGP网络环境，通过vmware桥接的方式，直接对接虚拟机内的k8s集群，与集群内部BGP网络实现互通。 拓扑图： 1.路由器配置- 123456789101112131415161718192021222324252627282930####router1：IOU1#show running-config interface Ethernet0/0 ip address 192.168.0.101 255.255.255.0 vrrp 1 ip 192.168.0.100router bgp 64513 bgp log-neighbor-changes neighbor 192.168.0.102 remote-as 64513 neighbor 192.168.0.170 remote-as 64512 neighbor 192.168.0.171 remote-as 64512 maximum-paths 2 no auto-summary###router2：IOU2#show running-config interface Ethernet0/0 ip address 192.168.0.102 255.255.255.0 vrrp 1 ip 192.168.0.100 vrrp 1 priority 100 router bgp 64513 bgp log-neighbor-changes neighbor 192.168.0.101 remote-as 64513 neighbor 192.168.0.170 remote-as 64512 neighbor 192.168.0.171 remote-as 64512 maximum-paths 2 no auto-summary 2.kube-route配置：k8s集群搭建步骤见k8s（一）、 1.9.0高可用集群本地离线部署记录在此前配置好的kube-route的yaml配置文件中，需添加如下几项参数，分别指定本地bgp as，对等体as、ip等。 12345- --advertise-cluster-ip=true #宣告集群IP- --advertise-external-ip=true #宣告svc外部ip，如果svc指定了external-ip则生效- --cluster-asn=64512 #指定本地集群bgp as号- --peer-router-ips=192.168.0.100 #指定对等体ip，这里可以写多个，以','隔开，本次实验路由器配置了vrrp，指定vip即可- --peer-router-asns=64513 #对等体as号 完整的kube-router.yaml文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170apiVersion: v1kind: ConfigMapmetadata: name: kube-router-cfg namespace: kube-system labels: tier: node k8s-app: kube-routerdata: cni-conf.json: | &#123; "name":"kubernetes", "type":"bridge", "bridge":"kube-bridge", "isDefaultGateway":true, "ipam": &#123; "type":"host-local" &#125; &#125;---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-systemspec: template: metadata: labels: k8s-app: kube-router tier: node annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: kube-router serviceAccount: kube-router containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: - --run-router=true - --run-firewall=true - --run-service-proxy=true - --advertise-cluster-ip=true - --advertise-external-ip=true - --cluster-asn=64512 - --peer-router-ips=192.168.0.100 - --peer-router-asns=64513 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName livenessProbe: httpGet: path: /healthz port: 20244 initialDelaySeconds: 10 periodSeconds: 3 resources: requests: cpu: 250m memory: 250Mi securityContext: privileged: true volumeMounts: - name: lib-modules mountPath: /lib/modules readOnly: true - name: cni-conf-dir mountPath: /etc/cni/net.d - name: kubeconfig mountPath: /var/lib/kube-router/kubeconfig readOnly: true initContainers: - name: install-cni image: busybox imagePullPolicy: Always command: - /bin/sh - -c - set -e -x; if [ ! -f /etc/cni/net.d/10-kuberouter.conf ]; then TMP=/etc/cni/net.d/.tmp-kuberouter-cfg; cp /etc/kube-router/cni-conf.json $&#123;TMP&#125;; mv $&#123;TMP&#125; /etc/cni/net.d/10-kuberouter.conf; fi volumeMounts: - mountPath: /etc/cni/net.d name: cni-conf-dir - mountPath: /etc/kube-router name: kube-router-cfg hostNetwork: true tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master operator: Exists volumes: - name: lib-modules hostPath: path: /lib/modules - name: cni-conf-dir hostPath: path: /etc/cni/net.d - name: kube-router-cfg configMap: name: kube-router-cfg - name: kubeconfig hostPath: path: /var/lib/kube-router/kubeconfig---apiVersion: v1kind: ServiceAccountmetadata: name: kube-router namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kube-router namespace: kube-systemrules: - apiGroups: - "" resources: - namespaces - pods - services - nodes - endpoints verbs: - list - get - watch - apiGroups: - "networking.k8s.io" resources: - networkpolicies verbs: - list - get - watch - apiGroups: - extensions resources: - networkpolicies verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kube-routerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-routersubjects:- kind: ServiceAccount name: kube-router namespace: kube-system 直接删除此前的kube-router配置，重新create这个yaml文件内的资源： 12kubectl delete -f kubeadm-kuberouter.yaml #更新配置参数前执行kubectl create -f kubeadm-kuberouter.yaml #更新配置参数后执行 接着部署一个测试用得nginx的pod实例，yaml文件如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@171 nginx]# cat nginx-deploy.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: "2" creationTimestamp: 2018-04-09T04:02:02Z generation: 4 labels: app: nginx name: nginx-deploy namespace: default resourceVersion: "111504" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/nginx-deploy uid: c28090c0-3baa-11e8-b75a-000c29858eabspec: replicas: 1 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx:1.9.1 name: nginx ports: - containerPort: 80 protocol: TCP resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30 查看podIP，本地curl测试： 123456789101112131415161718192021222324252627282930[root@171 nginx]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODEnginx-deploy-5964dfd755-lv2kb 1/1 Running 0 29m 10.244.1.35 171[root@171 nginx]# curl 10.244.1.35&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;[root@171 nginx]# 3.检验结果路由器查看BGP邻居：123456789101112131415161718IOU1#show ip bgp summary BGP router identifier 192.168.0.101, local AS number 64513BGP table version is 19, main routing table version 194 network entries using 560 bytes of memory6 path entries using 480 bytes of memory2 multipath network entries and 4 multipath paths1/1 BGP path/bestpath attribute entries using 144 bytes of memory1 BGP AS-PATH entries using 24 bytes of memory0 BGP route-map cache entries using 0 bytes of memory0 BGP filter-list cache entries using 0 bytes of memoryBGP using 1208 total bytes of memoryBGP activity 5/1 prefixes, 12/6 paths, scan interval 60 secsNeighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd192.168.0.102 4 64513 100 106 19 0 0 01:27:20 0192.168.0.170 4 64512 107 86 19 0 0 00:37:19 3192.168.0.171 4 64512 98 85 19 0 0 00:37:21 3IOU1# 路由器查看BGP路由条目： 1234567891011121314151617181920IOU1#show ip route bgpCodes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2 E1 - OSPF external type 1, E2 - OSPF external type 2 i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2 ia - IS-IS inter area, * - candidate default, U - per-user static route o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP a - application route + - replicated route, % - next hop overrideGateway of last resort is 192.168.0.1 to network 0.0.0.0 10.0.0.0/8 is variably subnetted, 4 subnets, 2 masksB 10.96.0.1/32 [20/0] via 192.168.0.171, 00:38:36 [20/0] via 192.168.0.170, 00:38:36B 10.96.0.10/32 [20/0] via 192.168.0.171, 00:38:36 [20/0] via 192.168.0.170, 00:38:36B 10.244.0.0/24 [20/0] via 192.168.0.170, 00:38:36B 10.244.1.0/24 [20/0] via 192.168.0.171, 00:38:39 可以看到，bgp邻居建立成功，k8s集群内部路由学习成功 开一台测试机检查：测试机修改默认网关指向路由器，模拟外部网络环境中的一台普通pc： 123456789101112131415161718192021222324252627282930[root@python ~]# ip routedefault via 192.168.0.100 dev eth3 proto static [root@python ~]# curl 10.244.1.35&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;[root@python ~]# 测试成功，至此，集群内部bgp网络，和外部bgp网络对接成功！ 友情提示：别忘了保存路由器的配置到nvram里（copy running-config startup-config），否则重启就丢配置啦。好久没碰网络设备了，这个茬给忘了，被坑了一次，嘿嘿 9-22踩坑今日尝试在k8s集群中添加与原集群node(192.168.9.x/24)不在同一个网段的新node(192.168.20.x/24),创建好了之后出现了非常奇怪的现象:新node中的kube-router与集群外的网络核心设备之间建立的peer邻居关系一直重复地处于established —&gt; idle状态频繁转变,非常的不稳定,当处于established状态时,新node与原node间的丢包率甚至达到70%以上，处于idle状态时,node间ping包正常问题截图: 在进入kube-router后使用gobgp neighbor查看发现新node与外部网络设备ebgp\原node的ibgp邻居关系一直处于频繁变化的状态无法稳定建立关系，百思不得其解，最后终于在github上找到了kube-router唯一的类似issue，项目成员解释如下：Github issue链接 个人理解是:kube-router只支持与同一个子网的node建立ibgp对等体关系，跨子网的节点无法建立对等体邻居关系；同时，ebgp协议的ebgp max hop属性，默认值为1，路由器设备一般支持手动修改此值，而kube-router较早之前的版本仅支持默认值1，无法手动配置此值，因此EBGP邻居与kube-router也必须在同一个子网中，后面的版本已解决此问题，升级版本后node跨子网建ebgp邻居不再有问题。]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes中的Pause容器]]></title>
    <url>%2F2018%2F05%2F09%2FKubernetes%E4%B8%AD%E7%9A%84Pause%E5%AE%B9%E5%99%A8.html</url>
    <content type="text"><![CDATA[Kubernetes中的Pause容器已知容器技术利用linux提供的如下六种命名空间隔离技术来提供相应级别的资源隔离: 12345678910111213141.UTS(UNIX Time-sharing system) 允许创建独立的hostname/domian_name namespace，作为独立的节点2.IPC(Inter Process Communication) 创建隔离的进程间通信方式，实现包括信号量、消息队列、管道、共享内存等资源的namespace隔离3.PID(Process ID) 允许树的子叶节点namespace创建自己专属的pid区间，pid对同级节点无效，仅父级namespace节点可见。(在子节点的pid，会在父节点映射为另一个pid标志， 因此在父节点的角度，每个子节点的进程拥有两个pid号)4.mount 挂载点namespace，各个namespace默认的mount挂载只在本空间内生效，不影响其他namespace，也可使用share共享挂载的方式互相影响， 或者slave从属挂 载的方式，实现主影响从。5.network 每个net ns拥有独立的网络资源(包括网络设备,IPv4/IPv6协议栈，防火墙，/proc/net、/sys/class/net目录等)6.user 每个user ns拥有独立的id、属性、key、根目录，组织结构等 且我们知道容器之间命令空间是可以共享的: 12- docker run --net=container:webtest -itd IMAGE #运行一个容器，此容器与名为webtest的容器共享同一个网络命名空间- docker run -it --pid=host IMAGE ps aux # 运行一个容器，其具有host宿主机级别的pid命名空间 所谓共享命名空间，其实是继承了指定容器或主机的命名空间(父级)。 有了这一层认知，那么对于pod中的pause容器的理解便会很容易。 Pause容器关于pause容器的共，网上盛传的一幅图为: 这里基于此图再展开来说明一番。 在k8s的管理视角中pod是一个最基本的调度单位，但是如果在下层docker引擎的角度来查看，你会发现每个pod中至少还包含了一个额外的pause容器: 1234567root@host001:~# kubectl get pods -o wide --all-namespaces | grep deptest11devdefault deptest11dev-6fd6c96d9c-r7fld 1/1 Running 0 15s 172.26.9.55 host003[root@host003 ~]# docker ps | grep deptest53edc1df3c13 centos "/bin/bash -c /run.sh" 12 minutes ago Up 12 minutes k8s_deptest11dev_deptest11dev-6fd6c96d9c-r7fld_default_98522401-f47f-11e9-8d5e-3440b5a2bb9c_0e6cfffadffdb gcr.io/google_containers/pause-amd64:3.0 "/pause" 12 minutes ago Up 12 minutes k8s_POD_deptest11dev-6fd6c96d9c-r7fld_default_98522401-f47f-11e9-8d5e-3440b5a2bb9c_0 可以理解为，pause容器是服务容器的基础设施容器，pause容器创建了父级的IPC、PID、network命名空间，pod之中的所有容器，它们的这3种命名空间全部继承自父级pause容器，它们通过pause容器开辟的空间实现彼此共享进程表、彼此之间localhost方式的通信，kubelet还可以通过pause容器收集pod的运行状态相关信息，大有一番先有盘古后有天的感觉~ 但实际上，在后面的kubernetes版本之中，将pod的PID这一项命名空间共享移除了，这么做的原因是kubernetes认为服务容器本身应当是单服务模式，产生的进程都是自身可控的，进程管理和回收应该由服务容器自身的init进程进行管理而不是由pause容器来托管。在如今的kubernetes pod中，PID命名空间已不在pod内部共享，下面从docker容器的角度来看下pause容器和服务容器之间的联系。 验证Pause容器短名是：e6cfffadffdb 12[root@host003 ~]# docker ps | grep deptest | grep pause | awk '&#123;print $1&#125;'e6cfffadffdb 可以看出服务容器的IPC和network这两个命名空间确实继承自pause容器： 123456[root@host003 ~]:~# docker inspect a0807a2d0f65 | jq '.[0] | .HostConfig' | grep Mode "NetworkMode":"container:e6cfffadffdbc631b589dd3c6712fc6d10a8356b9b39bd611160aaa600eafe67", "IpcMode": "container:e6cfffadffdbc631b589dd3c6712fc6d10a8356b9b39bd611160aaa600eafe67", "PidMode": "", "UTSMode": "", "UsernsMode": "",]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes踩坑(一): 部署问题记录]]></title>
    <url>%2F2018%2F04%2F27%2FKubernetes%E8%B8%A9%E5%9D%91(%E4%B8%80)_%20%E9%83%A8%E7%BD%B2%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95.html</url>
    <content type="text"><![CDATA[## 一、etcd服务启动后报错etcd cluster ID mismatch: 检车service配置cluster选项有无问题，若无问题，则可能是此前的etcd bootstrap加速启动缓存残留导致，坑爹的是rm -rf /var/lib/etcd/* 删除完了之后还是报错，必须 rm -rf /var/lib/etcd/才能彻底清除，删除完成后记得再创建该路径mkdir /var/lib/etcd，否则会有类似报错： 1etcd.service: Failed at step CHDIR spawning /usr/local/bin/etcd: No such file or directory 二、etcd服务启动报错： 1234/var/log/syslog:member ece8752232f7b4d6 has already been bootstrapped原因是非主节点的etcd服务是启动状态，解决办法：rm -rf /var/lib/etcd/mkdir /var/lib/etcd 三、其余节点（非第一台master）初始化完成后kubectl获取资源提示X509: 12# kubectl get nodesUnable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes") 解决方法：kubeadm init初始化完成后提示的操作执行一遍： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 第一个master节点执行完成后，需将scp /etc/kubernetes/pki/*到其他节点后，其他master节点才能初始化。在kubeadm初始化之前要保证systemctl status kubelet 查看到的状态保持activating (auto-restart)状态，否则kubeadm会报错 四、集群部署完成后，kubectl执行写资源没有权限，报错类似： 123456'''Error from server (Forbidden): error when creating "kubeadm-router.yaml": daemonsets.extensions is forbidden: User "system:node:yksv005211" cannot create daemonsets.extensions in the namespace "kube-system"Error from server (Forbidden): error when creating "kubeadm-router.yaml": serviceaccounts is forbidden: User "system:node:yksv005211" cannot create serviceaccounts in the namespace "kube-system"Error from server (Forbidden): error when creating "kubeadm-router.yaml": clusterroles.rbac.authorization.k8s.io is forbidden: User "system:node:yksv005211" cannot create clusterroles.rbac.authorization.k8s.io at the cluster scopeError from server (Forbidden): error when creating "kubeadm-router.yaml": clusterrolebindings.rbac.authorization.k8s.io is forbidden: User "system:node:yksv005211" cannot create clusterrolebindings.rbac.authorization.k8s.io at the cluster scope''' 此时 此前配置的变量KUBECONFIG=/etc/kubernetes/kubelet.conf 这是普通用户，没有权限，会有报错，需按照kubeadm初始化完成后的提示，操作一遍 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 或者亦可切换环境变量指向admin.conf内的配置，则拥有管理权限了: 1export KUBECONFIG=/etc/kubernetes/admin.conf 重点：为防止以上坑爹问题，建议干净系统、统一工具版本，初始化失败重来时，kubeadm reset，然后将相关组件全部重启，涉及路径全部清空重新创建空路径。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker、k8s常用命令速查及k8s restful api接口]]></title>
    <url>%2F2018%2F04%2F27%2Fdocker%E3%80%81k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%80%9F%E6%9F%A5%E5%8F%8Ak8s%20restful%20api%E6%8E%A5%E5%8F%A3.html</url>
    <content type="text"><![CDATA[## 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283docker容器所依赖的六种namespace级别的资源隔离技术：1.UTS(UNIX Time-sharing system) 允许创建独立的hostname/domian_name namespace，作为独立的节点2.IPC(Inter Process Communication) 创建隔离的进程间通信方式，实现包括信号量、消息队列、管道、共享内存等资源的namespace隔离3.PID(Process ID) 允许树的子叶节点namespace创建自己专属的pid区间，pid对同级节点无效，仅父级namespace节点可见。(在子节点的pid，会在父节点映射为另一个pid标志， 因此在父节点的角度，每个子节点的进程拥有两个pid号)4.mount 挂载点namespace，各个namespace默认的mount挂载只在本空间内生效，不影响其他namespace，也可使用share共享挂载的方式互相影响， 或者slave从属挂 载的方式，实现主影响从。5.network 每个net ns拥有独立的网络资源(包括网络设备,IPv4/IPv6协议栈，防火墙，/proc/net、/sys/class/net目录等)6.user 每个user ns拥有独立的id、属性、key、根目录，组织结构等 docker: - docker ps -a - docker build -t xxx . #根据Dockerfile生成image - docker images - docker exec -it &#123;ID\NAME&#125; /bin/bash | /bin/sh - docker run -it &#123;ID\NAME&#125; -p -v - docker start &#123;ID\NAME&#125; - docker stop &#123;ID\NAME&#125; - docker rm &#123;ID\NAME&#125; - docker rmi &#123;ID\NAME&#125; - docker save coredns/coredns:1.0.0 | gzip &gt; coredns.tar.gz #将已有的img打包 - docker load -i IMAGE #docker载入本地image打包文件 - docker stats &#123;ID\NAME&#125; #查看容器资源占用情况，不填查看全部 - docker cp - docker commit -p -m 'xxx' 1a889d5bbf99 xxx #将容器打标签提交为img，-p选项意思是打包时暂停容器，不会退出 - docker push xxx #push到容器仓库 k8s: - kubectl get pods -o wide - kubectl get pod xxx -o yaml #获取yaml配置文件 - kubectl get nodes -o wide - kubectl set image deployment xxx xxx=image_url #更改部署镜像 - kubectl describe pod mysql-deploy-766bd7dbcb-7nxvw #查看pod的创建运行记录 - kubectl scale deploy/kube-dns --replicas=3 #修改deploy的副本数 - kubectl create -f xxx.yaml #创建资源 - kubectl delete deploy mysql-deploy #删除资源 - kubectl get svc -o wide - kubectl get ep SVC_NAME #查看svc对应绑定的pod - kubectl get rs - kubectl get deploy/DEPLOY-NAME - kubectl get all #获取所有类型资源 - kubectl get componentstatuses #获取k8s各组件健康状态，简写为kubectl get cs - kubectl describe deploy/DEPLOY-NAME - kubectl status rollout deploy/DEPLOY-NAME - kubectl label nodes 171 disktype=ssd #添加标签 - kubectl label nodes 171 disktype- #删除标签 - kubectl label nodes 171 disktype=hdd --overwrite #修改标签 - kubectl logs POD_NAME #查看pod的日志，排错用 - kubectl get nodes -o json | jq '.items[] | .spec' #查看每个node的CIDR分配 - kubectl delete pod NAME --grace-period=0 --force #强制删除资源，在1.3版本去掉--force选项 - kubectl replace -f xxx.yaml #更改定义资源的yaml配置文件 - kubectl get secret -n kube-system | grep dashboard #查找secret - kubectl describe secret -n kube-system kubernetes-dashboard-token-ld92d #查看该secret的令牌 - kubectl scale --replicas=3 deployment/xxxx #横向扩展deploy的rs数量 - kubectl cordon NODENAME #将node设置为检修状态，不再向此node调度新的pod - kubectl drain NODENAME #将node设置为（排水）不可用状态，并且驱逐其上的pod转移至其他正常node上。这一步会进行两个步骤：1.将node设为cordon状态2.驱逐node上的pod - kubectl drain node2 --delete-local-data --force --ignore-daemonsets #忽略ds，否则存在ds的话无法正常执行 - kubectl uncordon NODENAME #取消检修状态、排水状态 - kubectl proxy --address=0.0.0.0 --port=8001 --accept-hosts=^.* &amp; #kubectl监听端口代理apiserver的服务，允许调用客户端免去认证步骤 - kubectl patch deployment my-nginx --patch '&#123;"spec": &#123;"template": &#123;"metadata": &#123;"annotations": &#123;"update-time": "2018-04-11 12:15" &#125;&#125;&#125;&#125;&#125;' #更新configmap之后挂载configmap 的pod中的环境变量不会自动更新，可以通过更新一下deployment.spec.template的注解内容来触发pod的滚动更新。**k8s restful api**#### GET方法## labelSelector# 获取标签&#123;app: pimsystemdev&#125;的podhttp://192.168.9.27:8001/api/v1/namespaces/default/pods?labelSelector=app%3Dpimsystemdev# 动态获取标签&#123;app: pimsystemdev&#125;的podhttp://192.168.9.27:8001/api/v1/namespaces/default/pods?watch&amp;labelSelector=app%3Dpimsystemdev## fieldSelector# 根据level 1字段获取信息http://192.168.9.27:8001/api/v1/namespaces/default/pods?fieldSelector=metadata.name%3Dykstaskdev-657c7f56fc-7vnd4]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（二）、对外服务(Traefik部署/https/session sticky使用)]]></title>
    <url>%2F2018%2F04%2F10%2Fk8s%EF%BC%88%E4%BA%8C%EF%BC%89%E3%80%81web%E6%9C%8D%E5%8A%A1(Traefik%E9%83%A8%E7%BD%B2_https_session%20sticky%E4%BD%BF%E7%94%A8).html</url>
    <content type="text"><![CDATA[前言在上文完成k8s集群搭建的基础上k8s（一）、1.9.0高可用集群本地离线部署记录，承接上文，本文简单介绍一下k8s对外暴露服务 拓扑图： 一、k8s对外暴露服务方式介绍1.Load Blance目前已经有很多主流的公有云平台已经都提供基于k8s服务的组件，工作的原理本质上是k8s的服务向云平台申请一个负载均衡器来对外服务。因此这种方式只适合在云平台内部使用，这里略过. 2.Nodeport针对某一个k8s service，k8s集群内的每一个node都将暴露node的一个指定接口，来为此service提供服务。此方式要求每一个node都提供一个端口，即使此node上没有承载有该service 的pod服务载体，因此此方式会带来一定资源的浪费和管理不便。 3.IngressIngress注入方式，有三个组件，来协同完成对外服务的过程： 1.reverse proxy LB 2.ingress controller 3.k8s Ingress组件 1.reverse proxy LB将服务请求反向代理至后端服务对应的node上，node收到后再由kube-proxy将请求转交给endpoint pod. 2.ingress controller监控apiserver上的svc关联关系的变化，若svc关联发生变化（例如svc后端对应的pods发生变化），则动态地获取变化，更新前面反向代理的配置文件和热重载。 3.k8s ingressk8s的一种资源类型，可以基于访问的虚拟主机、字段等进行区别路由映射到后端不同的k8s service上，ingress controller要实时监控每个ingress对象上指定的service来保证LB配置文件的热更新 ​​ 在本文，采用traefik作为ingress工具演示。 ### traefik： 由于微服务架构以及 Docker 、 kubernetes 编排工具最近才开始流行，因此常用的反向代理服务器 nginx并未提供对k8s ingress的支持，所以Ingress Controller 这个中间件的存在就是用来做 kubernetes 和前端LB来做衔接的；Ingress Controller工作的方式是读取k8s ingress，通过与k8s api交互实时监控service，当serivce映射关系变化时，能重写 nginx 配置和热更新。traefik的出现简化了这个流程，traefik本身是一个轻量级的reverse proxy LB，并且它原生支持跟 kubernetes API 交互，感知后端变化，因此traefik可以取代上面的1、2组件，简化结构。 traefik官方介绍图： 二、服务部署首先来部署一组简单的nginx应用容器。准备好yaml文件： 12[root@171 nginx]# lsnginx-deploy.yaml nginx_ingress.yaml nginx_service.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@171 nginx]# cat nginx-deploy.yaml apiVersion: extensions/v1beta1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: "2" creationTimestamp: 2018-04-09T04:02:02Z generation: 4 labels: app: nginx name: nginx-deploy namespace: default resourceVersion: "111504" selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/nginx-deploy uid: c28090c0-3baa-11e8-b75a-000c29858eabspec: replicas: 2 selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: app: nginx spec: containers: - image: nginx:1.9.1 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP resources: &#123;&#125; terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; terminationGracePeriodSeconds: 30status: availableReplicas: 2 conditions: - lastTransitionTime: 2018-04-09T04:57:27Z lastUpdateTime: 2018-04-09T04:57:27Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: "True" type: Available observedGeneration: 4 readyReplicas: 2 replicas: 2 updatedReplicas: 2 123456789101112131415161718192021222324[root@171 nginx]# cat nginx_service.yaml apiVersion: v1kind: Servicemetadata: creationTimestamp: 2018-04-09T11:34:09Z labels: run: nginx name: nginx namespace: default resourceVersion: "140236" selfLink: /api/v1/namespaces/default/services/nginx uid: eb57a21b-3be9-11e8-b75a-000c29858eabspec: clusterIP: 10.99.59.56 ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 123456789101112131415161718192021[root@171 nginx]# cat nginx_ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: creationTimestamp: 2018-04-09T11:39:48Z generation: 1 name: test namespace: default resourceVersion: "140644" selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/test uid: b54bbda8-3bea-11e8-b75a-000c29858eabspec: rules: - host: test.nginx.com http: paths: - backend: serviceName: nginx servicePort: 80status: loadBalancer: &#123;&#125; 创建nginx的deploy、svc、ing资源，使用–record命令后面可以看到revision记录： 123kubectl create -f nginx-deploy.yaml --recordkubectl create -f nginx-service.yaml --recordkubectl create -f nginx-ingress.yaml --record 过1分钟后查看状态： 12345678910[root@171 nginx]# kubectl get pods -o wide | grep nginxnginx-deploy-5964dfd755-b2xj5 1/1 Running 2 1d 10.244.0.16 170nginx-deploy-5964dfd755-v6tdq 1/1 Running 1 4h 10.244.0.17 170[root@171 nginx]# kubectl get ing | grep nginxtest test.nginx.com 80 23h[root@171 nginx]# kubectl get svc | grep nginxnginx ClusterIP 10.99.59.56 &lt;none&gt; 80/TCP 23h 在本地curl pod ip和service 的clusterIP测试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@171 nginx]# curl http://10.244.0.16&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;[root@171 nginx]# curl http://10.244.0.17&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;[root@171 nginx]# curl http://10.99.59.56&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; nginx资源已经创建好了，但是目前只能在本地和集群内访问，集群外部无法访问，需要把网络路由打通 三、网络配置1.vmware网络配置：(无网络限制这一步可跳过)在公司的电脑上搭的wmware虚拟机，公司网络上网认证限制外网，单人只允许使用单IP访问外网，因此虚拟机为了访问外网，使用的是NAT模式转接的物理机的网络，vmware NAT方式虚拟机通信配置：首先，每台vm都要配置：vmware–编辑–虚拟网络编辑器： 为了直接访问vm网段，在windows物理机上配置路由：打开cmd，输入： 1route add 192.168.0.0 MASK 255.255.255.0 192.168.0.1 IF 9 IF是接口编号，可以使用route print查看自己电脑上vmnet8网卡对应的网卡接口编号打开windows网络共享中心，找到物理网卡，右键–属性–共享，勾选开启网络连接共享（类似linux内核设置开启IP forward转发）： 2.k8s服务网段、pods网段路由，dns配置：步骤1是只是打通了外部到vm之间的网络，如果没有网络限制，vm跟外部在同一网段的可以跳过上面的步骤1。 步骤2是外部添加k8s的服务网段、pods网段：windows cmd添加12route add 10.96.0.0 MASK 255.240.0.0 192.168.0.169 IF 9route add 10.244.0.0 MASK 255.255.0.0 192.168.0.169 IF 9 windows添加dns记录：C:\Windows\System32\drivers\etc\hosts打开编辑，添加dns记录：12192.168.0.169 test.nginx.com192.168.0.169 ui.traefik.com 打开浏览器测试访问pod ip、服务的cluster ip：可以直接访问！下面开始部署traefik，通过虚拟主机名访问. 四、traefik部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130[root@171 traefik]# lstraefik-ds.yaml traefik_ingress.yaml traefik-rbac.yaml[root@171 traefik]# cat traefik-rbac.yaml ---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerrules: - apiGroups: - "" resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: traefik-ingress-controllerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controllersubjects:- kind: ServiceAccount name: traefik-ingress-controller namespace: kube-system[root@171 traefik]# cat traefik-ds.yaml ---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 hostNetwork: true containers: - image: traefik name: traefik-ingress-lb ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 8080 name: admin type: NodePort[root@171 traefik]# cat traefik_ingress.yaml apiVersion: extensions/v1beta1kind: Ingressmetadata: creationTimestamp: 2018-04-09T11:39:48Z generation: 1 name: traefik-ui namespace: default resourceVersion: "140644" selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/test uid: b54bbda8-3bea-11e8-b75a-000c29858eabspec: rules: - host: ui.traefik.com http: paths: - backend: serviceName: traefik-ingress-service servicePort: 80status: loadBalancer: &#123;&#125; 创建资源： 123kubectl create -f traefik-rbac.yaml --recordkubectl create -f traefik-ds.yaml --recordkubectl create -f traefik-ingress.yaml --record 1分钟后查看创建情况： 12345678910111213[root@171 traefik]# kubectl get pods -o wide -n kube-system| grep traetraefik-ingress-controller-8lht4 1/1 Running 0 3h 192.168.0.171 171traefik-ingress-controller-ddvws 1/1 Running 0 3h 192.168.0.170 170[root@171 traefik]# kubectl get ds -n kube-system| grep traetraefik-ingress-controller 2 2 2 2 2 &lt;none&gt; 3h[root@171 traefik]# kubectl get ing NAME HOSTS ADDRESS PORTS AGEtest test.nginx.com 80 1dtraefik-ui ui.traefik.com 80 3h 打开浏览器测试： 开启https提前准备好证书和key:12root@h001238:~/traefik/ssl# lscert.crt cert.key 创建secret保存https证书:1kubectl create secret generic traefik-cert --from-file=./cert.crt --from-file=./cert.key -n kube-system 编辑traefik配置文件: 12345678910111213# vim traefik.tomlinsecureSkipVerify = truedefaultEntryPoints = ["http","https"][entryPoints] [entryPoints.http] address = ":80" entryPoint = "https" [entryPoints.https] address = ":443" [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = "/opt/conf/k8s/ssl/cert.crt" keyFile = "/opt/conf/k8s/ssl/cert.key" 创建ConfigMap: 1kubectl create configmap traefik-conf --from-file=traefik.toml -n kube-system 修改DaemonSet部署文件,apply部署后直接删除原traefik pod进行配置更新.:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 hostNetwork: true volumes: - name: ssl secret: secretName: traefik-cert - name: config configMap: name: traefik-conf containers: - image: traefik name: traefik-ingress-lb volumeMounts: - mountPath: "/opt/conf/k8s/ssl" name: "ssl" - mountPath: "/opt/conf/k8s/conf" name: "config" ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO - --configFile=/opt/conf/k8s/conf/traefik.toml 查看更新后node上的端口监听:1234root@h001238:~/traefik# ss -tnlp | grep traeLISTEN 0 128 :::8080 :::* users:(("traefik",pid=18337,fd=6))LISTEN 0 128 :::80 :::* users:(("traefik",pid=18337,fd=3))LISTEN 0 128 :::443 :::* users:(("traefik",pid=18337,fd=5)) 开启session stickyTraefik支持session sticky, 仅需添加一些简单的配置。有两种方法，一种是在traefik端配置文件中修改，一种是在svc端添加注解。第一种：在traefik配置文件中顶格添加如下几行，为backend1这个后端开启session sticky。这种方式需要traefik频繁重载配置文件，不适用于实际场景，所以这里不作演示了: 1234[backends] [backends.backend1] # Enable sticky session [backends.backend1.loadbalancer.stickiness] 完整ConfigMap如下12345678910111213141516171819202122232425262728apiVersion: v1data: traefik.toml: | insecureSkipVerify = true defaultEntryPoints = ["http","https"] [entryPoints] [entryPoints.http] address = ":80" entryPoint = "https" [entryPoints.https] address = ":443" [entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = "/opt/conf/k8s/ssl/kokoerp.crt" keyFile = "/opt/conf/k8s/ssl/214417797960145.key" [metrics] [metrics.prometheus] entryPoint = "traefik" buckets = [0.1,0.3,1.2,5.0] [backends] [backends.backend1] # Enable sticky session [backends.backend1.loadbalancer.stickiness]kind: ConfigMapmetadata: creationTimestamp: 2018-10-18T07:58:57Z name: traefik-conf namespace: kube-system 第二种：在需要添加session sticky功能的service资源上添加注解如下: 1234annotations: traefik.ingress.kubernetes.io/affinity: "true" # 可选，可以自定义 traefik.ingress.kubernetes.io/session-cookie-name: backend 完成添加注解配置后即可实现该svc绑定的ingress的域名的session sticky功能。 这里使用prometheus举例将prometheus的deployment的replicas设置数量为2，生成两个pod后端实例：在prometheus的service上添加上方列出的annotation注解后，浏览器访问prometheus，即可看到traefik会为会话添加cookies，key为刚指定的值backend，value默认为后端pod实例：重复刷新，backend值不会改变，手动清除此cookies后，再次刷新： session sticky配置完成更为详细的会话滞黏配置说明请参考官方文档：配置文件Service注解 11.10补充prometheus + traefik 监控traefik配置添加metric段: 1234[metrics] [metrics.prometheus] entryPoint = "traefik" buckets = [0.1,0.3,1.2,5.0] 更新configmap后重启traefik-ingress-lb容器. 在prometheus中添加job: 1234- job_name: 'traefik' scrape_interval: 5s static_configs: - targets: ['traefik-ui'] #填服务名或任意node IP+指定ui端口 更新configmap后重启prometheus容器. 在prometheus-target中可以看到新job的状态: 在grafana中导入id为2870的模板,新版本的traefik metrics名称发生了一些小变化,因此默认模板获取不到数据,需自行对照job target详情页修改一下模板,熟悉prometheus和grafana的话也可以进行一些自定义修改:: Prometheus/grafana相关知识及使用可以参考专栏内的其他文章 总结k8s的各种功能组件、名词概念、资源类型（pod/svc/ds/rs/ingress/configmap/role….）、工作流程 等颇为复杂，理解起来不太容易，需要花时间阅读官方文档，因为k8s更新很快，有一些新功能或者即将淘汰的旧功能，官方的中文文档更新不太及时，建议直接阅读英文文档。 另外这里为了方便展示，用的windows查看效果，用linux curl工具测试是一样的，且网络的限制，使用的是vmware的NAT，需要多配置一步物理机网络到vm网络之间打通，再使用静态路由让k8s内部集群网络进行通信。有时间回家里电脑尝试下GNS3模拟路由器，搭建外部bgp，发布到集群bgp内部。]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s（一）、 1.9.0高可用集群本地离线部署记录]]></title>
    <url>%2F2018%2F04%2F08%2Fk8s%EF%BC%88%E4%B8%80%EF%BC%89%E3%80%811.9.0%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%9C%AC%E5%9C%B0%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95.html</url>
    <content type="text"><![CDATA[一、部署说明1.节点 12345master1: IP:192.168.0.170/24 hostname：171master2: IP:192.168.0.171/24 hostname：172VIP:192.168.0.169/24 2.工具版本： 123456etcd-v3.1.10-linux-amd64.tar.gzkubeadm-1.9.0-0.x86_64.rpmkubectl-1.9.0-0.x86_64.rpmkubelet-1.9.9-9.x86_64.rpmkubernetes-cni-0.6.0-0.x86_64.rpmsocat-1.7.3.2-2.el7.x86_64.rpm docker离线镜像： 12345678910etcd-amd64_v3.1.10.tark8s-dns-dnsmasq-nanny-amd64_v1.14.7.tark8s-dns-kube-dns-amd64_1.14.7.tark8s-dns-sidecar-amd64_1.14.7.tarkube-apiserver-amd64_v1.9.0.tarkube-controller-manager-amd64_v1.9.0.tarkube-proxy-amd64_v1.9.0.tarkubernetes-dashboard_v1.8.1.tarkube-scheduler-amd64_v1.9.0.tarpause-amd64_3.0.tar 以上安装包的云盘链接随后附上.有梯子翻墙的直接装，无需离线包。 组件作用说明集群两个最重要的核心：apiserver master和etcd 1.apiserver master：（需高可用）集群核心，集群API接口、集群各个组件通信的中枢；集群安全控制； 2.etcd ：（需高可用）集群的数据中心，用于存放集群的配置以及状态信息，非常重要，如果数据丢失那么集群将无法恢复；因此高可用集群部署首先就是etcd是高可用集群；kube-scheduler：调度器 （内部自选举）集群Pod的调度中心；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-scheduler处于活跃状态； 3.kube-controller-manager： 控制器（内部自选举）集群状态管理器，当集群状态与期望不同时，kcm会努力让集群恢复期望状态，比如：当一个pod死掉，kcm会努力新建一个pod来恢复对应replicasset期望的状态；默认kubeadm安装情况下–leader-elect参数已经设置为true，保证master集群中只有一个kube-controller-manager处于活跃状态； 4.kubelet: 将agent node注册上apiserver,是运行在每个node上的集群管理工具 5.scheduler：专门负责pod的调度任务，根据预定义的各种调度规则，将待调度的pod分配至某一个节点上。 二、提前准备1.关闭防火墙，selinux，swap 123456789101112systemctl stop firewalldsystemctl disable firewalldsystemctl disable firewalldsetenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinux sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config sed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinux sed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config swapoff -aecho 'swapoff -a ' &gt;&gt; /etc/rc.d/rc.local 2.内核开启网络支持 12345cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl -p /etc/sysctl.conf 若执行sysctl -p 时报错，尝试： 12modprobe br_netfilterls /proc/sys/net/bridge 三、配置keepalived123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103yum -y install keepalived # 192.168.0.170配置cat &gt;/etc/keepalived/keepalived.conf &lt;&lt;EOLglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; #修改为本机IP script "curl -k https://192.168.0.170:6443" interval 3 timeout 5 weight 11 fall 3 rise 2&#125;vrrp_instance VI_0 &#123; # 统一默认设置为BACKUP状态，成员会根据优先级值来自协商状态 state BACKUP interface eth0 virtual_router_id 80 # 主节点权重最高 依次减少 priority 110 advert_int 1 #修改为本地IP mcast_src_ip 192.168.0.170 track_interface &#123; eth0 &#125; authentication &#123; auth_type PASS auth_pass yks888 &#125; unicast_peer &#123; #注释掉本地IP #192.168.0.170 192.168.0.171 &#125; virtual_ipaddress &#123; 192.168.0.169/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOF# 192.168.0.171配置cat &gt;/etc/keepalived/keepalived.conf &lt;&lt;EOLglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; #修改为本机IP script "curl -k https://192.168.0.171:6443" interval 3 timeout 5 weight 11 fall 3 rise 2&#125;vrrp_instance VI_0 &#123; # 统一默认设置为BACKUP状态，成员会根据优先级值来自协商状态 state BACKUP interface eth0 virtual_router_id 80 # 主节点权重最高 依次减少 priority 100 advert_int 1 #修改为本地IP mcast_src_ip 192.168.0.171 track_interface &#123; eth0 &#125; authentication &#123; auth_type PASS auth_pass yks888 &#125; unicast_peer &#123; #注释掉本地IP 192.168.0.170 #192.168.0.171 &#125; virtual_ipaddress &#123; 192.168.0.169/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOFsystemctl enable keepalived &amp;&amp; systemctl restart keepalived 检查vip是否正常漂在master1上： 12345678910111213[root@170 pkgs]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:1c:5d:71 brd ff:ff:ff:ff:ff:ff inet 192.168.0.170/24 brd 192.168.0.255 scope global ens33 valid_lft forever preferred_lft forever inet 192.168.0.169/24 scope global secondary ens33 valid_lft forever preferred_lft forever 四、etcd配置1.配置环境变量： 12345export NODE_NAME=170 #当前部署的机器名称(随便定义，只要能区分不同机器即可)export NODE_IP=192.168.0.170 # 当前部署的机器 IPexport NODE_IPS="192.168.0.170 192.168.0.171 " # etcd 集群所有机器 IP# etcd 集群间通信的IP和端口export ETCD_NODES="170"=https://192.168.0.170:2380,"171"=https://192.168.0.171:2380 2.创建ca和证书和秘钥 1234567891011wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo 生成ETCD的TLS 秘钥和证书为了保证通信安全，客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信需要使用 TLS 加密，本节创建 etcd TLS 加密所需的证书和私钥。创建 CA 配置文件： 1234567891011121314151617181920cat &gt; ca-config.json &lt;&lt;EOF&#123;"signing": &#123;"default": &#123; "expiry": "87600h"&#125;,"profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125;&#125;&#125;&#125;EOF ==ca-config.json==：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；==signing==：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；==server auth==：表示 client 可以用该 CA 对 server 提供的证书进行验证；==client auth==：表示 server 可以用该 CA 对 client 提供的证书进行验证； 123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123;"CN": "kubernetes","key": &#123;"algo": "rsa","size": 2048&#125;,"names": [&#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System"&#125;]&#125;EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；==生成 CA 证书和私钥==： 12cfssl gencert -initca ca-csr.json | cfssljson -bare cals ca* ==创建 etcd 证书签名请求：== 1234567891011121314151617181920212223cat &gt; etcd-csr.json &lt;&lt;EOF&#123; "CN": "etcd", "hosts": [ "127.0.0.1", "192.168.0.170", "192.168.0.171" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF hosts 字段指定授权使用该证书的 etcd 节点 IP；每个节点IP 都要在里面 或者 每个机器申请一个对应IP的证书生成 etcd 证书和私钥： 123456789cfssl gencert -ca=ca.pem \ -ca-key=ca-key.pem \ -config=ca-config.json \ -profile=kubernetes etcd-csr.json | cfssljson -bare etcdls etcd*mkdir -p /etc/etcd/sslcp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/ 将生成的私钥文件分发到master2上： 1scp -r /etc/etcd/ssl root@192.168.0.171:/etc/etcd/ 3.安装etcd 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647wget http://github.com/coreos/etcd/releases/download/v3.1.10/etcd-v3.1.10-linux-amd64.tar.gztar -xvf etcd-v3.1.10-linux-amd64.tar.gzmv etcd-v3.1.10-linux-amd64/etcd* /usr/local/binmkdir -p /var/lib/etcd # 必须先创建工作目录cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \\ --name=$&#123;NODE_NAME&#125; \\ --cert-file=/etc/etcd/ssl/etcd.pem \\ --key-file=/etc/etcd/ssl/etcd-key.pem \\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/etcd/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\ --initial-advertise-peer-urls=https://$&#123;NODE_IP&#125;:2380 \\ --listen-peer-urls=https://$&#123;NODE_IP&#125;:2380 \\ --listen-client-urls=https://$&#123;NODE_IP&#125;:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://$&#123;NODE_IP&#125;:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=$&#123;ETCD_NODES&#125; \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF#注意，上方的环境变量一定要先配置好再进行这一步操作，在master2上export环境变量时注意修改ipmv etcd.service /etc/systemd/system/systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd master2同样方法安装etcd 验证服务 123456etcdctl \ --endpoints=https://$&#123;NODE_IP&#125;:2379 \ --ca-file=/etc/etcd/ssl/ca.pem \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ cluster-health 预期结果： 12342018-04-06 20:07:41.355496 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated2018-04-06 20:07:41.357436 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecatedmember 57d56677d6df8a53 is healthy: got healthy result from https://192.168.0.170:2379member 7ba40ace706232da is healthy: got healthy result from https://192.168.0.171:2379 若有失败，重新配置 123456789systemctl stop etcdrm -Rf /var/lib/etcdrm -Rf /var/lib/etcd-clustermkdir -p /var/lib/etcdsystemctl start etcd 五.docker安装若原来已经安装docker版本，建议卸载后安装docker-ce版本： 1yum -y remove docker docker-common 基于aliyun安装docker-ce方法： 1234567891011121314151617181920212223242526sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fast#查看repo中所有版本的docker-ceyum list docker-ce --showduplicates | sort -rdocker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stablesudo yum -y install docker-ce-17.03.2.ce#若提示依赖docker-ce-selinux，则先执行yum -y install docker-ce-selinux-17.03.2.ce# Step 4: 开启Docker服务sudo service docker start &amp;&amp; systemctl enbale docker 六.安装kubelet、kubectl、kubeadm、kubecni准备好提前下载好的rpm包： 12345[root@170 pkgs]# cd /root/k8s_images/pkgs[root@170 pkgs]# lsetcd-v3.1.10-linux-amd64.tar.gz kubeadm-1.9.0-0.x86_64.rpm kubectl-1.9.0-0.x86_64.rpm kubelet-1.9.9-9.x86_64.rpm kubernetes-cni-0.6.0-0.x86_64.rpm socat-1.7.3.2-2.el7.x86_64.rpm[root@170 pkgs]# yum -y install *.rpm 修改kubelet启动参数里的cgroup驱动以便兼容docker： 123456vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf#Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"systemctl enable docker &amp;&amp; systemctl restart dockersystemctl daemon-reload &amp;&amp; systemctl restart kubelet 在master2上同样操作 七.kubeadm初始化将准备好的本地的docker images镜像导入本地仓库内 123456[root@170 docker_images]# lsetcd-amd64_v3.1.10.tar k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tar k8s-dns-sidecar-amd64_1.14.7.tar kube-controller-manager-amd64_v1.9.0.tar kubernetes-dashboard_v1.8.1.tar pause-amd64_3.0.tarflannel:v0.9.1-amd64.tar k8s-dns-kube-dns-amd64_1.14.7.tar kube-apiserver-amd64_v1.9.0.tar kube-proxy-amd64_v1.9.0.tar kube-scheduler-amd64_v1.9.0.tar[root@170 docker_images]# for i in `ls`;do docker load -i $i;done config.yaml初始化配置文件： 12345678910111213141516171819202122232425262728cat &lt;&lt;EOF &gt; config.yaml apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationetcd: endpoints: - https://192.168.0.170:2379 - https://192.168.0.171:2379 caFile: /etc/etcd/ssl/ca.pem certFile: /etc/etcd/ssl/etcd.pem keyFile: /etc/etcd/ssl/etcd-key.pem dataDir: /var/lib/etcdnetworking: podSubnet: 10.244.0.0/16kubernetesVersion: 1.9.0api: advertiseAddress: "192.168.0.170"token: "b99a00.a144ef80536d4344"tokenTTL: "0s"apiServerCertSANs:- "170"- "171"- 192.168.0.169- 192.168.0.170- 192.168.0.171featureGates: CoreDNS: trueEOF 初始化集群 1kubeadm init --config config.yaml 结果： 12345678910111213To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configas root: kubeadm join --token b99a00.a144ef80536d4344 192.168.0.169:6443 --discovery-token-ca-cert-hash sha256:ebc2f64e9bcb14639f26db90288b988c90efc43828829c557b6b66bbe6d68dfa [root@170 docker_images]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME170 Ready master 5h v1.9.0 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-693.el7.x86_64 docker://18.3.0171 Ready master 4h v1.9.0 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-693.el7.x86_64 docker://18.3.0 注意：提示信息中的kubeadm join那一行请保存好，后期使用kubeadm扩增集群node时，需要在新的node上使用这行命令加入集群 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 配置master2： 123456789#拷贝pki 证书mkdir -p /etc/kubernetes/pkiscp -r /etc/kubernetes/pki/ root@192.168.0.171:/etc/kubernetes/ #拷贝初始化配置scp config.yaml root@192.168.0.171:/etc/kubernetes/#在master2上初始化kubeadm init --config /etc/kubernetes/config.yaml 八、安装网络插件kube-router1234567891011121314151617# kube-router的daemonSet部署yaml文件地址:# https://github.com/cloudnativelabs/kube-router/blob/master/daemonset/kubeadm-kuberouter.yamlkubectl apply -f kubeadm-kuberouter.yaml[root@170 docker_images]# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-546545bc84-j5mds 1/1 Running 2 5hkube-system kube-apiserver-170 1/1 Running 2 5hkube-system kube-apiserver-171 1/1 Running 2 4hkube-system kube-controller-manager-170 1/1 Running 2 5hkube-system kube-controller-manager-171 1/1 Running 2 4hkube-system kube-proxy-5dg4j 1/1 Running 2 4hkube-system kube-proxy-s6jtn 1/1 Running 2 5hkube-system kube-router-wbxvq 1/1 Running 2 4hkube-system kube-router-xrkbw 1/1 Running 2 4hkube-system kube-scheduler-170 1/1 Running 2 5hkube-system kube-scheduler-171 1/1 Running 2 4h 部署完成 默认情况下，为了保证master的安全，master是不会被调度到app的。给master添加调度taint污点，打破这个限制，使pod可以调度在master上： 1kubectl taint nodes --all node-role.kubernetes.io/master- 配置私有docker仓库，加速使用： 12345vim /lib/systemd/system/docker.service 注释#ExecStart=/usr/bin/dockerdExecStart=/usr/bin/dockerd --insecure-registry registry.yourself.com:5000#填写自己的docker仓库地址 2018-6-8补充：k8s集群通过kubeadm加入新的node： 在集群初始化完成时，会提示如下信息：12345678To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configas root: kubeadm join --token b99a00.a144ef80536d4344 192.168.0.169:6443 --discovery-token-ca-cert-hash sha256:ebc2f64e9bcb14639f26db90288b988c90efc43828829c557b6b66bbe6d68dfa 其中kubeadm join是为后期新的node加入集群时使用的，在公司生产上的集群部署好时此命令没有保存，且token时有实效性的。后期有node加入集群，join命令的两个参数可以在master节点上通过如下命令获取： token：123root@yksp009027:~# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSb97h00.a144ef08526d4345 &lt;forever&gt; &lt;never&gt; authentication,signing The default bootstrap token generated by 'kubeadm init'. system:bootstrappers:kubeadm:default-node-token hash: 12openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'7eb62962040916673ab58b0c40fb451caef7538452f5e6c8ff7719a81fdbf633 然后在新node上执行： 1kubeadm join --token=$TOKEN $MASTER_IP:6443 --discovery-token-ca-cert-hash sha256:$HASH 查看node，kubectl get nodes新node已进入Ready状态。（在join前请保证node上安装好kubelet、docker、kubeadm、kubernetes-cni、socat，并启动kubelet、docker，参考上方步骤）]]></content>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django-rest-framework (一)、序列化]]></title>
    <url>%2F2018%2F03%2F31%2FDjango-rest-framework%20(%E4%B8%80)%E3%80%81%E5%BA%8F%E5%88%97%E5%8C%96.html</url>
    <content type="text"><![CDATA[序列化django-rest-framework serializer可以很方便的实现model对象的序列化，对前端传递的数据进行验证等等，功能与django原生的form很相似，但是却比form更强大，下面体验一下。 开始安装： 1pip install django-rest-framework 创建一个新django项目：12django-admin.py startproject restpython manage.py startapp snippet rest/settings.py添加app： 12345INSTALLED_APPS = ( ... 'rest_framework', 'snippets',) 创建一个modelsnippet/models.py: 1234567891011121314151617181920212223from django.db import models# Create your models here.LANGUAGE_CHOICES = ( ('1', 'cn'), ('2', 'us'))STYLE_CHOICES = ( ('1', 't1'), ('2', 't2'))class Snippet(models.Model): created = models.DateTimeField(auto_now_add=True) title = models.CharField(max_length=100,blank=True, default='') code = models.TextField() linenos = models.BooleanField(default=False) language = models.CharField(choices=LANGUAGE_CHOICES, default= '1',max_length=5) style = models.CharField(choices=STYLE_CHOICES, default='1',max_length=5) class Meta: ordering = ('created',) 创建serializer.py文件：snippet/serializer.py: 12345678910111213141516171819202122from rest_framework import serializersfrom snippet.models import Snippet,LANGUAGE_CHOICES,STYLE_CHOICESclass Snippetserializer(serializers.Serializer): pk = serializers.IntegerField(read_only=True) title = serializers.CharField(required=False, allow_blank=True, max_length=32) code = serializers.CharField(style=&#123;'base_template': 'textarea.html'&#125;) linenos = serializers.BooleanField(required=False) language = serializers.ChoiceField(choices=LANGUAGE_CHOICES, default= '1') style = serializers.ChoiceField(choices=STYLE_CHOICES, default='1') def create(self, validated_data): return Snippet.objects.create(**validated_data) def update(self, instance, validated_data): instance.title = validated_data.get('title', instance.title) instance.code = validated_data.get('code', instance.code) instance.linenos = validated_data.get('linenos', instance.linenos) instance.language = validated_data.get('language', instance.language) instance.style = validated_data.get('style', instance.style) instance.save() return instance 创建表结构：12python manage.py makemigrationspython manage.py migrate 开始使用django命令行,写一些序列化测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041$ python manage.py shell&gt;&gt;&gt; from snippet.models import Snippet&gt;&gt;&gt; from snippet.serializers import Snippetserializer&gt;&gt;&gt; from rest_framework.renderers import JSONRenderer&gt;&gt;&gt; from rest_framework.parsers import JSONParser&gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; snippet= Snippet(code='foo="bar"\n')&gt;&gt;&gt; snippet.save()&gt;&gt;&gt; snippet= Snippet(code='print"hello,world"\n')&gt;&gt;&gt; snippet.save()&gt;&gt;&gt;&gt; obj = Snippetserializer(snippet)&gt;&gt;&gt; obj.data&#123;'language': '1', 'pk': 2, 'title': '', 'code': 'print"hello,world"\n', 'linenos': False, 'style': '1'&#125;&gt;&gt;&gt; obj['code']&lt;BoundField value=print"hello,world" errors=None&gt;&gt;&gt;&gt; content=JSONRenderer().render(obj.data)&gt;&gt;&gt; stream = BytesIO(content)&gt;&gt;&gt; content1 = JSONParser().parse(stream)&gt;&gt;&gt; content2=JSONRenderer().render(obj.data).decode()&gt;&gt;&gt; print(content)b'&#123;"pk":2,"title":"","code":"print\\"hello,world\\"\\n","linenos":false,"language":"1","style":"1"&#125;'&gt;&gt;&gt; type(obj)&lt;class 'snippet.serializers.Snippetserializer'&gt;&gt;&gt;&gt; type(obj.data)&lt;class 'rest_framework.utils.serializer_helpers.ReturnDict'&gt;&gt;&gt;&gt; type(content)&lt;class 'bytes'&gt;&gt;&gt;&gt; type(stream)&lt;class '_io.BytesIO'&gt;&gt;&gt;&gt; content2=JSONRenderer().render(obj.data).decode()&gt;&gt;&gt; type(content1)&lt;class 'dict'&gt;&gt;&gt;&gt; type(content2)&lt;class 'str'&gt;&gt;&gt;&gt; type(json.loads(content2))&lt;class 'dict'&gt;#可以看到，由rest serializer处理后的model对象变成了其的serializer对象，使用obj.data方法可以取出其内容，然后使用jsonrender方法将数据渲染成byte格式的json字符串，可以再通过bytesIO转换和通过jsonparser方法转换回dict格式。也可以直接在jsonrender的时候使用decode()方法处理，直接返回json格式str，然后通过json.loads()方法反序列化成dict格式。 同时，作为为django量身定制的框架，serializer也支持queryset的序列化（原生json模块不支持序列化queryset），在序列化的时候添加一个many = True参数即可： 1234567&gt;&gt;&gt; serializer= Snippetserializer(Snippet.objects.all(),many=True)&gt;&gt;&gt; serializer.data[OrderedDict([('pk', 1), ('title', ''), ('code', 'foo="bar"\n'), ('linenos', False), ('language', '1'), ('style', '1')]), OrderedDict([('pk', 2), ('title', ''), ('code', 'print"hello,world"\n'), ('linenos', False), ('language', '1'), ('style', '1')])]&gt;&gt;&gt; serializer.data[0]OrderedDict([('pk', 1), ('title', ''), ('code', 'foo="bar"\n'), ('linenos', False), ('language', '1'), ('style', '1')])&gt;&gt;&gt; serializer.data[0]['code']'foo="bar"\n' 梳理一下：1.model对象通过serializer方法序列化成新的对象obj,obj.data可以取出此对象的各items2.jsonrender方法可以将obj.data渲染成byte格式对象，再通过byteIO处理可以转换成python的byte格式数据流。也可以decode成json风格str类型。3.jsonparser方法可以将数据流(json风格str)反处理成dict格式 ModelSerializer如modelform一样，restframework同样存在可以直接基于model关联，进行field字段关联验证等功能的模块，ModelSerializer. 改写snippet/serializer.py： 1234567from rest_framework import serializersfrom snippet.models import Snippetclass SnippetSerializer(serializers.ModelSerializer): class Meta: model = Snippet fields = ('id','title','code','linenos','language','style') 写一个视图views.py，提供list查询、提交，单个对象查询、修改、删除方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from django.http import HttpResponsefrom django.views.decorators.csrf import csrf_exemptfrom rest_framework.renderers import JSONRendererfrom rest_framework.parsers import JSONParserfrom snippet.models import Snippetfrom snippet.serializers import SnippetSerializerclass JSONResponse(HttpResponse): def __init__(self,data,**kwargs): content = JSONRenderer().render(data) kwargs['content_type'] = 'application/json' super(JSONResponse,self).__init__(content,**kwargs)@csrf_exemptdef snippet_list(request): ''' List all code snippets, or create a new snippet. ''' if request.method == 'GET': snippets = Snippet.objects.all() serializer = SnippetSerializer(snippets, many=True) return JSONResponse(serializer.data) elif request.method == 'POST': data = JSONParser().parse(request) serializer = SnippetSerializer(data=data) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data,status=201) return JSONResponse(serializer.errors,status=400)@csrf_exemptdef snippet_detail(request,pk): try: snippet = Snippet.objects.get(pk=pk) except Snippet.DoesNotExist: return HttpResponse(status=404) if request.method == 'GET': serializer = SnippetSerializer(snippet) return JSONResponse(serializer.data) elif request.method == 'PUT': data = JSONParser().parse(request) serializer = SnippetSerializer(snippet, data=data) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data) return JSONResponse(serializer.errors, status=400) elif request.method == 'DELETE': snippet.delete() return HttpResponse(status=204) 定义urls规则关联视图函数：urls.py: 123456789from django.conf.urls import urlfrom django.contrib import adminfrom snippet import views as snippet_viewsurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^snippets/$',snippet_views.snippet_list), url(r'^snippets/detail/(?P&lt;pk&gt;[0-9]+)/$', snippet_views.snippet_detail),] 验证测试：开启服务： 1python manage.py runserver 测试： 1234$ curl http://127.0.0.1:8000/snippets/[&#123;"id":1,"title":"","code":"foo=\"bar\"\n","linenos":false,"language":"1","style":"1"&#125;,&#123;"id":2,"title":"","code":"print\"hello,world\"\n","linenos":false,"language":"1","style":"1"&#125;]ywq@ywq-ThinkPad-E470:/usr/local/myutils$ curl http://127.0.0.1:8000/snippets/detail/1$ curl http://127.0.0.1:8000/snippets/detail/1/&#123;"id":1,"title":"","code":"foo=\"bar\"\n","linenos":false,"language":"1","style":"1"&#125; 简单的接口OK了]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（六）、ORM操作进阶]]></title>
    <url>%2F2018%2F02%2F03%2FDjango%EF%BC%88%E5%85%AD%EF%BC%89%E3%80%81ORM%E6%93%8D%E4%BD%9C%E8%BF%9B%E9%98%B6.html</url>
    <content type="text"><![CDATA[前言上一篇中，介绍了django的单表操作，本篇描述一下django的多表操作，包括条件过滤、联表操作等. 一、双下划綫条件过滤： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 获取个数 # # models.Tb1.objects.filter(name='seven').count() # 大于，小于 # # models.Tb1.objects.filter(id__gt=1) # 获取id大于1的值 # models.Tb1.objects.filter(id__gte=1) # 获取id大于等于1的值 # models.Tb1.objects.filter(id__lt=10) # 获取id小于10的值 # models.Tb1.objects.filter(id__lte=10) # 获取id小于10的值 # models.Tb1.objects.filter(id__lt=10, id__gt=1) # 获取id大于1 且 小于10的值 # in # # models.Tb1.objects.filter(id__in=[11, 22, 33]) # 获取id等于11、22、33的数据 # models.Tb1.objects.exclude(id__in=[11, 22, 33]) # not in # isnull # Entry.objects.filter(pub_date__isnull=True) # contains # # models.Tb1.objects.filter(name__contains="ven") # models.Tb1.objects.filter(name__icontains="ven") # icontains大小写不敏感 # models.Tb1.objects.exclude(name__icontains="ven") # range # # models.Tb1.objects.filter(id__range=[1, 2]) # 范围bettwen and # 其他类似 # # startswith，istartswith, endswith, iendswith, # order by # # models.Tb1.objects.filter(name='seven').order_by('id') # asc # models.Tb1.objects.filter(name='seven').order_by('-id') # desc # group by # # from django.db.models import Count, Min, Max, Sum # models.Tb1.objects.filter(c1=1).values('id').annotate(c=Count('num')) # SELECT "app01_tb1"."id", COUNT("app01_tb1"."num") AS "c" FROM "app01_tb1" WHERE "app01_tb1"."c1" = 1 GROUP BY "app01_tb1"."id" # limit 、offset # # models.Tb1.objects.all()[10:20] # regex正则匹配，iregex 不区分大小写 # # Entry.objects.get(title__regex=r'^(An?|The) +') # Entry.objects.get(title__iregex=r'^(an?|the) +') # date # # Entry.objects.filter(pub_date__date=datetime.date(2005, 1, 1)) # Entry.objects.filter(pub_date__date__gt=datetime.date(2005, 1, 1)) # year # # Entry.objects.filter(pub_date__year=2005) # Entry.objects.filter(pub_date__year__gte=2005) # month # # Entry.objects.filter(pub_date__month=12) # Entry.objects.filter(pub_date__month__gte=6) # day # # Entry.objects.filter(pub_date__day=3) # Entry.objects.filter(pub_date__day__gte=3) # week_day # # Entry.objects.filter(pub_date__week_day=2) # Entry.objects.filter(pub_date__week_day__gte=2) # hour # # Event.objects.filter(timestamp__hour=23) # Event.objects.filter(time__hour=5) # Event.objects.filter(timestamp__hour__gte=12) # minute # # Event.objects.filter(timestamp__minute=29) # Event.objects.filter(time__minute=46) # Event.objects.filter(timestamp__minute__gte=29) # second # # Event.objects.filter(timestamp__second=31) # Event.objects.filter(time__second=2) # Event.objects.filter(timestamp__second__gte=31) 二、其他操作 123456# 执行原生SQL## from django.db import connection, connections# cursor = connection.cursor() # cursor = connections['default'].cursor()# cursor.execute("""SELECT * from auth_user where id = %s""", [1])# row = cursor.fetchone() 三、联表操作1.一对多表结构如下：1234567class Book(models.Model): name = models.CharField(max_length=32)class Author(models.Model): name = models.CharField(max_length=32) books = models.ForeignKey(to="Book",to_field="id") 查询:12345678#方式一a1=models.Author.objects.filter(id=1).first()a1.books.name #以属性的方式关联，Author类的对象通过books属性关联至Book对象，再通过.name属性获取Book对象的name字段#方式二res=models.Author.objects.filter(id=1).values("books_id","books__name")print(res)#在本地表通过ForeignKey关联，会在本地表内默认生成一个以指定属性名+'_'+'to_field字段名'的字段，例如这里会生成一个books_id字段，代指book.id字段，因此books_id不需跨表查询，books__id需跨表查询。通过双下划线，可以跨表关联字段，例如book__name字段代指book.name 改: 1models.Author.objects.filter(id=1).update("books":[b1,b2..]) 总结：一对多操作,单对象，通过”.”属性方法方式跨表操作，queryset通过”__”跨表取字段 2.多对多方式一：自定义第三张表： 12345678910class Book(models.Model): name = models.CharField(max_length=32)class Author(models.Model): name = models.CharField(max_length=32)class Relation(models.Model): b = models.ForeignKey('Book') a= models.ForeignKey('Author') 通过普通表进行反向间接查找 a1=models.Author.objects.filter(id=1).first() obj = a1.relation_set.all() #obj取出的是所有包含a1记录的relation类中的对象，queryset格式,使用类名+&quot;_set&quot;的格式时，类名字母需小写 obj1 = a1.relation_set.select_related(&apos;b&apos;).all() #obj1取出的是所有包含a1记录通过relation类关联的Book类的对象，queryset格式 for i in obj: print(i.b.name) for i in obj: print(i.name) 2 通过关系表relation进行查找 obj = models.Relation.objects.filter(a__=1).all() for i in obj: print(i.b.name) 通过中间关系表，相较于上面的反向查找更好理解。 方式二只定义两张表，关系表由django自动创建维护 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#models.pyclass Book(models.Model): name = models.CharField(max_length=32) class Author(models.Model): name = models.CharField(max_length=32) m = models.ManyToManyField('Book')# views.py # ManyToManyField obj = models.Author.objects.filter(id=1).first() # 增 # obj.m.add(3) # obj.m.add(3,4) # obj.m.add(*[1,2]) # 删 # obj.m.remove(3) # obj.m.remove(3,4) # obj.m.remove(*[1,2]) # 改 # obj.m.set([1,2,]) #查 obj.m.all() #查出所有的关联Book对象 obj.m.all().values("id","name") #查出所有的关联Book对象的name字段 # for row obj.m.all(): # print(row.id,row.name) #清除 # obj.m.clear() # 反向查 在没有定义ManyToManyField 字段的表中查 b= models.Book.objects.filter(id=1).first() b_authors = obj.author_set.all() #获取所有的对应的Author类的对象 # 反向的改 obj.author_set.set([1,2,3]) for row in girl_list: print(row.id,row.name) 总结：在没有ForeignKey或者ManyToManyField的表内反向查找时，需要使用使用类名+”_set”的格式来获取对象 one to one/maney to maney/foreign key各种混合关系查询1234567891011121314class HostInfo(models.Model): host = models.OneToOneField(to=Host, verbose_name="主机", primary_key=True) hostname = models.CharField(verbose_name="主机名", max_length=100, blank=True, null=True)class Host(models.Model): name = models.CharField(verbose_name="资产名", max_length=100, unique=True) ip = models.ManyToManyField(to=IP, verbose_name="IP", blank=True) category = models.ForeignKey(to=Category, verbose_name="分类") class Category(models.Model): name = models.CharField(max_length=20, verbose_name="分类") class IP(models.Model): address = models.CharField(verbose_name="IP地址", max_length=20) 123456789101112131415# 根据ip地址查host，hostinfoips = ("192.168.1.2","192.168.1.3",)host = Host.objects.filter(ip__address__in=ips)hostinfo = HostInfo.objects.filter(host__ip__address__in=ips)# 根据类型查host,hostinfohost = Host.objects.filter(category__name="physical")hostinfo = HostInfo.objects.filter(host__category__name="physical")# one to one单层关系正查反查：hostinfo = HostInfo.objects.get(hostname="test")host = hostinfo.hosthost = Host.objects.get(name="company-test1")hostinfo = host.hostinfo #onetoone反查使用反查对象类名首字母小写可获取到对象]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django ORM对象Json序列化问题]]></title>
    <url>%2F2018%2F01%2F31%2FDjango%20ORM%E5%AF%B9%E8%B1%A1Json%E5%BA%8F%E5%88%97%E5%8C%96%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[碰到了一个问题：在使用json.dumps()序列化Django ORM的Queryset对象，传递给前端的时候，程序报错： Object of type &#39;QuerySet&#39; is not JSON serializable 在python 中，常用的json 的序列化是从simplejson 基础上改变而来。这个json 包主要提供了dump,load 来实现dict 与 字符串之间的序列化与反序列化，这很方便的可以完成，但现在的问题是，这个json包不能序列化 django 的models 里面的对象的实例。经过一番度娘搜索，发现有如下解决方案：使用django.core自带的serializers模块： #django ORM的 Queryset对象默认无法被直接json.dumps()序列化，django.core提供的serializers模块提供将其序列化成str类型 #的功能，serializers处理后，再次json.dumps传给前端，前端需要经过两次json.Parse()处理，才能得到原对象类型，但是格式发 #生了变化，需要按新的方式取索引.例如：obj[&apos;pk&apos;]取主键，obj[&apos;fields&apos;][&quot;caption&quot;]取obj的caption字段 由QuerySet:[&lt;Business: Business object&gt;] 变为了： [{&quot;model&quot;: &quot;cmdb.business&quot;, &quot;pk&quot;: 1, &quot;fields&quot;: {&quot;caption&quot;: &quot;develop&quot;}}] 这样前端就可以正常获取数据了，只不过此字段需要两次json.Parse()处理。 至于使用models.Host.objects.get(id=xx)的方式获取到单个对象，而非Queryset对象，serializers默认也无法处理的问题，可以自定义json方法来实现dumps序列化 json默认只支持python原生的list、tuple、dict数据类型对象的序列化，若需要扩展其他类型对象的序列化功能，可以这样修改： 123456789101112131415161718192021222324252627import json as default_jsonfrom json.encoder import JSONEncoderclass BaseResponse(object): def __init__(self): self.status = True self.message = None self.data = None self.error = Noneo=BaseResponse()class JsonCustomEncoder(JSONEncoder): def default(self, o): if isinstance(o, BaseResponse): return o.__dict__ return JSONEncoder.default(self, o)o1=json.dumps(o,cls=JsonCustomEncoder)&gt;&gt;&gt; print(o1)&#123;"message": null, "error": null, "data": null, "status": true&#125;&gt;&gt;&gt; print(type(o1))&lt;class 'str'&gt;#在序列化时指定cls参数，cls=自定义的序列化类，在自定义序列化类的default方法中判断，如果是指定的类的实例的话，则将该类转换成dict格式返回，若指定类的实例，则使用json模块默认的序列化方法。最终得到的return值为str类型。 参考链接：Django model,QuerySet 序列化成json的方法]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bootstrap select组件]]></title>
    <url>%2F2018%2F01%2F30%2Fbootstrap%20select%E7%BB%84%E4%BB%B6.html</url>
    <content type="text"><![CDATA[一、组件开源地址以及API说明bootstrap-select开源地址：https://github.com/silviomoreto/bootstrap-select bootstrap-select使用示例：http://silviomoreto.github.io/bootstrap-select/examples/ bootstrap-select文档说明：http://silviomoreto.github.io/bootstrap-select/options/ 二、组件效果示例1、引入文件bootstrap-select给予bootstrap，boostrap又基于jquery，因此需引入如下文件，同时bootstrap-select需用到bootstrap/fonts路径下的图标，因此也需导入该目录。 12345678&lt;!-- Latest compiled and minified CSS --&gt;&lt;link rel="stylesheet" href="/static/bootstrap/css/bootstrap.min.css"/&gt;&lt;link rel="stylesheet" href="/static/bootstrap/css/bootstrap-select.min.css"&gt; &lt;!-- Latest compiled and minified JavaScript --&gt;&lt;script src="/static/jquery-1.12.4.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;script src="/static/bootstrap/js/bootstrap-select.min.js"&gt;&lt;/script&gt; 三.基础示例 单选： 1234567&lt;select class="selectpicker"&gt; &lt;option value="1"&gt;广东省&lt;/option&gt; &lt;option value="2"&gt;广西省&lt;/option&gt; &lt;option value="3"&gt;福建省&lt;/option&gt; &lt;option value="4"&gt;湖南省&lt;/option&gt; &lt;option value="5"&gt;山东省&lt;/option&gt; &lt;/select&gt; 多选：给一个select标签加上selectpicker样式即可。 1234567&lt;select class="selectpicker" multiple&gt; &lt;option value="1"&gt;广东省&lt;/option&gt; &lt;option value="2"&gt;广西省&lt;/option&gt; &lt;option value="3"&gt;福建省&lt;/option&gt; &lt;option value="4"&gt;湖南省&lt;/option&gt; &lt;option value="5"&gt;山东省&lt;/option&gt; &lt;/select&gt; 四、其他效果示例 选项搜索功能： 增加属性：data-live-search=”true”1234567&lt;select class="selectpicker" multiple data-live-search="true"&gt; &lt;option value="1"&gt;广东省&lt;/option&gt; &lt;option value="2"&gt;广西省&lt;/option&gt; &lt;option value="3"&gt;福建省&lt;/option&gt; &lt;option value="4"&gt;湖南省&lt;/option&gt; &lt;option value="5"&gt;山东省&lt;/option&gt; &lt;/select&gt; 选项分组 1234567891011121314151617&lt;select class="form-control selectpicker" data-live-search="true" multiple&gt; &lt;optgroup label="广东省"&gt; &lt;option value="1"&gt;广州市&lt;/option&gt; &lt;option value="2"&gt;深圳市&lt;/option&gt; &lt;option value="3"&gt;珠海市&lt;/option&gt; &lt;/optgroup&gt; &lt;optgroup label="广西"&gt; &lt;option value="1"&gt;南宁市&lt;/option&gt; &lt;option value="2"&gt;柳州&lt;/option&gt; &lt;option value="3"&gt;桂林市&lt;/option&gt; &lt;/optgroup&gt; &lt;optgroup label="山东"&gt; &lt;option value="1"&gt;烟台&lt;/option&gt; &lt;option value="2"&gt;青岛&lt;/option&gt; &lt;option value="3"&gt;济南&lt;/option&gt; &lt;/optgroup&gt; &lt;/select&gt; 设置最多选项：data-max-options=”2”1234567&lt;select class="selectpicker" multiple data-live-search="true" data-max-options="2"&gt; &lt;option value="1"&gt;广东省&lt;/option&gt; &lt;option value="2"&gt;广西省&lt;/option&gt; &lt;option value="3"&gt;福建省&lt;/option&gt; &lt;option value="4"&gt;湖南省&lt;/option&gt; &lt;option value="5"&gt;山东省&lt;/option&gt; &lt;/select&gt; 选项显示为带颜色的标签 123456 &lt;select class="form-control selectpicker" title="请选择省份" multiple&gt; &lt;option data-content="&lt;span class='label label-success'&gt;广东省&lt;/span&gt;"&gt;广东省&lt;/option&gt; &lt;option data-content="&lt;span class='label label-info'&gt;广西省&lt;/span&gt;"&gt;广西省&lt;/option&gt; &lt;option data-content="&lt;span class='label label-warning'&gt;福建省&lt;/span&gt;"&gt;福建省&lt;/option&gt; &lt;option data-content="&lt;span class='label label-danger'&gt;山东省&lt;/span&gt;"&gt;山东省&lt;/option&gt; &lt;/select&gt; 缩略框颜色样式： 12345678910111213141516171819&lt;select class="selectpicker" data-style="btn-primary"&gt; ...&lt;/select&gt;&lt;select class="selectpicker" data-style="btn-info"&gt; ...&lt;/select&gt;&lt;select class="selectpicker" data-style="btn-success"&gt; ...&lt;/select&gt;&lt;select class="selectpicker" data-style="btn-warning"&gt; ...&lt;/select&gt;&lt;select class="selectpicker" data-style="btn-danger"&gt; ...&lt;/select&gt; 五、取值赋值：取值：关于组件取值保持原生的jquery方法，比如 var value = $(‘#sel’).val(); 这样是不是很简单，需要注意的是，如果是多选，这里得到的value变量是一个数组变量，形如 [‘1’,’2’,’3’]。 赋值 组件赋值就需要稍微变换一下了，如果你直接 $(‘#sel’).val(‘1’); 这样赋值将会无效，正确的赋值方法为： 12$('.selectpicker').selectpicker('val', '1');$('.selectpicker').selectpicker('val', ['1','2','3']) //多个选项赋值 在一些级联选择的使用场景中，经常需要在赋值的时候顺便触发一下组件的change事件，我们可以这么做。 1$('.selectpicker').selectpicker('val', '1').trigger("change"); 如果是多选的赋值，也是一样:1$('.selectpicker').selectpicker('val', ['1','2','3']).trigger("change"); 六、组件其他用法 1234567891011121314151617全选： $('.selectpicker').selectpicker('selectAll'); 反选： $('.selectpicker').selectpicker('deselectAll'); 适应手机模式： $('.selectpicker').selectpicker('mobile'); 组件禁用：$('.disable-example').prop('disabled', true);$('.disable-example').selectpicker('refresh');组件启用：$('.disable-example').prop('disabled', false);$('.disable-example').selectpicker('refresh');组件销毁：$('.selectpicker').selectpicker('destroy'); 以编程方式切换bootstrap-select菜单的打开/关闭。 1$('.selectpicker').selectpicker('toggle') 以编程方式隐藏bootstrap-select使用hide方法（这仅影响bootstrap-select自身的可见性）。1$('.selectpicker')..selectpicker('hide') 本文转载至：JS组件系列——再推荐一款好用的bootstrap-select组件，亲测还不错 手动mark一下，留后用. 补充：组件操作后刷新：1$('.selectpicker').selectpicker('refresh'); //使用JS更新该selectpicker对象的内容后，需用此方法手动刷新探测 select点击按钮（即下拉框展开）事件： 1$('#business').on('shown.bs.select',function () &#123;&#125; //可使用ajax动态获取option]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax使用方法]]></title>
    <url>%2F2018%2F01%2F30%2FAjax%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95.html</url>
    <content type="text"><![CDATA[前言在提及ajax之前，得先提及js以及jquery，关于jquery API调用方式，参考此站，收录齐全，查起来很方便：jQuery API 速查表 Form提交： 传统的form提交，在用户点击form内的submit按钮后，将整个form表单内的内容全部提交至后台，获取响应并刷新页面。 Ajax提交 （Asynchronous Javascript And XML）（异步 JavaScript 和 XML）1.Ajax 是一种用于创建快速动态网页的技术。2.Ajax 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。3.通过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 Ajax使用方式1.ajax原生用法123456789101112131415161718192021$('#send').click(function()&#123; //请求的5个阶段，对应readyState的5种值 //0: 未初始化，send方法未调用； //1: 正在发送请求，send方法已调用； //2: 请求发送完毕，send方法执行完毕； //3: 正在解析响应内容； //4: 响应内容解析完毕； var data = 'name=yang'; var xhr = new XMLHttpRequest(); //创建一个ajax对象 xhr.onreadystatechange = function(event)&#123; //对ajax对象进行监听 if(xhr.readyState == 4)&#123; //4表示解析完毕 if(xhr.status == 200)&#123; //200为正常返回 console.log(xhr) &#125; &#125; &#125;; xhr.open('POST','url',true); //建立连接，参数一：发送方式，二：请求地址，三：是否异步，true为异步 xhr.setRequestHeader('Content-type','application/x-www-form-urlencoded'); //可有可无 xhr.send(data); //发送&#125;); 原生方式使用起来事实上并不是很便利，jquery也封装了ajax方法，推荐使用jquery ajax 2.JQuery ajax用法前端jquery ajax：12345678910111213141516171819$('button[id="AsInfo_sub"]').click(function()&#123; $.ajax(&#123; url:"/cmdb_modify", data:$('#AssetInfo').serialize(), //传输给后台的源数据,以dict&#123;k:v&#125;的格式传输 type:'POST', dataType:'JSON', //将接收到的数据进行json Parse处理，等同于JSON.parse(rdata) traditional: true, //传输给后台的数据可以包含集合list类型，例如复选框传入的多个数据，为[]类型 beforeSend:function()&#123; $("#AsInfo_sub").attr(&#123;"disabled":"disabled"&#125;) &#125;, //数据向后台提交成功前执行的回调函数 success: function(rdata)&#123; alert("提交成功"); location.reload() &#125;, //成功接收到后台返回数据后执行的回调函数，形参rdata用来接收后台传回的数据 error: function () &#123; $('#AssetInfo .error_info').text(rdata['error']) &#125; //未能成功接收到后台返回数据后执行的回调函数 &#125;)&#125;); Django后端处理请求函数：123456789101112131415161718192021222324252627def modify(request): res=&#123;"status":True,"error":False,"data":None&#125; if request.method=='POST': try: obj_info=&#123; 'hostname' : request.POST.get("hostname"), 'host_ip' : request.POST.get("host_ip"), 'owner_name' : request.POST.get("owner_name"), 'owner_id' : request.POST.get("owner_id"), 'asset_tag' : request.POST.get("asset_tag"), 'online_time' : request.POST.get("online_time"), 'OS' : request.POST.get("OS"), 'OS_type' : request.POST.get("OS_type"), 'off_time' : request.POST.get("off_time"), 'SN' : request.POST.get("SN"), 'Brand' : request.POST.get("Brand"), 'memory' : int(request.POST.get("memory")), 'CPU_num' : int(request.POST.get("CPU_num")), &#125; nid=request.POST.get('Asset_ID') models.Host.objects.filter(id=int(nid)).update(**obj_info) except Exception as e: res['error']=e res['status']=False return HttpResponse(json.dumps(res)) 因ajax不刷新页面的特性，因此返回http请求时，推荐使用Httpresponse(str)的方式,若返回的数据为dict格式，可使用json处理后返回给前端，前端再反处理为JSON格式使用。 注意：针对ajax请求，后端即使使用render、redirect等方式，前端也不会刷新、跳转页面，若有此需求，可在前端回调函数内手动执行。]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（五）、ORM基础]]></title>
    <url>%2F2018%2F01%2F29%2FDjango%EF%BC%88%E4%BA%94%EF%BC%89%E3%80%81ORM%E5%9F%BA%E7%A1%80.html</url>
    <content type="text"><![CDATA[前言在Python中，最知名的ORM非SQLAlchemy莫属了，同时，Django作为一款功能丰富的框架，其中也内置了专属的ORM，其语法逻辑与SQLAlchemy十分相似，有过SQLAlchemy使用经验的可以很容易上手。SQLAlchemy基础使用参考：Python ORM ：SQLAlchemy基础使用 一、常用字段类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071AutoField(Field) - int自增列，必须填入参数 primary_key=TrueBigAutoField(AutoField) - bigint自增列，必须填入参数 primary_key=True - SmallIntegerField(IntegerField): - 小整数 -32768 ～ 32767PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField) - 正小整数 0 ～ 32767IntegerField(Field) - 整数列(有符号的) -2147483648 ～ 2147483647PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField) - 正整数 0 ～ 2147483647BigIntegerField(IntegerField): - 长整型(有符号的) -9223372036854775808 ～ 9223372036854775807BooleanField(Field) - 布尔值类型NullBooleanField(Field): - 可以为空的布尔值CharField(Field) - 字符类型 - 必须提供max_length参数， max_length表示字符长度TextField(Field) - 文本类型FileField(Field) - 字符串，路径保存在数据库，文件上传到指定目录 - 参数： upload_to = "" 上传文件的保存路径 storage = None 存储组件，默认django.core.files.storage.FileSystemStorageImageField(FileField) - 字符串，路径保存在数据库，文件上传到指定目录 - 参数： upload_to = "" 上传文件的保存路径 storage = None 存储组件，默认django.core.files.storage.FileSystemStorage width_field=None, 上传图片的高度保存的数据库字段名（字符串） height_field=None 上传图片的宽度保存的数据库字段名（字符串）DateTimeField(DateField) - 期+时间格式 YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]DateField(DateTimeCheckMixin, Field) - 期格式 YYYY-MM-DDTimeField(DateTimeCheckMixin, Field) - 时间格式 HH:MM[:ss[.uuuuuu]]DurationField(Field) - 长整数，时间间隔，数据库中按照bigint存储，ORM中获取的值为datetime.timedelta类型FloatField(Field) - 浮点型DecimalField(Field) - 10进制小数 - 参数： max_digits，小数总长度 decimal_places，小数位长度BinaryField(Field) - 二进制类型 二、常见字段参数 1234567891011121314151617181920 null 数据库中字段是否可以为空 db_column 数据库中字段的列名 db_tablespace default 数据库中字段的默认值 primary_key 数据库中字段是否为主键 db_index 数据库中字段是否可以建立索引 unique 数据库中字段是否唯一 unique_for_date 数据库中字段【期】部分是否唯一 unique_for_month 数据库中字段【/】部分是否唯一 unique_for_year 数据库中字段【/】部分是否唯一 注意：当unique 为 True时，你不需要再指定db_index创建索引，因为unique暗示创建索引 verbose_name Admin中显示的字段名称 blank Admin中是否允许用户输入为空 editable Admin中是否可以编辑 help_text Admin中该字段的提示信息choices Admin中显示选择框的内容，较少且几乎不变动的数据，可以将其放在程序内存 中，不用单独创建一张表从而避免跨表操作 如：type = models.IntegerField(choices=[(0, 'man'),(1, 'woman'),],default=1) 三、表结构创建在django程序子目录或主目录创建models.py文件，在该文件内定义表结构，比如创建一个主机表，创建方式如下： 12345678910111213141516171819from django.db import models# Create your models here.class Host(models.Model): id = models.AutoField(primary_key=True) hostname = models.CharField(max_length=16,db_index=True) host_ip = models.GenericIPAddressField(db_index=True) owner_name=models.CharField(max_length=16) owner_id = models.CharField(max_length=16) asset_tag=models.CharField(max_length=16) online_time=models.DateField(default=None) OS = models.CharField(max_length=16) OS_type=models.CharField(max_length=16) off_time=models.DateField(default=None) SN=models.CharField(max_length=32) Brand=models.CharField(max_length=8) memory=models.SmallIntegerField(default=None) CPU_num=models.IntegerField(default=None) cmd输入: 12python manage.py makemigrations #检测表结构变化python manage.py migrate #表结构构建 django默认使用的是自带的sqlite，使用Navicat连接此sqlite路径，登录查看表结构： 四、增删改查在前端写几个简单的增删改查的表格，往后端提交数据，后端在同级目录views.py中引入model.py对前端请求进行处理： 后端views.py：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from cmdb import models#增def addhost(request): ref=request.META.get('HTTP_REFERER') #获取request referer来源url print(request.POST) if request.method == "POST": #方式一： #obj=models.Host(hostname='',host_ip='xxx',...) #obj.save() #方式二： #models.Host.objects.create(hostname="xxx",host_ip='xxx',...) #直接加，各字段直接传参，推荐使用 #方式三：各字段名作key，值作value，构建一个dict，使用models.Host.objects.create(**dict)的方式创建，推荐字段较多时使用。 obj_info=&#123; 'hostname' : request.POST.get("hostname"), 'host_ip' : request.POST.get("host_ip"), 'owner_name' : request.POST.get("owner_name"), 'owner_id' : request.POST.get("owner_id"), 'asset_tag' : request.POST.get("asset_tag"), 'online_time' : request.POST.get("online_time"), 'OS' : request.POST.get("OS"), 'OS_type' : request.POST.get("OS_type"), 'off_time' : request.POST.get("off_time"), 'SN' : request.POST.get("SN"), 'brand' : request.POST.get("brand"), 'memory' : int(request.POST.get("mem")), 'CPU_num' : int(request.POST.get("CPU_num")), &#125; models.Host.objects.create(**obj_info) return redirect(ref) #查def search(request): if request.method=="GET": Owner=request.GET.get('Owner',None) print(Owner) sear_res=models.Host.objects.filter(owner_name=Owner) print(sear_res) #查询出来的结果是一个queryset集合的格式，当做list格式理解 return render(request,'cmdb_index.html',&#123;"search_result":sear_res&#125;)#删def delete(request): ref=request.META.get('HTTP_REFERER') #获取request referer来源url nid=int(request.POST.get('nid')) models.Host.objects.filter(id=nid).delete() #query then delete return redirect(ref)#改def modify(request): res=&#123;"status":True,"error":False,"data":None&#125; ref=request.META.get('HTTP_REFERER') #获取request referer来源url if request.method=='POST': try: obj_info=&#123; 'hostname' : request.POST.get("hostname"), 'host_ip' : request.POST.get("host_ip"), 'owner_name' : request.POST.get("owner_name"), 'owner_id' : request.POST.get("owner_id"), 'asset_tag' : request.POST.get("asset_tag"), 'online_time' : request.POST.get("online_time"), 'OS' : request.POST.get("OS"), 'OS_type' : request.POST.get("OS_type"), 'off_time' : request.POST.get("off_time"), 'SN' : request.POST.get("SN"), 'Brand' : request.POST.get("Brand"), 'memory' : int(request.POST.get("memory")), 'CPU_num' : int(request.POST.get("CPU_num")), &#125; nid=request.POST.get('Asset_ID') models.Host.objects.filter(id=int(nid)).update(**obj_info) #查询然后更新，即修改 except Exception as e: res['error']=e res['status']=False return HttpResponse(json.dumps(res)) 前端代码就不贴了，涉及到一些ajax的操作，后面再整理一下ajax的基本操作.]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（四）、URL路由系统]]></title>
    <url>%2F2018%2F01%2F17%2FDjango%EF%BC%88%E5%9B%9B%EF%BC%89%E3%80%81URL%E8%B7%AF%E7%94%B1%E7%B3%BB%E7%BB%9F.html</url>
    <content type="text"><![CDATA[前言接前几节内容，django处理一次请求完整的生命周期如下： -&gt; URL对应关系（匹配） -&gt; 视图函数处理 -&gt; 返回用户字符串或html等内容其中第一个，即是url匹配，在django中，支持多种匹配方式，下面罗列整理一下 一、基础匹配 12url(r'^index/', views.index),url(r’^login/’, common_view.login) 参考：Django（二）、Http请求处理，此处不再赘述。此方式的匹配逻辑比较简单，为了满足更多的需求，因此引入正则匹配 二、正则匹配在django urls匹配法则中，默认将链接中匹配到的元素提取出来，作为参数，传递给视图函数处理，可以提取多个匹配元素，按照顺序作为参数传递，也可以分别多参数进行命名，这样就无需考虑传参顺序了。 例如：在页面中点击”详细信息”查看更多urls.py写法：12url(r'^index_test/', enginetest_views.index_test), url(r'^index-detail-(\d+).html', enginetest_views.index_detail), 视图函数写法： 1234567infodict=&#123;"id":1,"name":"alice","age":"20","email":"a@xxx.com"&#125;def index_test(request): return render(request,"index_test.html",&#123;"info":infodict&#125;)def index_detail(request,uid): return render(request,"index_detail.html",&#123;"info_obj":infodict&#125;) 对应模板页面：index_test.html： 1234567891011121314&lt;h2&gt;INFO:&lt;/h2&gt;&lt;table&gt; &lt;tr&gt; &lt;td&gt;id&lt;/td&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&#123;&#123; info.id &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; info.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&lt;a href="/index-detail-&#123;&#123; info.id &#125;&#125;.html"&gt;更多信息&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; index_detail.html： 1234567&lt;h1&gt;INFO DETAIL&lt;/h1&gt;&lt;ul&gt; &lt;li&gt;&#123;&#123; info_obj.id &#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123; info_obj.name &#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123; info_obj.age &#125;&#125;&lt;/li&gt; &lt;li&gt;&#123;&#123; info_obj.email &#125;&#125;&lt;/li&gt;&lt;/ul&gt; 效果如下： 点击“更多信息” 这里可以发现：url(r&#39;^index-detail-(\d+).html&#39;, enginetest_views.index_detail),中，(\d+)匹配到的数字，传递给了视图函数index_detail(request,uid)中的形参uid，作为其实参。 若匹配到的元素有多个，则依次作为参数传递给视图函数。例如：url(r&#39;^index-detail-(\d+)-(\d+).html&#39;, enginetest_views.index_detail), 则函数需接收两个参数：def index_detail(request,uid,nid)当参数的个数更多时，可以写作：def index_detail(request,*args,**kwargs)` 但以上方式接收函数，传递参数的顺序及接收参数的实参顺序必须一一对应，因此，为了避免顺序混乱造成的影响，建议使用如下方式，在urls绑定时，予以声明，例如: 123456789url(r'^index-detail-(?P&lt;uid&gt;\d+)-(?P&lt;nid&gt;\d+).html', views.detail)#这样视图函数在接收参数时即可根据形参名称对应关系来接收，不用顾忌次序问题def func(request, nid, uid): passdef func(request, *args): args = (x,x) def funct(request,*args,**kwargs): kwargs = &#123;'nid': x, 'uid': x&#125; 三、多级路由在项目中，一个项目可能包含多个子程序，若将所有的路由绑定全部写在主程序的目录中，大量的路由绑定条目会使程序看起来比较臃肿，不利于分工合作。因此，django也支持多级路由，首先在子程序目录中创建urls.py文件，在主程序中指定某条件的路由，全部跳转向二级子程序urls.py中，再在子程序中写明细路由。例如：主程序urls.py： 123456from django.conf.urls import url,includefrom django.contrib import adminurlpatterns = [ url(r'^cmdb',include("cmdb.urls")), ] 子程序cmdb/urls.py: 123456789#Author :ywqfrom django.conf.urls import url,includefrom django.contrib import adminfrom cmdb import views as cmdb_viewurlpatterns = [ url(r'^/', cmdb_view.index), url(r'^_search/', cmdb_view.index_search), ] 这样，django在匹配url时，会将主程序，以及子程序的匹配条件进行拼接，主程序的条件是’^cmdb’，子程序的条件是’/‘，’^_search/‘，那么合并就是’^cmdb/‘,’^cmdb_search/‘]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（三）、模板引擎渲染]]></title>
    <url>%2F2018%2F01%2F15%2FDjango%EF%BC%88%E4%B8%89%EF%BC%89%E3%80%81%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E%E6%B8%B2%E6%9F%93.html</url>
    <content type="text"><![CDATA[前言：在第一篇中已经了解到，django支持将html作为模板放在指定的路径中（templates）。同时，django模板引擎可以动态地获取数据，来渲染模板网页文件，实现动态效果。 一、.变量替换：django的模板引擎支持使用的方式来实现变量名替换，变量需要以dict的数据格式包裹返回给django引擎来进行渲染。举例： 1.python manage.py startapp enginetest 2.在templates里写一个login_test.html文件：1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="/login_test/" method="get"&gt; &lt;input type="text" name="info" placeholder="info"&gt; &lt;p&gt;&lt;h2&gt;INFO:&#123;&#123; info &#125;&#125;&lt;/h2&gt;&lt;/p&gt; &lt;input type="submit"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 3.在enginetest/views.py里添加函数： 123456def login_test(request): info="" if request.method=='GET': info=request.GET.get('info') print('123',info) return render(request,"login_test.html",&#123;'info':info&#125;) 4.urls.py文件里添加： 12from enginetest import views as enginetest_viewsurl(r'^login_test/', enginetest_views.login_test), 打开http://127.0.0.1:8000/login_test，默认为空： 输入任意内容，点击提交： 可以发现，django将html模板中的值，替换成了函数中返回的字典中的变量的值。 二、变量for循环：当处理请求的函数返回的数据需渲染为循环展示时，可以使用for循环。 1.在templates里写一个index_test.html文件 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;INFO:&lt;/h2&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;email&lt;/td&gt; &lt;/tr&gt; &#123;% for row in info %&#125; //模板中的变量名，绑定的是函数传递过来的dict中的key name. &lt;tr&gt; &lt;td&gt;&#123;&#123; row.name &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; row.email &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 2.views.py定义函数： 1234def index_test(request): infolist=[&#123;"name":"alice","email":"a@xxx.com"&#125;,&#123;"name":"bob","email":"b@xxx.com"&#125;] return render(request,"index_test.html",&#123;"info":infolist&#125;) 3.urls.py添加绑定 1url(r'^index_test/', enginetest_views.index_test), 4.打开http://127.0.0.1:8000/index_test 以上使用的infolist为list格式，当使用dict格式时，可以在模板语言中这样写来实现循环，例如：5.函数中的数据格式改为dict：1234def index_test(request): infodict=&#123;"alice":&#123;"age":"20","email":"a@xxx.com"&#125;,"bob":&#123;"age":"25","email":"b@xxx.com"&#125;&#125; return render(request,"index_test.html",infodict) 6.html模板语言改写： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;INFO:&lt;/h2&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;age&lt;/td&gt; &lt;td&gt;email&lt;/td&gt; &lt;/tr&gt; &#123;% for key,value in info.items %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; key &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; value.age &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; value.email &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/table&gt; ------------------------------------------------------ &lt;table&gt; &lt;tr&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;age&lt;/td&gt; &lt;td&gt;email&lt;/td&gt; &lt;/tr&gt; &#123;% for key in info.keys %&#125; &lt;tr&gt; &lt;td&gt;&#123;&#123; key &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/table&gt; ------------------------------------------------------ &lt;table&gt; &lt;tr&gt; &lt;td&gt;name&lt;/td&gt; &lt;td&gt;age&lt;/td&gt; &lt;td&gt;email&lt;/td&gt; &lt;/tr&gt; &#123;% for value in info.values %&#125; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&#123;&#123; value.age &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; value.email &#125;&#125;&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 7.打开http://127.0.0.1:8000/index_test： 可以发现，无论是key，或是value，都可以单独for循环，也可以一起循环。需要注意的是，在python中，list和dict按索引取元素的方式，例如：list[0]取第二个元素，dict[‘k1’]取key为k1的元素的value，但在模板语言中，统一变为list.0,dict.k1，且无需引号，这一点要注意。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（二）、Http请求处理]]></title>
    <url>%2F2018%2F01%2F15%2FDjango%EF%BC%88%E4%BA%8C%EF%BC%89%E3%80%81Http%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[前言最常见的http请求method，是get和post方式，除此之外还有head\option\delete\put\patch等方式。默认输入url的请求方式即为get，当请求方式为get时，若带有需要传递的其他参数，则会将该参数的name和value补充在url中一并提交，post方式则将数据放在内容中一并提交，不再url中显示。在浏览器发送的request数据包的header中，可以看到这些内容，例如： 那么在django中，是如何获取这些请求的方式和如何定义处理这些请求的呢？现在来分析上一节（ Django（一）、基本使用）的内容。 一、FBV方式处理请求模板页面： 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action="/login_test/" method="post"&gt; &lt;input type="text" name="info" placeholder="info"&gt; &lt;input type="submit"&gt; &lt;/form&gt; &lt;p&gt;&lt;h2&gt;INFO:&#123;&#123; info &#125;&#125;&lt;/h2&gt;&lt;/p&gt; &lt;p&gt;&lt;h2&gt;Method:&#123;&#123; method &#125;&#125;&lt;/h2&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 12345def login_test(request): info="" if request.method=='POST': info=request.POST.get('info') return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;) 客户端的请求，作为实参传递给形参request，request.method可以获取该请求header中的method方式，reques.POST.get(‘info’)可以获取客户端通过POST方式传递过来的name为info的key对应的value。 在urls.py中绑定该函数1url(r'^index_test/', enginetest_views.index_test), 打开http://127.0.0.1:8000//login_test/默认如下：输入任意值点击提交后如下： TIPS：当前端为复选框，向后端Django传递的值有多个时，可以通过request.POST[GET].getlist(‘KEY’)的方式，获取值的数组，格式为[VALUE1,VALUE2..] 以上使用的是函数绑定url，即FBV（function base view）方式处理请求，url绑定放在views.py文件中的函数,绑定的写法为：路径→函数，url(r’^login/‘, common_view.login)同时，django支持CBV（class base view）方式处理请求。 二、CBV的方式处理请求views.py中的类写法： 12345678910111213141516from django.views import View #引入django View父类# Create your views here.class Login_test(View): def get(self,request): info="" return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;) def post(self,request): info=request.POST.get('info') return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;)# def login_test(request):# info=""# if request.method=='POST':# info=request.POST.get('info')# return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;) urls.py绑定写法：12# url(r'^login_test/', enginetest_views.login_test),url(r'login_test/',enginetest_views.Login_test.as_view()) //将上方注释，替换为此句 实现效果和上方FBV一致。原理介绍： 12345678910111213141516171819'''以上FBV方式，function base view，url绑定放在views.py文件中的函数,绑定的写法为：路径→函数，url(r'^login/', common_view.login),CBV方式，class base view，自定义个类，继承django.view中的View父类，绑定写法为：路径→类，url(r'^login/', common_view.Test.as_view()),使用FBV的方式，各请求方法对应的逻辑需要自己判断编写，使用CBV的方式，由django中的View类中的dispatch方法来判断请求method，并交给对应method的函数来处理请求。使用CBV时，可以自定义一些操作，例如在接收到请求处理之前，和请求处理完毕答复之前，都可以进行一些操作。''''''#dispatch是django处理请求时优先执行的函数，该函数接收并分析请求头部中的方法字段，根据请求中的方法，使用getattr，将请求交给相应的函数去处理。参考View.dispatch源码： def dispatch(self, request, *args, **kwargs): # Try to dispatch to the right method; if a method doesn't exist, # defer to the error handler. Also defer to the error handler if the # request method isn't on the approved list. if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed return handler(request, *args, **kwargs)''' 使用CBV，可以自定义或插入添加一些功能，例如简单修改View.dispatch方法，在接收到请求和回应请求之后分别输出打印： 1234567891011121314class Login_test(View): def dispatch(self, request, *args, **kwargs): print('****,recv request') res=super(Login_test,self).dispatch(request, *args, **kwargs) #继承父类的方法处理请求，res接收父类方法return值。 print('****,return request') return res def get(self,request): info="" return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;) def post(self,request): info=request.POST.get('info') print(info) return render(request,"login_test.html",&#123;'info':info,'method':request.method&#125;) 在客户端访问http://127.0.0.1:8000/login_test/时，查看console的输出，可以看到记录： 123****,recv request****,return request[15/Jan/2018 15:00:45] "GET /login_test/ HTTP/1.1" 200 327 总结：FBV和CBV两种方式没有孰优孰劣，只是CBV方式有着更高的自定义性，可以实现更多的功能。]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django（一）、基本使用]]></title>
    <url>%2F2018%2F01%2F03%2FDjango%EF%BC%88%E4%B8%80%EF%BC%89%E3%80%81%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[一、安装pip install django 二、创建项目进入目标路径，执行：django-admin startproject xxx(项目名称)默认创建的目录结构： 1234567891011mysite - mysite # 对整个程序进行配置 - init - settings # 配置文件 - url # URL对应关系 - wsgi # 遵循WSIG规范，uwsgi + nginx - manage.py # 管理Django程序： - python manage.py - python manage.py startapp xxx #创建子app（模块） - python manage.py makemigrations #当有表结构变化时刷新表结构 - python manage.py migrate #初始化表结构 运行django： 1python manage.py runserver 127.0.0.1:8000 #地址和端口默认为本机8000端口时，可省略 以上操作也可以在pycharm上进行。 三、基础操作1.创建后台管理账户python manage.py migrate #初始化表结构python manage.py createsuperuser #按照提示输入，email按回车直接跳过 创建完成后，在浏览器输入：127.0.0.1:8000/admin可进入Django自带的后台管理page 2.添加页面每增加一个page时（例如增加login页面）：一、写一个html page，创建templates目录，在templates目录下存放创建html文件：二、创建static目录，将html文件所需的静态文件放在这个目录下，例如css、js、图片文件三、定义函数，在相应的app的views.py写一个处理/login路径请求的函数 例如：123456def login(request): #f=open('template/login.html','rb') #data=f.read() #return HttpResponse(data) return render(request,'login.html') #配置好temlpate路径后，可以使用render来简写以上内容，render实际是对httpresponse的封装，实现代码的简写，功能一致。 四、在urls文件中增加规则，例如增加登陆页面login： 12345 urlpatterns = [ path('admin/', admin.site.urls), path('cmdb/',views.home), path('login/',views.login),] 五、在setting中修改配置文件添加template路径、static路径，关闭csrf保护 12345678910111213141516TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 1234STATIC_URL = '/static/'STATICFILES_DIRS = ( os.path.join(BASE_DIR, 'static'),) 添加templates路径，django会在setting配置文件里加载html模板文件的路径，并使用模板语言对模板进行渲染，这个下篇再讲。添加static路径，是为了避免客户端在请求html内的静态文件时，urls.py中无对应处理静态文件路径的方法的问题，否则，读取静态文件会报404。 123456789MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] CSRF： Cross Site Request Forgery, 跨站点伪造请求，django默认开启此保护机制，在涉及此知识点之前，首先注释掉这行代码，否则会报403。 最后：一个基础的django程序就可以运行起来了，cmd输入python manage.py runserver,浏览器输入：127.0.0.1:8000/login：]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS：Dom的基本使用]]></title>
    <url>%2F2017%2F12%2F28%2FJS%EF%BC%9ADom%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[前言：文档对象模型（Document Object Model，DOM）是一种用于HTML和XML文档的编程接口。它给文档提供了一种结构化的表示方法，可以改变文档的内容和呈现方式。我们最为关心的是，DOM把网页和脚本以及其他的编程语言联系了起来。DOM属于浏览器，而不是JavaScript语言规范里的规定的核心内容。 一、查找对象（元素）1、直接查找document.getElementById 根据ID获取一个标签document.getElementsByName 根据name属性获取标签集合document.getElementsByClassName 根据class属性获取标签集合document.getElementsByTagName 根据标签名获取标签集合 因为标签的tag id是唯一的，所以第一种方式获取到的元素是唯一的对象，后三种获取到的对象是一个数组（list），需要用for循环来操作。 2、间接关系查找parentNode // 父节点（标签）childNodes // 所有子节点firstChild // 第一个子节点lastChild // 最后一个子节点nextSibling // 下一个兄弟节点previousSibling // 上一个兄弟节点 parentElement // 父节点标签元素children // 所有子标签firstElementChild // 第一个子标签元素lastElementChild // 最后一个子标签元素nextElementtSibling // 下一个兄弟标签元素previousElementSibling // 上一个兄弟标签元素 获取到元素之后，为了让它‘动’起来，需要操作元素。 二、操作元素1、内容innerText 标签内的文本内容outerTextinnerHTML HTML内容value 值 以上均可以方法均可以直接查看，也可以使用“=”重新赋值 2、属性 attributes // 获取所有标签属性，可以用”=”重新赋值setAttribute(key,value) // 设置标签属性getAttribute(key) // 获取指定标签属性 123var atr = document.createAttribute("class");atr.nodeValue="democlass";document.getElementById('n1').setAttributeNode(attr); 3、class操作className // 获取所有类名classList.remove(cls) // 删除指定类classList.add(cls) // 添加类 4、其他操作document.geElementById(‘form’).submit() 提交表单 表单提交的默认方式为：在form标签中，submit类的标签点击按钮可以提交form表单内部的所有input text类型标签中的文本，使用Dom操作这个方式，可以给任意标签定义事件触发提交表单动作。 //窗口操作console.log 输出框alert 弹出框confirm 确认框 // URL和刷新location.href 获取URLlocation.href = “url” 重定向location.reload() 重新加载 // 定时器setInterval 多次定时器clearInterval 清除多次定时器setTimeout 单次定时器clearTimeout 清除单次定时器 有了操作，那么就得有触发操作的事件发生. 三、事件各类事件及触发条件： 四、样例： 1.修改标签内容12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="d1"&gt;我是标题&lt;/div&gt; &lt;span&gt;我是子标签&lt;/span&gt; &lt;span&gt;我是子标签&lt;/span&gt; &lt;span&gt;我是子标签&lt;/span&gt; &lt;span&gt;我是子标签&lt;/span&gt; &lt;script&gt; document.getElementById('d1').innerText='我是新标题'; tags=document.getElementsByTagName('span'); //通过标签名称类型来获取标签对象数组 for (var i =0;i&lt;tags.length;i+=1) &#123; tags[i].innerText='新子标题'; &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2.提交栏，点击出现提交栏 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Add Info&lt;/title&gt; &lt;style&gt; .c1&#123; background-color: black; position: fixed; left: 0; right: 0; top: 0; bottom: 0; z-index:2; opacity: 0.4; &#125; .c2&#123; background-color: white; height: 300px; width: 300px; position: fixed; left: 50%; top: 50%; margin-left: -150px; margin-top: -300px; z-index: 3; //层次级别，越大越置顶 &#125; .hide&#123; display: none; &#125; &lt;/style&gt;&lt;/head&gt; &lt;!--显示层--&gt;&lt;body style="margin: 0 auto"&gt; &lt;div id="d1" style="margin-left: 100px;margin-top: 100px"&gt; &lt;input type="button" value="Add" style="background-color: aqua;height: 30px;width: 70px;" onclick="show_tags();"/&gt; &lt;/div&gt; &lt;!--遮罩层--&gt; &lt;div id='d2' class="c1 hide"&gt;&lt;/div&gt; &lt;!--提交层--&gt; &lt;div id='d3' class="c2 hide"&gt; &lt;div&gt;名称：&lt;/div&gt; &lt;div&gt;&lt;input type="text" style="height: 30px;width: 200px;margin-left: 40px"/&gt;&lt;/div&gt; &lt;div&gt;资料：&lt;/div&gt; &lt;div&gt;&lt;input type="text" style="height: 30px;width: 200px;margin-left: 40px"&gt;&lt;/div&gt; &lt;div&gt;备注：&lt;/div&gt; &lt;div&gt;&lt;input type="text" style="height: 30px;width: 200px;margin-left: 40px"&gt;&lt;/div&gt; &lt;div style="margin-top: 20px"&gt; &lt;input type="button" value="submit" onclick="hide_tags();" style="margin-left: 40px"&gt; &lt;input type="button" value="cancel" onclick="hide_tags();" style="margin-right: 40px"&gt; &lt;/div&gt; &lt;/div&gt; &lt;script&gt; function hide_tags() &#123; document.getElementById('d2').classList.add('hide'); document.getElementById('d3').classList.add('hide'); &#125; function show_tags() &#123; document.getElementById('d2').classList.remove('hide'); document.getElementById('d3').classList.remove('hide'); &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 效果： 3.滚动横幅（有关很的low网站内常见） 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="n1" style="font-size: 30px"&gt;欢迎XXX莅临指导&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/div&gt; &lt;script&gt; function run() &#123; var lable_info=document.getElementById("n1"); //根据标签id获取元素div var content=lable_info.innerText; //获取元素标签内的文字内容 var f =content.charAt(0); //获取字符串内的第一个字符 var other = content.substring(1,content.length); //获取第二个字符到最后一个字符 var new_info =other + f; lable_info.innerText= new_info; &#125; setInterval('run()',300); //间隔运行，单位ms &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 效果： 4.创建标签 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt; &lt;input type="button" value="Add" onclick="AddEle1();"/&gt; &lt;input type="button" value="Add" onclick="AddEle2();"/&gt; &lt;/p&gt; &lt;div id="1"&gt; &lt;p&gt;&lt;input type="text" style="height: 30px;width: 200px"/&gt;&lt;/p&gt; &lt;/div&gt; &lt;script&gt; function AddEle1() &#123; ele="&lt;p&gt;&lt;input type='text' style='height: 30px;width: 200px'/&gt;&lt;/p&gt;"; obj=document.getElementById('1'); obj.insertAdjacentHTML("beforeEnd",ele); //直接以字符串的形式添加标签及设定样式,再insertAdjacentHTML("beforeEnd",ele);指定位置，插入元素，位置参数只有 //四个选项，分别是beforeBegin、AfterBegin、beforeEnd、afterEnd &#125; function AddEle2() &#123; tag_input=document.createElement("input"); tag_input.setAttribute("type","text"); tag_input.style.height='30px'; tag_input.style.width="200px"; tag_p=document.createElement('p'); tag_p.appendChild(tag_input); obj=document.getElementById('1'); obj.appendChild(tag_p); //document.createElement方式创建input标签，再设定属性，指定style，再将input标签放入p标签中，最后将p标签放入obj中, &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 5.鼠标悬停变色，挪走还原 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;table border="1px" width="200px"&gt; &lt;tr&gt;&lt;td&gt;a&lt;/td&gt;&lt;td&gt;d&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;b&lt;/td&gt;&lt;td&gt;e&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;c&lt;/td&gt;&lt;td&gt;f&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt; &lt;script&gt; var obj = document.getElementsByTagName('td'); for (var i =0;i&lt;obj.length;i++) &#123; obj[i].onmouseover = function() &#123; this.style.backgroundColor="green"; //this代指obj[i]这个对象，这里不能直接使用obj[i]，会报错 &#125;; obj[i].onmouseout= function() &#123; this.style.backgroundColor=''; &#125; &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 5.练习对照某当网主页，借用了几张图片，写了个相似的页面出来，非广告党 o(￣︶￣)o 使用以上Dom知识点，实现了悬停展开下拉栏、搜索框默认文字鼠标触碰消失、边缘固定返回顶部button等]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python写堡垒机项目]]></title>
    <url>%2F2017%2F12%2F15%2F%E7%94%A8Python%E5%86%99%E5%A0%A1%E5%9E%92%E6%9C%BA%E9%A1%B9%E7%9B%AE.html</url>
    <content type="text"><![CDATA[前言堡垒机是一种运维安全审计系统。主要的功能是对运维人员的运维操作进行审计和权限控制，风险规避。同时堡垒机还有账号集中管理，单点登陆的功能。 堡垒机有以下两个至关重要的功能：集中管理安全审计 当公司的服务器变的越来越多后，需要操作这些服务器的人就肯定不只是一个运维人员，同时也可能包括多个开发人员，那么这么多的人操作业务系统，如果权限分配不当就会存在很大的安全风险，举几个场景例子：设想你们公司有300台Linux服务器，A开发人员需要登录其中5台WEB服务器查看志或进行问题追踪等事务，同时对另外10台hadoop服务器有root权限，在有300台服务器规模的网络中，按常理来讲你是已经使用了ldap权限统一认证的，你如何使这个开发人员只能以普通用户的身份登录5台web服务器，并且同时允许他以管理员的身份登录另外10台hadoop服务器呢？并且同时他对其它剩下的200多台服务器没有访问权限 小型公司的运维团队为了方面，整个运维团队的运维人员还是共享同一套root密码，这样内部信任机制虽然使大家的工作方便了，但同时存在着极大的安全隐患，很多情况下，一个运维人员只需要管理固定数量的服务器，毕竟公司分为不同的业务线，不同的运维人员管理的业务线也不同，但如果共享一套root密码，其实就等于无限放大了每个运维人员的权限，也就是说，如果某个运维人员想干坏事的话，后果很严重。为了降低风险，于是有人想到，把不同业务线的root密码改掉就ok了么，也就是每个业务线的运维人员只知道自己的密码，这当然是最简单有效的方式，但问题是如果同时用了ldap,这样做又比较麻烦，即使设置了root不通过ldap认证，那新问题就是，每次有运维人员离职，他所在的业务线的密码都需要重新改一次。 因此，堡垒机的诞生就是为了规避这些高风险的问题，同时减少繁琐的重复性工作。 工作流程图： 一、需求分析1.所有生产服务器配置iptables安全策略，只能通过堡垒机来登陆，用户首先登陆进堡垒机，再通过堡垒机跳转登陆target host.2.各IT组，按职能划分一个统一的可以真实登陆target host的账户3.组内的成员，使用自己的账号登陆堡垒机，再使用小组账号登陆target host.4.审计。记录下各人员的操作记录，出现问题时可以溯源 二、表结构设计本着最少的字段数据冗余的原则，设计了如下几张表，如果各路大神有更好的设计思路，请指点一二~ 表创建代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import os,sysBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.append(BASE_DIR)from sqlalchemy import Table, Column, Enum,Integer,String, ForeignKey,UniqueConstraintfrom sqlalchemy.orm import relationshipfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import create_enginefrom conf import configBase = declarative_base()user_m2m_group_bind_host = Table('user_m2m_group_bind_host', Base.metadata, Column('id',Integer,autoincrement=True,primary_key=True), Column('user_id', Integer, ForeignKey('user.id')), Column('group_bind_host_id', Integer, ForeignKey('group_bind_host.id')), )user_m2m_group = Table('user_m2m_group', Base.metadata, Column('user_id', Integer, ForeignKey('user.id')), Column('group_id', Integer, ForeignKey('group.id')), )class Host(Base): __tablename__ = 'host' id = Column(Integer,primary_key=True) hostname = Column(String(64),unique=True) ip = Column(String(64),unique=True) port = Column(Integer,default=22) groups=relationship('Group',secondary='group_bind_host') def __repr__(self): return self.hostnameclass Group(Base): __tablename__ = 'group' id = Column(Integer, primary_key=True) name = Column(String(64), unique=True) login_passwd=Column(String(64)) bind_hosts = relationship("Host",secondary='group_bind_host') users=relationship('User',secondary='user_m2m_group') def __repr__(self): return self.nameclass Group_Bind_Host(Base): __tablename__ = "group_bind_host" __table_args__ = (UniqueConstraint('group_id','host_id', name='_host_remoteuser_uc'),) id = Column(Integer, primary_key=True) group_id = Column(Integer, ForeignKey('group.id')) host_id = Column(Integer,ForeignKey('host.id')) users=relationship('User',secondary='user_m2m_group_bind_host',backref='group_bind_hosts') #host = relationship("Host") #host_group = relationship("Group",backref="bind_hosts") #group = relationship("Group")class User(Base): __tablename__ = 'user' id = Column(Integer,autoincrement=True,primary_key=True) username = Column(String(32)) password = Column(String(128)) groups = relationship("Group",secondary="user_m2m_group") bind_hosts = relationship("Group_Bind_Host", secondary='user_m2m_group_bind_host') def __repr__(self): return self.usernameclass Log_audit(Base): __tablename__= 'log_audit' id = Column(Integer,autoincrement=True,primary_key=True) user_id = Column(Integer) user_name = Column(String(32)) host_ip = Column(String(32)) login_user = Column(String(32)) action_type = Column(String(16)) cmd=Column(String(128)) date = Column(String(16))if __name__ == "__main__": engine=create_engine(config.engine_param) Base.metadata.create_all(engine) # 创建表结构 三、项目代码1.整体结构： 2.功能说明：1.管理功能——表结构初始化创建——添加组——添加主机——添加用户——添加组-主机绑定 doc目录下提供了几个添加元素的example文档，使用yaml模块解析这些文档，解析为dict数据类型，将解析出的数据添加进数据库内，例如：对应实现添加user功能的函数： 2.用户视图——查看属组 —查看属组有权限登陆的主机——直接输入IP登陆主机 —查看该IP主机是否有可用的有权限的账户可供登陆——开始会话连接，执行命令时向记录审计志表添加item ssh会话实现： 使用了paramiko的demo模块，这个模块本身的交互功能是使用select模型来实现的，使用select监听会话句柄、sys.stdin（标准输入）的可读可写状态，实现字符在终端界面的输入输出。关于I/O多路复用几种模型的个人解读，可以翻看此前的博客：网络编程之I/O模型（以吃牛肉面为例）。 对paramiko交互模块进行修改添加记录功能：当标准输入触发回车键时（代表一个命令输入完毕开始执行），记录下执行人、登陆用户、时间、命令内容，写入数据库。修改后的交互模块代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126# Copyright (C) 2003-2007 Robey Pointer &lt;robeypointer@gmail.com&gt;## This file is part of paramiko.## Paramiko is free software; you can redistribute it and/or modify it under the# terms of the GNU Lesser General Public License as published by the Free# Software Foundation; either version 2.1 of the License, or (at your option)# any later version.## Paramiko is distributed in the hope that it will be useful, but WITHOUT ANY# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR# A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more# details.## You should have received a copy of the GNU Lesser General Public License# along with Paramiko; if not, write to the Free Software Foundation, Inc.,# 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.import socketimport sysfrom paramiko.py3compat import uimport datetimefrom moudle import table_initfrom moudle.db_conn import session# windows does not have termios...try: import termios import tty has_termios = Trueexcept ImportError: has_termios = Falsedef interactive_shell(chan,user_obj,choose_host,choose_group): if has_termios: posix_shell(chan,user_obj,choose_host,choose_group) else: windows_shell(chan)def posix_shell(chan,user_obj,choose_host,choose_group): import select oldtty = termios.tcgetattr(sys.stdin) try: tty.setraw(sys.stdin.fileno()) tty.setcbreak(sys.stdin.fileno()) chan.settimeout(0.0) cmd = '' tab_key = False while True: r, w, e = select.select([chan, sys.stdin], [], []) if chan in r: try: x = u(chan.recv(1024)) if tab_key: if x not in ('\t','\r\n'): #print('tab:',x) cmd += x tab_key = False if len(x) == 0: sys.stdout.write('\r\n*** EOF\r\n') break sys.stdout.write(x) sys.stdout.flush() except socket.timeout: pass if sys.stdin in r: x = sys.stdin.read(1) if '\r' != x: cmd +=x #输入字符不包含回车，则命令还未输入完成，包含回车且输入字符长度大于0，则记录志 if '\r' == x and len(cmd)&gt;0: log_item = table_init.Log_audit(user_id=user_obj.id, user_name=user_obj.username, host_ip=choose_host.ip, login_user=choose_group.name, action_type='cmd', cmd=cmd , date=datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") ) session.add(log_item) session.commit() cmd = '' if '\t' == x: tab_key = True # if len(x) == 0: # break chan.send(x) finally: termios.tcsetattr(sys.stdin, termios.TCSADRAIN, oldtty)# thanks to Mike Looijmans for this codedef windows_shell(chan): import threading sys.stdout.write("Line-buffered terminal emulation. Press F6 or ^Z to send EOF.\r\n\r\n") def writeall(sock): while True: data = sock.recv(256) if not data: sys.stdout.write('\r\n*** EOF ***\r\n\r\n') sys.stdout.flush() break sys.stdout.write(data) sys.stdout.flush() writer = threading.Thread(target=writeall, args=(chan,)) writer.start() try: while True: d = sys.stdin.read(1) if not d: break chan.send(d) except EOFError: # user hit ^Z or F6 pass 四、运行效果：在堡垒机上添加用户，在目标主机上添加一个对应登陆用户：在堡垒机上该用户环境变量配置文件中加入： 1/usr/local/python3/bin/python3 /usr/local/packages/MyFortress/bin/user_interface.py 使其打开shell后自动运行堡垒机程序，效果如下： 查看数据库审计志表，记录正常： 写了一个星期，终于能跑起来了，github链接：https://github.com/yinwenqin/MyFortress-Server]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python ORM ：SQLAlchemy基础使用]]></title>
    <url>%2F2017%2F12%2F05%2FPython%20ORM%20%EF%BC%9ASQLAlchemy%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[1.ORM介绍：orm英文全称object relational mapping,就是对象映射关系程序，简单来说我们类似python这种面向对象的程序来说一切皆对象，但是我们使用的数据库却都是关系型的，为了保证一致的使用习惯，通过orm将编程语言的对象模型和数据库的关系模型建立映射关系，这样我们在使用编程语言对数据库进行操作的时候可以直接使用编程语言的对象模型进行操作就可以了，而不用直接使用sql语言。等同于在数据库之上利用数据库提供的api再进行一次封装，将不同的sql结构、语句，转换成对象、属性等易于程序猿编写的code格式。 2.SQLAlchemySQLAlchemy是Python下的一款数据库对象关系映射（ORM）工具，能满足大多数数据库操作需求，同时支持多种数据库引擎（SQLite，MySQL，Postgresql，Oracle等）。所谓ORM（Object Relational Mapper）可以理解为“将数据库中的表映射为程序中的类”，表中的一行即为类的一个实例。比如Users表映射为User类，表中的一行数据映射为User()实例。SQLAlchemy在Web开发中应用较多，但作为一个数据分析、数据挖掘人员，最好也能掌握这门灵活的数据库操作技术。它的主要优点有：对数据表的抽象，允许开发人员首先考虑数据模型，同时使得Python程序更加简洁易读。对各种数据库引擎的封装，使得开发人员在面对不同数据库时，只需要做简单修改即可，工作量大大减少。 缺点：无可避免的，自动化意味着映射和关联管理，代价是牺牲性能（早期，这是所有不喜欢ORM人的共同点）。现在的各种ORM框架都在尝试使用各种方法来减轻这块（LazyLoad惰性加载，Cache），效果还是很显著的。 3.基础使用：首先需要安装sqlalchemy包，直接pip安装：pip install sqlalchemy代码中的注释很详细，就不再写了。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#Author :ywqimport sqlalchemyfrom sqlalchemy import create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column,Integer,Stringfrom sqlalchemy.orm import sessionmakerengine=create_engine('mysql+pymysql://ywq:qwe@192.168.0.71/test', encoding='utf-8',echo=True) #use mysql DB，and pymysql as the driverBase=declarative_base() #生成orm映射关系基类'''定义表结构和创建表'''class User(Base): __tablename__='user' id =Column(Integer,primary_key=True) name=Column(String(8)) password=Column(String(16)) # def __repr__(self): # return "User(id=%i,name='%s',password='%s')" %(self.id,self.name,self.password)Base.metadata.create_all(engine) #连接engine，创建表结构，与一般子类调用父类不同，这里是 # Base作为父类，调用所有上方创建的子类。#Base.metadata.drop_all(engine) # 删除所有表Session_class=sessionmaker(bind=engine) #创建与数据库的会话session class ,注意,这里返回给session的是个class,不是实例Session=Session_class() #创建session实例，可以把session理解成pymysql的游标'''insert data into table'''# user_table_obj=User(name='ywq',password='qwe') #生成想要创建的数据库表中的数据# print(user_table_obj.name,user_table_obj.id) #此时数据还未添加进库中，打印出来的结果为None## Session.add(user_table_obj)# print(user_table_obj.name,user_table_obj.id) #添加后发现还是为None，因为还没有commit提交## Session.commit()'''del data from table'''#Session.delete(User(User.id !=1)) #just delete#Session.query(User).filter(User.id != 1).delete() #query then delete'''query data from table'''# field_user=Session.query(User).filter_by(name='ywq').first()# print(field_user,field_user.name,field_user.id) #field_user是一段内存映射对象，查看具体字段还要调用对象属性# #为了查看方便，使返回的结果无需调用属性，直接查看，可以在class User# #类中定义内置方法，如上注释部分# print(Session.query(User.name,User.id,User.password).all()) #查看所有指定字段objs = Session.query(User).filter(User.id&gt;10).filter(User.id&lt;15).all() #条件查询,查询出来的结果是一个listfor i in objs:print(i.name,i.id)'''modify data from table'''# user_table_obj=Session.query(User).filter_by(id=1).first()# user_table_obj.name='ywq1' #调用对象属性重新赋值即可立即修改表中字段## Session.commit()## '''# rollback# '''# user_table_obj=Session.query(User).filter_by(name='ywq').first()# user_table_obj.name='ywq2'## user_table_add=User(name='cqy',password='qwe')# Session.add(user_table_add)## print(Session.query(User).filter(User.name.in_(['ywq','cqy'])).all()) #查看session里刚添加和修改的数据，此时数据还未commit## Session.rollback()# print(Session.query(User).filter(User.name.in_(['ywq','cqy'])).all()) #再次查看发现刚添加修改的数据没了#'''advanced query'''users = Session.query(sqlalchemy.distinct(User.name)) # 去重查询,根据name进行去重users = Session.query(User).order_by(User.name) # 排序查询,正序查询users = Session.query(User).order_by(User.name.desc()) # 排序查询,倒序查询users = Session.query(User).order_by(sqlalchemy.desc(User.name)) # 排序查询,倒序查询的另外一种形式users = Session.query(User.id, User.name) # 只查询部分属性users = Session.query(User.name.label("user_name")) # 结果集的列取别名 4.联表和外键抛出问题：怎样实现一个使用对象的属性实现跨表的增删改查呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#Author :ywqfrom sqlalchemy import Table, Column, Integer,String,DATE, ForeignKeyfrom sqlalchemy.orm import relationshipfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmakerengine=create_engine('mysql+pymysql://ywq:qwe@192.168.0.71/test?charset=utf8', encoding='utf-8',echo=True)Base = declarative_base()#多对多场景，为避免多字段冗余，节约表空间，因此将对象的对应关系单独存于一张表book_m2m_author = Table('book_m2m_author', Base.metadata, Column('book_id',Integer,ForeignKey('books.id')), Column('author_id',Integer,ForeignKey('authors.id')), )class Book(Base): __tablename__ = 'books' id = Column(Integer,primary_key=True) name = Column(String(64)) authors = relationship('Author',secondary=book_m2m_author,backref='books') ''' 直接通过给类定义authors属性的方式，关联Author表中的实例，但book表和author表没有直接联系，得通过第三张纯外键表来实现，因此 secondary=book_m2m_author的作用体现于此，backref='books'，代表在book类中创建了author的属性的同时，在内容中会创建维护 一个赋予给author类的属性，即可通过author.books来关联book中的实例。 authors = relationship('Author',secondary=book_m2m_author,backref='books') 可以这样理解它，假设已有book1，查询它的author,那么过程就是： author_id=session.query(book_m2m_author.author_id).filter(book_m2m_author.book_id==book1.id) author=session.query(authors).filter(authors.id==author_id) ''' def __repr__(self): return self.nameclass Author(Base): __tablename__ = 'authors' id = Column(Integer, primary_key=True) name = Column(String(32)) def __repr__(self): return self.nameBase.metadata.create_all(engine)Session_class = sessionmaker(bind=engine) #创建与数据库的会话session class ,注意,这里返回给session的是个class,不是实例s = Session_class() #生成session实例b1 = Book(name="Byte of Python")b2 = Book(name="Python核心编程")b3 = Book(name="The zen of python")a1 = Author(name="Alice")a2 = Author(name="Bob")a3 = Author(name="jack")b1.authors = [a1,a2] b2.authors = [a1,a2,a3] #直接给book实例的authors属性赋值，过程：1.先查询出book实例的表id 2.查出author实例的表id #3.在book_m2m_author表中记录books.id authors.id的对应关系 s.add_all([b1,b2,b3,a1,a2,a3])s.commit() 代码中注释已经很详细，不再细说。]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python I/O多路复用模块select使用]]></title>
    <url>%2F2017%2F12%2F01%2Flinux-IO%E5%A4%8D%E7%94%A8%E6%A8%A1%E5%9E%8Bselect.html</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#Author :ywqimport selectimport socketimport queueserver = socket.socket()server.bind(('localhost',9000))server.listen(1000)server.setblocking(False) #不阻塞msg_dic = &#123;&#125;inputs = [server,]#inputs = [server,conn] #[conn,]#inputs = [server,conn,conn2] #[conn2,]outputs = [] ##outputs = [r1,] #while True: readable ,writeable,exceptional= select.select(inputs, outputs, inputs ) print('Readable:',readable,) print('writable:',writeable) print('exception:',exceptional) print('input:',inputs) for r in readable: if r is server: #代表来了一个新连接 conn,addr = server.accept() print("来了个新连接",conn) inputs.append(conn) #是因为这个新建立的连接还没发数据过来，现在就接收的话程序就报错了 #所以要想实现这个客户端发数据来时server端能知道，就需要让select再监测这个conn msg_dic[conn] = queue.Queue() #初始化一个队列，后面存要返回给这个客户端的数据 #print(msg_dic) else: #conn2 data = r.recv(1024) print("收到数据",data) msg_dic[r].put(data) outputs.append(r) #放入返回的连接队列里 # r.send(data) # print("send done....") for w in writeable: #要返回给客户端的连接列表 data_to_client = msg_dic[w].get() w.send(data_to_client) #返回给客户端源数据 outputs.remove(w) #确保下次循环的时候writeable,不返回这个已经处理完的连接了 for e in exceptional: if e in outputs: outputs.remove(e) inputs.remove(e) del msg_dic[e] print('Readable:',readable,) print('writable:',writeable) print('exception:',exceptional) ''' 1.select.select(rlist,wlist,xlist)运行后生成三个列表，对应变量readable，writeable，exceptional， 三个变量类型也均为list。 2.三个参数：rlist, wlist, xlist,分别代表需要使用select监控的三个装有fd、socket fd的列表， select会监控列表里的fd，一旦传入的参数rlist内有可读的fd对象，则将该对象加入readable变量中。 一旦传入的参数wlist列表中有可写的fd对象，则将该对象加入writeable变量中， 一旦传入的参数xlist列表中有可连接error报错的fd对象，则将该对象加入exceptional变量中。 3.首先要把服务端socket加入inputs内让socket监控它，一旦服务端变为可读状态，即代表有新连接进来了（此时连接还未建立成功） 开始循环，遍历readable、writeable、exceptional三个列表，列表有数据则运行相应指令 与客户端连接建立后，应将客户端socket加入inputs列表，socket监控其是否可读、是否连接报错，此时刚刚建立连接，服务端 还不能直接socket.recv(1024)，否则会报错，因为客户端消息还没发过来，要等待下一次循环。 连接建立后，进入下一轮循环，此时需等待客户端传消息过来，传消息过来后，if not is server,则执行else语句， 接收client端传来的消息，并且把client socket加入outputs列表，socket监控其是否可写，并且创建客户端专属队列， 准备开始传消息 4.进入for in writeable的循环，检测发现客户端已经是可写状态了，则开始传送数据。其实writeable也可以监控inputs列表 不用单独创建outputs列表，毕竟所有的连接socket都已经在inputs里边了，但是为了使遍历writeable列表速度更快， 最好把可写列表单独出来，节省资源。 5.总结：select的本质就是为了替程序快速监控fd、socket的I/O状态，以便根据I/O状态快速开始操作，可读可写时则 开始I/O操作，线程去执行其他计算操作。I/O操作时不消耗计算资源。以此交错开来尽量合理化地利用单线程资源。 ''' 一个简单的echo server，使用select I/O复用模型，可以实现多并发连接请求的同时低资源消耗。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python队列通信：rabbitMQ使用]]></title>
    <url>%2F2017%2F11%2F26%2Fpython%E9%98%9F%E5%88%97%E9%80%9A%E4%BF%A1%EF%BC%9ArabbitMQ%E4%BD%BF%E7%94%A8.html</url>
    <content type="text"><![CDATA[（一）、前言 为什么引入消息队列？ 1.程序解耦 2.提升性能 3.降低多业务逻辑复杂度 （二）、python操作rabbit mqrabbitmq配置安装基本使用参见上节文章，不再复述。若想使用python操作rabbitmq，需安装pika模块，直接pip安装：pip install pika 1.最简单的rabbitmq producer端与consumer端对话：producer： 123456789101112131415161718#Author :ywqimport pikaauth=pika.PlainCredentials('ywq','qwe') #save auth indoconnection = pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth)) #connect to rabbitchannel = connection.channel() #create channelchannel.queue_declare(queue='hello') #declare queue#n RabbitMQ a message can never be sent directly to the queue, it always needs to go through an exchange.channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') #the body is the msg contentprint(" [x] Sent 'Hello World!'")connection.close() consumer： 12345678910111213141516171819202122#Author :ywqimport pikaauth=pika.PlainCredentials('ywq','qwe') #auth infoconnection = pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth)) #connect to rabbitchannel = connection.channel() #create channelchannel.queue_declare(queue='hello') #decalre queuedef callback(ch, method, properties, body): print(" [x] Received %r" % body)channel.basic_consume(callback, queue='hello', no_ack=True)print(' [*] Waiting for messages. To exit press CTRL+C')channel.start_consuming() 消息传递消费过程中，可以在rabbit web管理页面实时查看队列消息信息。 2.持久化的消息队列，避免宕机等意外情况造成消息队列丢失。consumer端无需改变，在producer端代码内加上两个属性，分别使消息持久化、队列持久化，只选其一还是会出现消息丢失，必须同时开启： 12delivery_mode=2 #make msg persisdentdurable=True 属性插入位置见如下代码（producer端）： 123456789101112131415161718192021222324252627#Author :ywqimport pika,sysauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.queue_declare(queue='test1',durable=True) #durable=Ture, make queue persistentmsg=''.join(sys.argv[1:]) or 'Hello'channel.basic_publish( exchange='', routing_key='test1', body=msg, properties=pika.BasicProperties( delivery_mode=2 #make msg persisdent ))print('Send done:',msg)connection.close() 3.公平分发在多consumer的情况下，默认rabbit是轮询发送消息的，但有的consumer消费速度快，有的消费速度慢，为了资源使用更平衡，引入ack确认机制。consumer消费完消息后会给rabbit发送ack，一旦未ack的消息数量超过指定允许的数量，则不再往该consumer发送，改为发送给其他consumer。 producer端代码不用改变，需要给consumer端代码插入两个属性：12channel.basic_qos(prefetch_count= *) #define the max non_ack_countchannel.basic_ack(delivery_tag=deliver.delivery_tag) #send ack to rabbitmq 属性插入位置见如下代码（consumer端）： 12345678910111213141516171819202122232425262728293031#Author :ywqimport pika,timeauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.queue_declare(queue='test2',durable=True)def callback(chann,deliver,properties,body): print('Recv:',body) time.sleep(5) chann.basic_ack(delivery_tag=deliver.delivery_tag) #send ack to rabbitchannel.basic_qos(prefetch_count=1)'''注意，no_ack=False 注意，这里的no_ack类型仅仅是告诉rabbit该消费者队列是否返回ack，若要返回ack，需要在callback内定义prefetch_count=1,未ack的msg数量超过1个，则此consumer不再接受msg，此配置需写在channel.basic_consume上方，否则会造成non_ack情况出现。'''channel.basic_consume( callback, queue='test2')channel.start_consuming() 三、消息发布/订阅 上方的几种模式都是producer端发送一次，则consumer端接收一次，能不能实现一个producer发送，多个关联的consumer同时接收呢？of course，rabbit支持消息发布订阅，共支持三种模式，通过组件exchange转发器，实现3种模式： fanout: 所有bind到此exchange的queue都可以接收消息，类似广播。direct: 通过routingKey和exchange决定的哪个唯一的queue可以接收消息，推送给绑定了该queue的consumer，类似组播。topic:所有符合routingKey(此时可以是一个表达式)的routingKey所bind的queue可以接收消息，类似前缀列表匹配路由。 1.fanoutpublish端（producer）： 12345678910111213141516171819202122232425262728#Author :ywqimport pika,sys,timeauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.exchange_declare(exchange='hello', exchange_type='fanout' )msg=''.join(sys.argv[1:]) or 'Hello world %s' %time.time()channel.basic_publish( exchange='hello', routing_key='', body=msg, properties=pika.BasicProperties( delivery_mode=2 ))print('send done')connection.close() subscribe端（consumer）： 1234567891011121314151617181920212223242526272829303132333435#Author :ywqimport pikaauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.exchange_declare( exchange='hello', exchange_type='fanout')random_num=channel.queue_declare(exclusive=True) #随机与rabbit建立一个queue，comsumer断开后，该queue立即删除释放queue_name=random_num.method.queuechannel.basic_qos(prefetch_count=1)channel.queue_bind( queue=queue_name, exchange='hello')def callback(chann,deliver,properties,body): print('Recv:',body) chann.basic_ack(delivery_tag=deliver.delivery_tag) #send ack to rabbitchannel.basic_consume( callback, queue=queue_name,)channel.start_consuming() 实现producer一次发送，多个关联consumer接收。使用exchange模式时：1.producer端不再申明queue，直接申明exchange2.consumer端仍需绑定队列并指定exchange来接收message3.consumer最好创建随机queue，使用完后立即释放。随机队列名在web下可以检测到： 2.direct使用exchange同时consumer有选择性的接收消息。队列绑定关键字，producer将数据根据关键字发送到消息exchange，exchange根据 关键字 判定应该将数据发送至指定队列，consumer相应接收。即在fanout基础上增加了routing key. producer： 123456789101112131415161718192021222324252627#Author :ywqimport pika,sysauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.exchange_declare(exchange='direct_log', exchange_type='direct', )while True: route_key=input('Input routing key:') msg=''.join(sys.argv[1:]) or 'Hello' channel.basic_publish( exchange='direct_log', routing_key=route_key, body=msg, properties=pika.BasicProperties( delivery_mode=2 ) )connection.close() consumer： 1234567891011121314151617181920212223242526272829303132333435363738#Author :ywqimport pika,sysauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info))channel=connection.channel()channel.exchange_declare( exchange='direct_log', exchange_type='direct')queue_num=channel.queue_declare(exclusive=True)queue_name=queue_num.method.queueroute_key=input('Input routing key:')channel.queue_bind( queue=queue_name, exchange='direct_log', routing_key=route_key)def callback(chann,deliver,property,body): print('Recv:[level:%s],[msg:%s]' %(route_key,body)) chann.basic_ack(delivery_tag=deliver.delivery_tag)channel.basic_qos(prefetch_count=1)channel.basic_consume( callback, queue=queue_name)channel.start_consuming() 同时开启多个consumer，其中两个接收notice，两个接收warning，运行效果如下： 3.topic相较于direct，topic能实现模糊匹配式工作方式（在consumer端指定匹配方式），只要routing key包含指定的关键字，则将该msg发往绑定的queue上。 rabbitmq通配符规则：符号“#”匹配一个或多个词，符号“*”匹配一个词。因此“abc.#”能够匹配到“abc.m.n”，但是“abc.**‘’ 只会匹配到“abc.m”。‘.’号为分割符。使用通配符匹配时必须使用‘.’号分割。 producer： 123456789101112131415161718192021222324252627#Author :ywqimport pika,sysauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info ))channel=connection.channel()channel.exchange_declare(exchange='topic_log', exchange_type='topic', )while True: route_key=input('Input routing key:') msg=''.join(sys.argv[1:]) or 'Hello' channel.basic_publish( exchange='topic_log', routing_key=route_key, body=msg, properties=pika.BasicProperties( delivery_mode=2 ) )connection.close() consumer： 1234567891011121314151617181920212223242526272829303132333435363738#Author :ywqimport pika,sysauth_info=pika.PlainCredentials('ywq','qwe')connection=pika.BlockingConnection(pika.ConnectionParameters( '192.168.0.158',5672,'/',auth_info))channel=connection.channel()channel.exchange_declare( exchange='topic_log', exchange_type='topic')queue_num=channel.queue_declare(exclusive=True)queue_name=queue_num.method.queueroute_key=input('Input routing key:')channel.queue_bind( queue=queue_name, exchange='topic_log', routing_key=route_key)def callback(chann,deliver,property,body): print('Recv:[type:%s],[msg:%s]' %(route_key,body)) chann.basic_ack(delivery_tag=deliver.delivery_tag)channel.basic_qos(prefetch_count=1)channel.basic_consume( callback, queue=queue_name)channel.start_consuming() 运行效果： rabbitmq三种publish/subscribe模型简单介绍完毕，以上！]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rabbitmq安装配置与基本操作]]></title>
    <url>%2F2017%2F11%2F22%2Frabbitmq%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[1.安装yum - y install gccrpm -ivh erlang-18.1-1.el6.x86_64.rpmrpm -ivh socat-1.7.1.3-1.el6.rf.x86_64.rpm [root@localhost packages]# rpm -ivh rabbitmq-server-3.6.14-1.el6.noarch.rpmPreparing… ########################################### [100%] 1:rabbitmq-server ########################################### [100%] 安装rabbitmq时不显示各路径，默认执行文件路径为：/usr/sbin/默认配置文件模板：/usr/share/doc/rabbitmq-server-3.6.14/rabbitmq.config.example默认配置文件路径：/etc/rabbitmq/ #为空，需要复制模板进去修改 2、安装 RabbitMQ Web管理插件（插件必须在server启动之前配置） 在终端执行如下命令即安装成功：rabbitmq-plugins enable rabbitmq_managementservice rabbitmq-server restart 3、启动RabbitMQ Server[html] view plain copy/etc/init.d/rabbitmq-server start或service rabbitmq-service start 4、配置远程权限远程连接rabbitmq server的话，需要配置权限,首先在rabbitmq server上创建一个用户rabbitmqctl add_user ywq qwe 同时还要配置权限，允许从外面访问rabbitmqctl set_permissions -p / ywq “.“ “.“ “.*” #set_permissions [-p ] rabbitmqctl set_user_tags ywq administrator # set role as admin 打开浏览器登录：http://192.168.0.158:15672 也可以，直接跳转到下列web管理登录 账号密码默认都是 guest，可以用设置的账号登陆 5、管理（rabbitmqctl在server启动之后配置）Rabbitmq服务器的主要通过rabbitmqctl和rabbimq-plugins两个工具来管理，以下是一些常用功能。1）. 服务器启动与关闭 启动: rabbitmq-server –detached 关闭:rabbitmqctl stop 若单机有多个实例，则在rabbitmqctl后加–n 指定名称2）. 插件管理 开启某个插件：rabbitmq-pluginsenable xxx 关闭某个插件：rabbitmq-pluginsdisablexxx 注意：重启服务器后生效。3）.virtual_host管理 新建virtual_host: rabbitmqctladd_vhost xxx 撤销virtual_host:rabbitmqctl delete_vhost xxx 查看vhost：rabbitmqctl list_vhost4）. 用户管理 新建用户：rabbitmqctl add_user xxx pwd 删除用户: rabbitmqctl delete_user xxx 改密码: rabbimqctl change_password {username} {newpassword} 设置用户角色：rabbitmqctl set_user_tags {username} {tag …} Tag可以为 administrator,monitoring, management5）. 权限管理 权限设置：set_permissions [-pvhostpath] {user} {conf} {write} {read} Vhostpath Vhost路径 user 用户名 Conf 一个正则表达式match哪些配置资源能够被该用户访问。 Write 一个正则表达式match哪些配置资源能够被该用户读。 Read 一个正则表达式match哪些配置资源能够被该用户访问。6）. 获取服务器状态信息 服务器状态：rabbitmqctl status 队列信息：rabbitmqctl list_queues[-p vhostpath] [queueinfoitem …] Queueinfoitem可以为：name，durable，auto_delete，arguments，messages_ready， messages_unacknowledged，messages，consumers，memory Exchange信息：rabbitmqctllist_exchanges[-p vhostpath] [exchangeinfoitem …] Exchangeinfoitem有：name，type，durable，auto_delete，internal，arguments. Binding信息：rabbitmqctllist_bindings[-p vhostpath] [bindinginfoitem …] Bindinginfoitem有：source_name，source_kind，destination_name，destination_kind，routing_key，arguments Connection信息：rabbitmqctllist_connections [connectioninfoitem …] Connectioninfoitem有：recv_oct，recv_cnt，send_oct，send_cnt，send_pend等。 Channel信息：rabbitmqctl list_channels[channelinfoitem …] Channelinfoitem有consumer_count，messages_unacknowledged，messages_uncommitted，acks_uncommitted，messages_unconfirmed，prefetch_count，client_flow_blocked]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux I/O模型（以吃牛肉面为例）]]></title>
    <url>%2F2017%2F11%2F16%2Flinux-IO%E6%A8%A1%E5%9E%8B-%E5%90%83%E7%89%9B%E8%82%89%E9%9D%A2%E4%B8%BA%E4%BE%8B.html</url>
    <content type="text"><![CDATA[一、前言同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，有什么区别，为什么性能差别巨大，结合我个人的理解做一下总结。（仅讨论linux场景下）知识点说明： 用户（进程）空间和内核空间：操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户（进程）空间。 标准 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。linux上偶尔因程序异常占用大量内存，需手动释放page cache，echo 1 &gt; procsys/vm/drop_caches，就是因为这个原因。 标准 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。例如socket在传输数据包时为了防止过于频繁的传输，有时会将一些小的数据包整合在一起一次性发，导致socket粘包。 二.I/O模式对于一次系统IO的访问，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间，即从kernel空间copy到用户空间。所以说，当一个read操作发生时，它会经历两个阶段： Input，等待数据进入内核空间 (Waiting for the data to be ready) Output，将数据从内核空间拷贝到进程空间中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 同步阻塞 I/O（blocking IO） 同步非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：signal driven IO在实际中比较少用，所以我只说明其余四种Model。 1.同步阻塞 I/O（blocking IO）在linux中，默认情况下所有的socket都是blocking，一个典型的流程图： 2.同步非阻塞I/O：linux下，可以通过设置socket使其变为non-blocking当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 3.I/O多路复用：IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。性能强大的模型，典型案例就是nginx，使用epoll模型的nginx单点支撑5W并发非常轻松。select的优势在于它可以同时处理多个connection。如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-thread + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个process的上半部分其实是一直被block的(即向kernel发起select调用的部分)，需要等kernel回复之后，再向kernel发起一个内核态向用户态copy数据的系统调用,发起这个调用后，process就可以进行context switch了，这个时候内核需要进行的工作是将数据从内核态复制到用户态(即I/O)。因此对于process而言，这种模型即为I/O复用。 4.异步I/O：用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 看的很费劲？OK，那就自己动手画几张形象生动点的图吧，身为吃货当然以“吃”举例1.同步阻塞 I/O（blocking IO） 2.同步非阻塞I/O： 3.I/O多路复用： 4.异步I/O： 感觉心里想着牛肉面，这几个模型都好理解了，Over~]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python多进程线程练习：主机批量管理]]></title>
    <url>%2F2017%2F11%2F14%2Fpython%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E7%BB%83%E4%B9%A0%EF%BC%9A%E4%B8%BB%E6%9C%BA%E6%89%B9%E9%87%8F%E7%AE%A1%E7%90%86.html</url>
    <content type="text"><![CDATA[需求:1.主机信息配置文件用configparser解析2.可批量执行命令，上传文件，结果实时返回3.主机用户名密码、端口可以不同4.执行远程命令使用paramiko模块5.多台主机链接创建多个进程并发，单台主机上的多条执行命令使用多线程并发 一、安装模块paramiko模块是遵循python模块标准的openssh协议模块，可以使用pip快速安装。首先需安装pip工具，windows的python安装包一般没有集成pip，需单独安装下载pip源码包：https://pypi.python.org/pypi/pip#downloads打开windows cmd，打开后进入解压源码包后进入源目录执行：python setup.py install安装完成后，pip安装paramikopip install paramiko安装成功后，开始code 二、生成配置文件： 1234567891011121314def addhost(): while True: num=input('Add host to conf.ini,Input last num,or q|Q to continue :') if num== 'q' or 'Q': break else: ip=str('192.168.0.%s'%num) config.add_section(ip) config.set(ip,'username','root') config.set(ip,'password','xxxxxxxxxxx') config.write(sys.stdout) config.write(open('sample.ini','w'))addhost() 密码正好是统一的不用改了。生成配置文件如下： 三、加载配置文件，连接主机执行命令，所有代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119from multiprocessing import Processimport configparser,sys,threading,osimport paramikoconfig=configparser.ConfigParser()config.read(u'sample.ini')host_list=config.sections()'''configparser用法：#config.set('section','username','root') #给大标题内的key赋值# print(config.get("size","size")) #查看大标题内的指定小标题# print(config.sections() ) #查看所有section大标题'''def addhost(): while True: num=input('\033[0;32mAdd host to conf.ini,Input last num,or q|Q to continue :\033[0m') if num== 'q' or 'Q': break else: ip=str('192.168.0.%s'%num) config.add_section(ip) config.set(ip,'username','root') config.set(ip,'password','yinwenqin') config.write(sys.stdout) config.write(open('sample.ini','w'))def ssh_conn(host,cmd_list): usr=config.get(host,'username') passwd=config.get(host,'password') print('\033[1;32mProcess ID:\033[0m',os.getpid()) ssh = paramiko.SSHClient()# 允许连接不在know_hosts文件中的主机 ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())# 连接服务器 ssh.connect(hostname=host, port=22, username=usr, password=passwd) for c in cmd_list: t=threading.Thread(target=exec,args=(host,ssh,c,)) #将连接实例传递给线程执行 t.start() t.join() ssh.close()def exec(host,ssh,c): stdin, stdout, stderr = ssh.exec_command(c) # 执行命令,分别保存标准输入、标准输出、标准错误结果到变量 # 获取命令结果 result = stdout.read().decode() print('\033[1;33mExec thread ID:\033[0m',threading.current_thread()) print('\033[1;33mHost %s exec cmd &lt;&lt;%s&gt;&gt; result&gt;&gt;:\033[0m' %(host,c)) print(result)def sftp_conn(host,cmd_list): usr=config.get(host,'username') passwd=config.get(host,'password') print('\033[1;32mProcess ID:\033[0m',os.getpid()) transport = paramiko.Transport((host,22)) transport.connect(username=usr,password=passwd) sftp = paramiko.SFTPClient.from_transport(transport) filename_list=cmd_list[1:] for file in filename_list: if cmd_list[0] == 'put': t=threading.Thread(target=put,args=(host,sftp,file,)) t.start() t.join() elif cmd_list[0] == 'get': t=threading.Thread(target=get,args=(host,sftp,file,)) t.start() t.join() transport.close()def put(host,sftp,file): sftp.put(file,'/root/%s' %file) print('\033[1;33mExec thread ID:\033[0m',threading.current_thread()) print('\033[1;33mHost %s exec cmd &lt;&lt;put %s&gt;&gt; result&gt;&gt;:\033[0m' %(host,file)) print('\033[1;33mFile info:\033[0m \n',os.stat(file))def get(host,sftp,file): sftp.get('/root/%s' %file, 'D:\Python\Myscript\process &amp; threading\%s' %file) print('\033[1;33mExec thread ID:\033[0m',threading.current_thread()) print('\033[1;33mHost %s exec cmd &lt;&lt; get %s&gt;&gt; result&gt;&gt;:\033[0m' %(host,file)) print('\033[1;33mFile info:\033[0m \n',os.stat('D:\Python\Myscript\process &amp; threading\%s' %file))if __name__ == '__main__': addhost() while True: try: print('\033[0;31mHosts list:\033[0m') print(host_list) choice=input('\033[0;32mChoose host to login:( divide multi hosts by space ):\033[0m \n') if len(choice)==0:continue choice_list=choice.split() print(choice_list) choice2=input('\033[0;32m 1.Exec cmd \n2.Put/get file \nPlease choose \033[0m \n ') print(choice2) cmd=input('\033[0;32mInput cmd ( &lt;;&gt; to devide cmds , &lt;space&gt; to divide files ):\033[0m \n') print(cmd) if int(choice2) == 1: cmd_list=cmd.split(';') #use ';' to devide cmd str for i in choice_list: print(i) p=Process(target=ssh_conn,args=(i,cmd_list)) p.start() p.join() elif int(choice2) == 2: cmd_list=cmd.split() #use ';' to devide cmd str for i in choice_list: print(i) p=Process(target=sftp_conn,args=(i,cmd_list)) p.start() p.join() except Exception as e: print('Error:',e) 实现了批量连接多个主机，在每个主机上同时执行多条命令，上传或下载多个文件。 运行效果： 老爷本开8台虚拟机一个IDE跑了这么久不容易啊，快热炸了，路过顶一下再走呗~![这里写图片描述]]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python：thread & process]]></title>
    <url>%2F2017%2F11%2F13%2FPython%E7%BA%BF%E7%A8%8B%E5%92%8C%E8%BF%9B%E7%A8%8B.html</url>
    <content type="text"><![CDATA[一、进程和线程的区别1.线程是CPU的最小执行单位，直接运行在CPU上的是线程而不是进程；2.进程是线程资源的集合，一个进程至少包含一个线程3.线程之间可以共享内存资源，进程之间无法直接共享内存空间。 二、线程1.Create thread： 12345678910111213import threading,timestart_time=time.time()def run(): time.sleep(2) print('In threading',threading.current_thread()) #print current sub thread idfor i in range(5): t=threading.Thread(target=run,) t.start()print('In the main threading',threading.current_thread())print('Running time :',time.time()-start_time) Running result: 1234567In the main threading &lt;_MainThread(MainThread, started 9888)&gt;Running time : 0.0009999275207519531In threading &lt;Thread(Thread-1, started 9956)&gt;In threading &lt;Thread(Thread-2, started 8524)&gt;In threading &lt;Thread(Thread-4, started 3404)&gt;In threading &lt;Thread(Thread-3, started 7692)&gt;In threading &lt;Thread(Thread-5, started 9524)&gt; 可以发现，运行时间并没有计算进子线程运行的时间，主线程本身并没有等待子线程，创建的子线程独立运行，子线程运行结束后，程序再退出。 2.Join 承接上文，主线程不会等待子线程执行，两者并行互不干扰。如有主线程需用到子线程的执行结果的场景，则需要使主进程阻塞等待子进程。 123456789101112131415161718192021222324252627282930import threading,timestart_time=time.time()def run(num): time.sleep(1) print('In the threading :',threading.current_thread())# t1=threading.Thread(target=run,args=(1,))# t2=threading.Thread(target=run,args=(2,))# t1.start()# t2.start()# t1.join()# t2.join()obj_t=[]for i in range(10): t=threading.Thread(target=run,args=(1,)) t.start() # t.join() obj_t.append(t) for obj in obj_t: obj.join()print('In the main threading',threading.current_thread())# 注意,这里将obj.join()单独操作的原因是避免每个子线程都被阻塞，这样多线程就变成了串行，没有意义了。可以自行对比join操作放在start之后和start之外的运行时间print('Running time :',time.time()-start_time)'''Without the join attribute ,the main thread won't wait for the sub threading,if add it ,the main thread will waiting for the sub thread util sub thread run end.''' Running result 123456In the threading : &lt;Thread(Thread-4, started 9912)&gt;In the threading : &lt;Thread(Thread-2, started 1188)&gt;In the threading : &lt;Thread(Thread-1, started 7424)&gt;In the threading : &lt;Thread(Thread-5, started 9348)&gt;In the threading : &lt;Thread(Thread-3, started 10128)&gt;In the main threading &lt;_MainThread(MainThread, started 8204)&gt; 3.Daemon 线程 与子线程独立于主线程与之并行运行不同，守护线程在被主线程（或子线程）创建后，若主线程运行结束，不会等待守护进程，且程序直接退出。1234567891011121314151617181920import threading,timedef main_run(): time.sleep(1) print('In the main thread',threading.current_thread()) sub_t=threading.Thread(target=sub_run) sub_t.start()def sub_run(): time.sleep(2) print('In the sub thread ',threading.current_thread()) daemon_t=threading.Thread(target=daemon_run) daemon_t.setDaemon(True) daemon_t.start()def daemon_run(): time.sleep(3) print('In the daemon thread ',threading.current_thread())main_run() Running res: 1234In the main thread &lt;_MainThread(MainThread, started 10064)&gt;In the sub thread &lt;Thread(Thread-1, started 6716)&gt;Process finished with exit code 0 可以看到主线程率先执行完成，程序再继续等待子线程，子线程执行完后，没有等待守护线程，程序直接退出。 3.互斥锁为避免多个线程在同时操作同一个数据时造成混乱，在必要的场景下需要给数据加锁 1234567891011121314151617import threading,timelock=threading.Lock()num=0def run(): global num num+=1 lock.acquire() print('Current thread:\n',threading.current_thread()) print('Current num',num) time.sleep(1) lock.release() #Only one of the thread can run the content protected by the mutex lock.for i in range(5): t=threading.Thread(target=run) t.start() Running result 123456789101112131415Current thread: &lt;Thread(Thread-1, started 2348)&gt;Current num: 1Current thread: &lt;Thread(Thread-2, started 6128)&gt;Current num: 5Current thread: &lt;Thread(Thread-3, started 7564)&gt;Current num: 5Current thread: &lt;Thread(Thread-4, started 10104)&gt;Current num: 5Current thread: &lt;Thread(Thread-5, started 10128)&gt;Current num: 5 4.信号量承接上文，互斥锁即同时只能允许有一个线程在操作数据，引入信号量就可以指定允许的线程数的上限。 123456789101112131415161718import threading,timedef run(n): semaphore.acquire() time.sleep(1) print("run the thread: %s\n" %n) semaphore.release()if __name__ == '__main__': #run only in this module semaphore = threading.BoundedSemaphore(2) #创建semaphore实例，最多允许2个线程同时运行锁住的内容 for i in range(10): t = threading.Thread(target=run,args=(i,)) t.start()while threading.active_count() != 1: pass #print threading.active_count()else: print('----all threads done---') Running reslut： 12345678910111213141516171819run the thread: 0run the thread: 1run the thread: 2run the thread: 3run the thread: 4run the thread: 5run the thread: 6run the thread: 7run the thread: 8run the thread: 9----all threads done--- 可以看到每次打印的数量为指定的信号量semaphore值：2 5.死锁：当使用多个互斥锁(同步锁)同时使用且造成冲突时，会出现进程死锁情况： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import threadingimport timedef foo(): lockA.acquire() print('func foo ClockA lock') lockB.acquire() print('func foo ClockB lock') lockB.release() print('func foo ClockB release') lockA.release() print('func foo ClockA release')def bar(): lockB.acquire() print('func bar ClockB lock') time.sleep(2) """ 用sleep模拟io操作，首先，foo进程的线程1率先执行完，bar进程的线程1在执行到这里的时候，lockB是锁上的还未释放，因此 foo进程的线程2无法申请lockB，阻塞住了，而sleep两秒过后，bar的线程1想要申请lockA，此时lockA是被foo的阻塞住的线程2 持有的,foo线程2被阻塞住无法释放lockA，因此bar这边想申请lockA也是不行的，由此双方都阻塞住，造成了死锁 """ lockA.acquire() print('func bar ClockA lock') lockB.release() print('func bar ClockB release') lockA.release() print('func bar ClockA release')def run(): foo() bar()l = []lockA = threading.Lock()lockB = threading.Lock()for i in range(10): t = threading.Thread(target=run,args=()) t.start() l.append(t)for t in l: t.join() 执行到bar的时候会阻塞住，结果如下，原因见上方代码内注释： 123456func foo ClockA lockfunc foo ClockB lockfunc foo ClockB releasefunc foo ClockA releasefunc bar ClockB lockfunc foo ClockA lock 6.rlock 递归锁在python中为了支持在同一线程中多次请求同一资源，避免死锁竞争，python提供了可重入锁RLock。这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。例如： 123456789101112131415161718192021222324252627import threading,timelock=threading.Lock()#rlock=threading.RLock()num=0def sub_thread_run(): print('Come into the second lock') lock.acquire() #rlock.acquire() global num num+=1 print('Current num in the sub thread :',num) lock.release() #rlock.release() print('Unlock the second lock')def run(): global num while num &lt; 4: num+=1 lock.acquire() print('Current num in the main:',num) t=threading.Thread(target=sub_thread_run()) t.start() lock.release()run() running result： 123Current num in the main: 1Come into the second lock... 可以到看会卡在第二层锁内无法出来，改为内层使用递归锁则正常运行： 12345678910111213141516171819202122232425262728import threading,timelock=threading.Lock()rlock=threading.RLock()num=0def sub_thread_run(): print('Come into the second lock') #lock.acquire() rlock.acquire() global num num+=1 print('Current num in the sub thread :',num) #lock.release() rlock.release() print('Unlock the second lock')def run(): global num while num &lt; 4: num+=1 lock.acquire() print('Current num in the main:',num) t=threading.Thread(target=sub_thread_run()) t.start() lock.release()run() running result： 12345678910Current num in the main: 1Come into the second lockCurrent num in the sub thread : 2Unlock the second lockCurrent num in the main: 3Come into the second lockCurrent num in the sub thread : 4Unlock the second lockProcess finished with exit code 0 7.event事件触发（信号触发），定义一个flag，在多线程之间通过这一个flag标志位互相传递信号，从而触发下一步动作，起到线程间协调沟通的作用。一个小游戏：红灯停，黄绿灯行 12345678910111213141516171819202122232425262728293031323334353637383940import threading,timeevent=threading.Event() #create event instancedef redlight(): event.set() #set the event flag num=1 while True: if 0&lt; num &lt;= 2: print('\033[0;32mGreen light on\033[0m') elif num &lt;=3: print('\033[0;33m Yellow light on\033[0m') elif num &lt;=5: print('\033[0;31m Red light on\033[0m') if event.isSet(): event.clear() #clear the event flag else: #when num &gt;8 num=0 #return back the num to zero event.set() num+=1 time.sleep(1)def car(car_num): while True: if event.isSet(): #Return the bool value,Ture or False print('car %i is running.'%car_num) time.sleep(1) else: print('Red light on ,car %i waiting... '%car_num) time.sleep(1)task_redlight=threading.Thread(target=redlight,)task_redlight.start()for i in range(2): car_task=threading.Thread(target=car,args=(i,)) car_task.start() running result： 123456789101112131415Green light oncar 0 is running.car 1 is running.Green light oncar 0 is running.car 1 is running. Yellow light oncar 0 is running.car 1 is running. Red light onRed light on ,car 0 waiting... Red light on ,car 1 waiting... Red light onRed light on ,car 0 waiting... Red light on ,car 1 waiting... 8.Queue队列的作用：1.解耦，实现程序松耦合2.提升效率，非阻塞 生产者、消费者模式： 123456789101112131415161718192021222324import threading,time,queueq=queue.Queue()def producer(): for i in range(1,6): q.put('包子 %i'%i) print('包子 %i 熟了' %i) time.sleep(0.5) print('今天包子卖完了')def consumer(name): while True: baozi=q.get() print('%s is eating %s' %(name,baozi)) time.sleep(1)p=threading.Thread(target=producer,)p.start()c1=threading.Thread(target=consumer,args=('ywq',))c2=threading.Thread(target=consumer,args=('cqy',))c1.start()c2.start() running result： 1234567891011包子 1 熟了ywq is eating 包子 1包子 2 熟了cqy is eating 包子 2包子 3 熟了ywq is eating 包子 3cqy is eating 包子 4包子 4 熟了ywq is eating 包子 5包子 5 熟了今天包子卖完了 三、进程C库 python 的GIL决定了python的多线程模型只是一个伪多线程模型，同时只能有一个线程在执行，通过定时高频的CPU上下文切换来实现多线程的假象，因此在CPU计算密集型的场景中使用C python多线程时，频繁的上下文切换甚至会导致性能更低，因此，多线程适用于I/O密集型场景。为了发挥出多Core硬件的性能，既然不能多线程，那就多进程把Core都占掉吧~mutilprocessing模块语法与threading模块类似： 1.create process： 123456789101112131415import multiprocessing,osdef run(): print('run pid :',os.getpid()) print('run father pid:',os.getppid()) f=multiprocessing.Process(target=sub_run,) f.start()def sub_run(): print('sub_run father pid:',os.getppid())if __name__=='__main__': p=multiprocessing.Process(target=run) p.start() print('father pid of main process:',os.getppid()) #the IDE process running result: 1234father pid of main process: 11524run pid : 12320run father pid: 13292sub_run father pid: 12320 2.进程间通信2.1、queue队列 12345678910111213141516171819202122#Author :ywqfrom multiprocessing import Queue,Processimport timequeue=Queue()def sub_run(arg): while arg.qsize() &gt;0: print('get',arg.get()) time.sleep(0.5)def run(): for i in range(97,102): queue.put(chr(i)) p=Process(target=sub_run,args=(queue,)) p.start() p.join()if __name__=='__main__': run() running result： 12345get aget bget cget dget e 2.2、pipe管道传输，语法类似socket 12345678910111213141516171819#Author :ywqfrom multiprocessing import Process, Pipelocal,peer = Pipe() #Pipe实例化时会创建两个对象，分别是管道的两端。相当于在主进程内实例化了两个 #传呼机，生成子进程的时候通过参数把其中一个传呼机给了子进程，两个进程间通过传呼机来通信def send(pipe): pipe.send(['hello']) pipe.close()def recv(): p = Process(target=send, args=(peer,)) p.start() print(local.recv()) # prints "[42, None, 'hello']" p.join()if __name__ == '__main__': recv() print(local) print(peer) running result： 123['hello']&lt;multiprocessing.connection.PipeConnection object at 0x00000000028524A8&gt;&lt;multiprocessing.connection.PipeConnection object at 0x0000000000D734A8&gt; 2.3、manager以上的queue、pipe方式都是在两个进程间传输通信，而不是像线程里一样两个进程都可以直接操作内存中的数据，因为进程的memory是彼此独立的。manager模块内部实现了通过中间代理实时克隆数据的方式，使多个进程都可以同时操作同一份数据。A manager object support data types include：list, dict, value, Lock, RLock, Semaphore, BoundedSemaphore, Condition, Event, Queue… 1234567891011121314151617181920212223#Author :ywqfrom multiprocessing import Process, Managerdef f(d, l): d['a'] = 'yes' l.append(0) print(l)if __name__ == '__main__': manager=Manager() dict = manager.dict() list = manager.list() p_list = [] for i in range(5): p = Process(target=f, args=(dict, list)) p.start() p_list.append(p) for sub_process in p_list: sub_process.join() print(list) print(dict) running result： 1234567[0][0, 0][0, 0, 0][0, 0, 0, 0][0, 0, 0, 0, 0][0, 0, 0, 0, 0]&#123;'a': 'yes'&#125; 3.进程池生成子进程的过程是直接将父进程的资源克隆一份，因此生成进程相对线程而言需要开销较大，为防止多进程导致资源用尽，所以需要线程池来约束同一时间最多可以运行的子进程数量。 1234567891011121314151617181920212223242526#Author :ywqfrom multiprocessing import process,Poolimport os,timedef callback(num): print('\033[0;33mThe callback func pid:%s \033[0m' %os.getpid()) #回调函数由主进程执行def sub_run(num): print('Num:',num) print('\033[0;32mSub process is running:%s \033[0m'%os.getpid()) time.sleep(1)def run(): pool=Pool(processes=2) #processes=2，实例化一个Pool对象，pool大小限制为2 print('\033[0;31mMain process running: %s \033[0m '%os.getpid()) for i in range(5): #pool.apply(func=sub_run,args=(i,)) #子线程串行实施 #pool.apply_async(func=sub_run,args=(i,)) #子线程并行实施 pool.apply_async(func=sub_run,args=(i,),callback=callback) #执行完前方func的函数后，再来执行callback回调函数 #回调函数由主进程来执行 pool.close() #必须先close()后join，语法要求 pool.join() #不join的话主进程不会等待进程池里的子进程if __name__ == '__main__': #windows平台下实例化pool多进程必须先判定__name__=='__main__' run() running result： 12345678910Main process running: 19436 Num: 0Sub process is running:18988 Num: 1Sub process is running:18928 Num: 2Sub process is running:18988 The callback func pid:19436 The callback func pid:19436 The callback func pid:19436 四、测试4.1计算场景，单进程VS多线程多线程：123456789101112131415161718192021#Author :ywqimport threading,timenum=0def calc(): global num a=num num+=1 num+=astart_time=time.time()for i in range(100000): t=threading.Thread(target=calc) t.start() #calc()#print(num) #数值太大，不打印了print(time.time()-start_time,' sencods') result： 116.822962284088135 seconds 单进程：注释掉clac()，取消注释tresult：10.5590319633483887 seconds 多线程计算完成用时16S，单进程用时0.55S，差距很大。。。 4.2、I/O场景，单进程VS多线程 单进程：123456789101112131415#Author :ywqimport threading,time,osdef write(): f=open('test.txt','a') f.write('just for test \n') f.close()start_time=time.time()for i in range(30000): #t=threading.Thread(target=write) #t.start() write() print(time.time()-start_time,'seconds') result： 112.502715110778809 seconds 多线程： 123456789101112131415#Author :ywqimport threading,time,osdef write(): f=open('test.txt','a') f.write('just for test \n') f.close()start_time=time.time()for i in range(30000): t=threading.Thread(target=write) t.start() #write()print(time.time()-start_time,'seconds') result： 19.011515378952026 seconds 多进程：起三个进程，每个进程写1W行，共3W行1234567891011121314151617181920212223242526#Author :ywqimport threading,time,osfrom multiprocessing import Processdef write(): print('Pid:',os.getpid()) for i in range(10000): #t=threading.Thread(target=write) #t.start() f=open('test.txt','a') f.write('just for test \n') f.close()if __name__=='__main__': start_time=time.time() p_list=[] for i in range(3): p=Process(target=write,) p.start() p_list.append(p) for process in p_list: process.join() print(time.time()-start_time,'seconds') result： 1234Pid: 20516Pid: 21564Pid: 216005.744328498840332 seconds 总结：python（C库python）因为早期设定的GIL，在CPU的计算指令到达一定的数量以后立即切换运算另一个线程，频繁快速的上下文切换以此来实现了伪多线程，实际上只有一个线程任务在被计算。python的多线程不适合计算密集型场景，但适用于I/O密集型场景，因此引入多进程。但多进程无法像多线程一样便利地互相共享数据，只能借助于队列或管道来通信，或第三方代理来实现数据共享，另外多进程的开销相比多线程要大很多。根据场景选择。]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 写一个简单的FTPv2]]></title>
    <url>%2F2017%2F11%2F10%2FPython%20%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84FTPv2.html</url>
    <content type="text"><![CDATA[**相较上一篇v1的改进版，当前实现了如下功能：1.上传2.下载3.查看server端保存的文件列表4.上传、下载显示进度条5.server端将client信息保存为json格式存入本地，实时加载和写入 client code：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#Author :ywqimport socket,os,json,sysimport threadingimport timemsg_dic=&#123;&#125;size_dic=&#123;&#125;clac_event=threading.Event()class Scok_Client(object): def __init__(self): self.client=socket.socket() def connect(self,IP,Port): self.client.connect((IP,Port)) def usage(self): print( ''' Usage:action + parameter Put Filename Get Filename Ls cd ../.. ''' ) def auth(self): user=input('Input username:') passwd=input('Input passwd:') msg_dic['Username']=user msg_dic['Password']=passwd def progressbar(self,a,b): percentage = a/b num = int(percentage*100) view = '\r %d%%[%-100s]%d%%'%(0,'#'*num,100) sys.stdout.write(view) sys.stdout.flush() def interactive(self): while True: self.auth() while True: try: cmd_str=input('Input cmd:') cmd=cmd_str.split()[0] print('Exec action:',cmd ) if hasattr(self,'cmd_%s' %cmd) : act=getattr(self,'cmd_%s' %cmd) act(cmd_str) except Exception as e: print('Error:',e) self.usage() def cmd_put(self,*args): cmd,filename=args[0].split() if os.path.isfile(filename): msg_dic['Action']=cmd msg_dic['Filename']=filename file_size=os.path.getsize(filename) msg_dic[filename+'_Filesize']=file_size print('\033[1;32mSend Info to Server:\033[0m \n',msg_dic) self.client.send(json.dumps(msg_dic).encode()) server_ack=self.client.recv(8192) print('Server ACK:',server_ack) f=open(filename,'rb') finished_size=0 start_time=time.time() clac_start_time=start_time for line in f: self.client.send(line) finished_size+=len(line) while time.time() - start_time &gt;0.5: self.progressbar(finished_size,file_size) start_time=time.time() #print percentage every 0.5 second print('\033[1;31m%s done,use time:%i seconds\033[0m' %(cmd,time.time()-clac_start_time)) f.close() def cmd_get(self,*args): cmd,filename=args[0].split() msg_dic['Action']=cmd msg_dic['Filename']=filename self.client.send(json.dumps(msg_dic).encode()) file_size=int(self.client.recv(8192).decode()) print('\033[1;32mGet File,size:\033[0m',file_size) self.client.send(b'Client ACK') f=open(filename,'wb') finished_size=0 while finished_size &lt; file_size: data=self.client.recv(8192) f.write(data) finished_size+=len(data) self.progressbar(finished_size,file_size) else: print('\033[1;32mReceive done\033[0m',os.stat(filename)) f.close() def cmd_ls(self,*args): cmd=args[0].split()[0] msg_dic['Action']=cmd self.client.send(json.dumps(msg_dic).encode()) file_list=self.client.recv(8192) print('\033[1;32mMy file list in server:\033[0m',file_list.decode())ftp_client=Scok_Client()ftp_client.connect('localhost',999)ftp_client.interactive() server code：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#Author :ywqimport socketserver,sys,os,jsonclient_dic=&#123;&#125;def dic_init(username,password): client_dic[username]=&#123;&#125; client_dic[username]['file_list']=[] client_dic[username]['Username']=username client_dic[username]['Password']=passworddic_init('ywq','qwe')with open('client_auth.json','w')as auth_conf: json.dump(client_dic,auth_conf) ''' save the client auth_info as json_format into local '''with open('client_auth.json','r+') as auth_conf: client_dic=json.load(auth_conf) #load client auth_infodef auth(recv_dic): client_user=recv_dic['Username'] client_passwd=recv_dic['Password'] if client_user == client_dic[client_user]['Username'] and client_passwd == client_dic[client_user]['Password']: pass else: exit('Auth failed')class MyHandler(socketserver.BaseRequestHandler): def handle(self): while True: try: recv_dic=json.loads(self.request.recv(8192).decode()) print('\033[1;31mReceive client dic\033[0m\n',recv_dic) auth(recv_dic) client_cmd=recv_dic['Action'] if hasattr(self,'client_%s' %client_cmd ): exec_cmd=getattr(self,'client_%s' %client_cmd) exec_cmd(recv_dic) except Exception as e: print('Error:',e) def client_put(self,*args): recv_dic=args[0] client_user=recv_dic['Username'] client_dic[client_user].update(recv_dic) filename=recv_dic['Filename'] filesize=int(recv_dic[filename+'_Filesize']) print(filesize) received_size=0 if os.path.isfile(filename): print('file is already exist') exit(2) else: f=open(filename,'wb') self.request.send(b'Server ACK') while received_size &lt; filesize: data=self.request.recv(8192) f.write(data) received_size+=len(data) else: print('\033[1;32m Recv done! \033[0m',os.stat(filename)) f.close() client_dic[client_user]['file_list'].append(filename) print(client_dic[client_user]['file_list']) with open('client_auth.json','r+') as auth_conf: json.dump(client_dic,auth_conf) def client_get(self,*args): recv_dic=args[0] filename=recv_dic['Filename'] file_size=os.stat(filename).st_size #or os.path.getsize() self.request.send(str(file_size).encode()) #send file size to client client_ack=self.request.recv(8192) f=open(filename,'rb') for line in f: self.request.send(line) print('\033[1;32m Send done ,total size:\033[0m',file_size) f.close() def client_ls(self,*args): recv_dic=args[0] client_user=recv_dic['Username'] user_file_list=client_dic[client_user]['file_list'] self.request.send(str(user_file_list).encode()) print('\033[1;32m Send file_list to user %s:\033[0m' %client_user)IP,Port='localhost',999server=socketserver.ThreadingTCPServer((IP,Port),MyHandler)print('\033[1;32mWaiting for connection:\033[0m')server.serve_forever() 运行效果： 运行完成后的json文件：]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python写一个简单的FTP]]></title>
    <url>%2F2017%2F11%2F07%2FPython%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84FTP.html</url>
    <content type="text"><![CDATA[## FTP v1.0版，仅实现文件上传，防止粘包，打印进度条，正在写v2版Server： 1234567891011121314151617181920212223242526272829import os,socketserverclass MyHandler(socketserver.BaseRequestHandler): def handle(self): while True: try: cmd,filename=self.request.recv(1024).decode().split() print('Recv:',cmd,filename) if os.path.isfile(filename): file_size=os.path.getsize(filename) print('file size:',file_size) self.request.send(str(file_size).encode()) #send file size to client client_ack=self.request.recv(1024) #wait for client ack print('Client ACK:',client_ack.decode()) f=open(filename,'rb') for line in f: self.request.send(line) print('server send success:',filename) f.close() except ValueError as e: print('error,please retry:',e) except ConnectionResetError as e: print('error:',e)if __name__ == "__main__": Host,Port='localhost',999 server=socketserver.TCPServer((Host,Port),MyHandler) server.serve_forever() Client： 12345678910111213141516171819202122232425262728293031323334import sysimport socket,osclient=socket.socket()client.connect(('localhost',999))while True: try: action=input('Input action and filename:') cmd,filename=action.split() if len(action) &lt;1:continue client.send(action.encode()) file_size=int(client.recv(1024).decode()) ack=client.send(b'Already!') received_size=0 f=open(filename,'wb') def progressbar(): percentage = received_size/file_size num = int(percentage*100) view = '\r %d%%[%-100s]%d%%'%(0,'#'*num,100) sys.stdout.write(view) sys.stdout.flush() while received_size &lt; file_size: data=client.recv(1024) received_size+=len(data) f.write(data) progressbar() else: print('Recv done') f.close() os.system('dir') except ValueError as e: print('error:',e)client.close() 运行效果：]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python练习题：类与对象]]></title>
    <url>%2F2017%2F10%2F26%2FPython%E7%BB%83%E4%B9%A0%E9%A2%98%EF%BC%9A%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1.html</url>
    <content type="text"><![CDATA[一、题目要求 创建 北京 和 上海 两个校区 创建 Linux \ Python \ Go 3个课程 创建 北京校区的Python 16期， Go开发第一期，和上海校区的Linux 10期 班级 管理员 创建了 北京校区的 学员 小晴 ，并将其 分配 在了 班级 python 16期 管理员 创建了 讲师 王二 , 并将其分配 给了 班级 python 16期 讲师 王二 创建 了一条 python 16期的 上课纪录 Day6 讲师 王二 为Day6这节课 所有的学员 批了作业 ，小晴得了A, 李雷得了C-, 赵帅得了B 学员李磊 查看了自己所报的所有课程 学员 李磊 在 查看了 自己在 py16期 的 成绩列表 ，然后退出了 学员小晴 给 讲师 王二好评 二、code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#Author :ywqCourse_list=[]class School(object): def __init__(self,school_name): self.school_name=school_name self.students_list=[] self.teachers_list=[] global Course_list def hire(self,obj): self.teachers_list.append(obj.name) print('[System Notice]:现在雇佣新员工 %s,welcome!' %obj.name) def enroll(self,obj): self.students_list.append(obj.name) print('[System Notice]:学员%s 信息注册成功,ID:%s' %(obj.name,obj.id))class Grade(School): def __init__(self,school_name,grade_code,grade_course): super(Grade,self).__init__(school_name) self.code=grade_code self.course=grade_course self.members=[] Course_list.append(self.course) print('[System Notice]:Now, school \'%s\' grade \'%s\' create course \'%s\' ' %(self.school_name,self.code,self.course)) def course_info(self): print(''' Syllabus(课程大纲) of %s is : day 1: day 2: day 3: day 4: day 5: ''' %self.course)Python = Grade('BJ',16,'Python')Go = Grade('BJ',1,'Go')Linux = Grade('SH',10,'Linux')class School_member(object): def __init__(self,name,age,sex,role): self.name=name self.age=age self.sex=sex self.role=role self.course_list=[] print('-----------My name is %s ,and I am a %s ------------ ' %(self.name,self.role))stu_num_id=00class Students(School_member): def __init__(self,name,age,sex,role,course): super(Students,self).__init__(name,age,sex,role) global stu_num_id stu_num_id+=1 stu_id=course.school_name +'S'+str(course.code)+str(stu_num_id).zfill(2) #保证stu_num_id有两位数， #只有一位数时前面填充0，只能对str类型操作 self.id=stu_id self.mark_list=&#123;&#125; def study(self,course): print('------------I come here to learn %s ,ID %s ------------' %(course.course,self.id)) def pay(self,course): print('------------I pay 2000$ for %s ------------' % course.course) self.course_list.append(course.course) def Praise(self,obj): print('------------%s Praise %s:Wonderful! ------------' %(self.name,obj.name)) def mark_check(self): for i in self.mark_list.items(): print(i) def out(self): print('[System Notice]:So sadly,the mark of %s was so bad,he(she) opt out at last ' %self.name)tea_num_id=00class Teachers(School_member): def __init__(self,name,age,sex,role,course): super(Teachers,self).__init__(name,age,sex,role) global tea_num_id tea_num_id+=1 #保证有两位数，个位数时往前面填充0 Tea_id=course.school_name +'T'+str(course.code)+str(tea_num_id).zfill(2) self.id=Tea_id def teach(self,course): print('------------I come here to teach guys %s,id %s ------------' %(course.course,self.id)) def record_mark(self,Date,course,obj,level): print('It is %s \'s mark in the Day %s of the course %s : %s' %(obj.name,Date,course.course,level) ) obj.mark_list['Day'+Date]=levela=Students('小晴',21,'W','student',Python)Python.enroll(a)a.study(Python)a.pay(Python)b=Students('李雷',22,'M','student',Python)Python.enroll(b)b.study(Python)b.pay(Python)c=Students('赵帅',23,'M','student',Python)Python.enroll(c)c.study(Python)c.pay(Python)t = Teachers('王二',30,'M','teacher',Python)Python.hire(t)t.teach(Python)t.record_mark('6',Python,a,'A')t.record_mark('6',Python,c,'B')t.record_mark('1',Python,b,'C-')t.record_mark('2',Python,b,'C-')t.record_mark('3',Python,b,'C-')t.record_mark('4',Python,b,'C-')t.record_mark('5',Python,b,'C-')t.record_mark('6',Python,b,'C-')print(b.course_list) #查看已报名课程b.mark_check() #评分查询b.out()a.Praise(t) 三、运行结果： 1234567891011121314151617181920212223242526272829303132333435[System Notice]:Now, school 'BJ' grade '16' create course 'Python' [System Notice]:Now, school 'BJ' grade '1' create course 'Go' [System Notice]:Now, school 'SH' grade '10' create course 'Linux' -----------My name is 小晴 ,and I am a student ------------ [System Notice]:学员小晴 信息注册成功,ID:BJS1601------------I come here to learn Python ,ID BJS1601 ------------------------I pay 2000$ for Python -----------------------My name is 李雷 ,and I am a student ------------ [System Notice]:学员李雷 信息注册成功,ID:BJS1602------------I come here to learn Python ,ID BJS1602 ------------------------I pay 2000$ for Python -----------------------My name is 赵帅 ,and I am a student ------------ [System Notice]:学员赵帅 信息注册成功,ID:BJS1603------------I come here to learn Python ,ID BJS1603 ------------------------I pay 2000$ for Python -----------------------My name is 王二 ,and I am a teacher ------------ [System Notice]:现在雇佣新员工 王二,welcome!------------I come here to teach guys Python,id BJT1601 ------------It is 小晴 's mark in the Day 6 of the course Python : AIt is 赵帅 's mark in the Day 6 of the course Python : BIt is 李雷 's mark in the Day 1 of the course Python : C-It is 李雷 's mark in the Day 2 of the course Python : C-It is 李雷 's mark in the Day 3 of the course Python : C-It is 李雷 's mark in the Day 4 of the course Python : C-It is 李雷 's mark in the Day 5 of the course Python : C-It is 李雷 's mark in the Day 6 of the course Python : C-['Python']('Day5', 'C-')('Day3', 'C-')('Day4', 'C-')('Day1', 'C-')('Day2', 'C-')('Day6', 'C-')[System Notice]:So sadly,the mark of 李雷 was so bad,he(she) opt out at last ------------小晴 Praise 王二:Wonderful! ------------]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python手记（三）：类的继承]]></title>
    <url>%2F2017%2F10%2F22%2FPython%E6%89%8B%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF.html</url>
    <content type="text"><![CDATA[Python手记（三）：类的继承12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#Author :ywqclass human: def __init__(self,name,age): self.name=name self.age=age def think(self): print('Human %s is thinking about something ' %self.name) def talk(self): print('Human %s is talking with others' % self.name)class animals: def eat(self): print('animal %s need to eat for survival' % self.name)class man(human,animals): def __init__(self,name,age,money): self.name=name self.age=age self.money=money def fighting(self): print('Man should fighting for a family')class woman(human,animals): def born(self): print('woman can reproduction') def talk(self,obj): super(woman,self).talk() print('woman %s is talking with %s' % (self.name,obj.name))a=man("Alex",22,100)a.fighting()a.think()a.talk()a.eat()b=woman('Alice',20)b.born()b.eat()b.talk(a)#######&gt;&gt;&gt;Man should fighting for a family&gt;&gt;&gt;Human Alex is thinking about something &gt;&gt;&gt;Human Alex is talking with others&gt;&gt;&gt;animal Alex need to eat for survival&gt;&gt;&gt;woman can reproduction&gt;&gt;&gt;animal Alice need to eat for survival&gt;&gt;&gt;Human Alice is talking with others&gt;&gt;&gt;woman Alice is talking with Alex 结论：1.在父类（human）和子类（man）都定义了构造函数时（其他函数类同），以子类本身优先级最高，所以需要按照子类中构造函数的的标准提供3个变量（name.age,money）; 2.若子类的函数与父类同名，则优先只运行子类，若希望与父类一起运行，则可在子类函数内使用super(SUBCLASS_NAME,self),FUNC_NAME()方式调用父类函数，两者一起运行； 3.如果子类没有定义构造函数，可以直接继承调用父类的构造函数，方法等； 4.如果继承的父类有多个时，按从左至右的顺序装配，优先级左边最高； 5.基类必须要有构造函数才能运行，且类的实例的变量数要与构造函数参数一一对应； 6.子类可以是从某个父类中继承一个构造函数，这个继承的父类一定要写在第一位，后面的继承的父类可以无需拥有构造函数]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 手记（二）：装饰器]]></title>
    <url>%2F2017%2F09%2F10%2FPython%20%E6%89%8B%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E8%A3%85%E9%A5%B0%E5%99%A8.html</url>
    <content type="text"><![CDATA[一、装饰器decorator的作用与组成：**作用：在不修改被修饰函数主体及调用方式的前提下，为被修饰函数提供新的功能组成：内嵌函数+高阶函数+闭包=》装饰器 内嵌（部）函数：定义：在一个函数体内创建另外一个函数，这种函数就叫内嵌函数(基于python支持静态嵌套域)例如： 12345678910111213141516171819&gt;&gt;&gt;&gt;&gt;&gt; num=1&gt;&gt;&gt; def level1():... num=2... print(num,'level1')... def level2():... num=3... print(num,'level2')... def level3():... num=4... print(num,'level3')... level3()... level2()...&gt;&gt;&gt; level1()2 level13 level24 level3&gt;&gt;&gt; 注释掉level3中的num赋值，则num继承上一级level2作用域中的值：1234567891011121314151617&gt;&gt;&gt; num=1&gt;&gt;&gt; def level1():... num=2... print(num,'level1')... def level2():... num=3... print(num,'level2')... def level3():... #num=4... print(num,'level3')... level3()... level2()...&gt;&gt;&gt; level1()2 level13 level23 level3 注释掉level2、level3中的num赋值，则level2、level3中num均继承level1作用域中的值：123456789101112131415161718&gt;&gt;&gt; num=1&gt;&gt;&gt; def level1():... num=2... print(num,'level1')... def level2():... #num=3... print(num,'level2')... def level3():... #num=4... print(num,'level3')... level3()... level2()...&gt;&gt;&gt; level1()2 level12 level22 level3&gt;&gt;&gt; 总结：变量在函数中的作用域只能正向继承，但层级越近优先级越高，本地优先级最高，全局优先级最低。 高阶函数：满足下列条件之一的函数，即可被称之为高阶函数：1.某一函数当做参数传入另一个函数中2.函数的返回值包含n个函数,n&gt;0例如：123456789101112&gt;&gt;&gt; def foo(func): ####stage2... res=func() #####stage3... return res ####stage4...&gt;&gt;&gt; def calc():... print([i for i in range(3)])... return 'run end'...&gt;&gt;&gt;&gt;&gt;&gt; foo(calc) ####stage1[0, 1, 2]'run end' 执行过程：stage1.将calc函数主体这个对象，calc作为实参传递给foo，替代形参funcstage2:运行foo函数stage3：将calc()赋值给变量res，并开始执行calc()函数，stage4：将clac()的函数返回值传递给res 好像并没有什么X用，绕了一圈不如直接调用calc()，那么接着往下 闭包python中的闭包从表现形式上定义（解释）为：如果在一个内部函数里，对在外部作用域（但不是在全局作用域）的变量进行了引用，并且函数返回值也是内部函数本身，那么该内部函数就被认为是闭包(closure)。12345678def outside _func(para1): def inside_func(para2): return para1*para2 return inside_func # 将inside_func作为outside_func的返回值a = outside_func(12) # 此时 para1=12，执行outside_func(12)print(a) #此时的inside_func并未赋予参数被执行，因此上方执行的结果只能是一段inside_func的内存指针 print(a(21)) # para2=21，等价于print(outside_func(12)(21)) 此时的inside_func就叫做闭包，闭包的作用：1.隐藏内部函数，只暴露外部函数2.如果内部函数使用了外部函数的变量，则此外部变量不会随着外部函数一起销毁，而会一直保存在内存中 二、内嵌函数+高阶函数+闭包=》装饰器假设要给函数增加一个记录运行时间的功能，要求不改变函数主体和调用方式： ** 12345678910111213141516171819&gt;&gt;&gt; import time&gt;&gt;&gt; def timmer(func): #stage3... def warpper(): ... start_time=time.time() #stage4... func() #stage5，link to the function test1（）... stop_time=time.time() #stage6... print('the func run time is %s second' %(stop_time-start_time))... return warpper #stage7...&gt;&gt;&gt;&gt;&gt;&gt; def test1():... time.sleep(1)... print('in the test1')...&gt;&gt;&gt; test1 = timmer(test1) #stage1&gt;&gt;&gt; test1() #stage2in the test1the func run time is 1.0000572204589844 second&gt;&gt;&gt; 过程解析：stage1：将test1函数作为实参传递给timmer函数，运算得出的结果的内存指针再重新指向给test1stage2：开始执行test1（读取test1内存指针并开始运算）stage3：执行timmer(func)，即timmer(test1)stage4：执行warpper，记录当前时间赋值给start_timestage5：执行func()，即test1()函数stage6：test1()执行完成后记录当前时间，赋值给stop_timestage7：warpper的执行结果return给timmer，执行全部完成。 ps：stage1中的test1=timmer(test1)看起来比够简洁，在python中有语法糖这么一个存在，@符号表示，test1=timmer(test)等价于在定义test1的上方@timmer ** 函数在执行过程中可能会带有参数，那么带参数的函数如何配合装饰器中使用呢？改良版： ** 12345678910111213141516171819&gt;&gt;&gt; import time&gt;&gt;&gt; def timmer(func):... def warpper(*args,**kwargs): #可接受任意类型参数... start_time=time.time()... func(*args,**kwargs)... stop_time=time.time()... print('the func run time is %s second' %(stop_time-start_time))... return warpper...&gt;&gt;&gt;@timmer #等价于test1=timmer(test1)&gt;&gt;&gt; def test1(n):... time.sleep(n)... print('in the test1') ... return 'run end'... &gt;&gt;&gt; test1(2) #等价于timmer(test1)(2),test1作为timmer的参数，2作为warpper的参数in the test1the func run time is 2.0001142024993896 second&gt;&gt;&gt; 函数可能会手动定义返回值，例如上方定义了return ‘run end’,但是经过装饰器包装了之后现在并没有返回值，如何保留函数原本的返回值呢？进阶版： 12345678910111213141516171819&gt;&gt;&gt; import time&gt;&gt;&gt; def timmer(func):... def warpper(*args,**kwargs):... start_time=time.time()... func(*args,**kwargs)... stop_time=time.time()... print('the func run time is %s second' %(stop_time-start_time))... return func(*args,**kwargs)... return warpper...&gt;&gt;&gt; @timmer... def test1(n):... time.sleep(n)... return 'run end'...&gt;&gt;&gt; test1(1)the func run time is 1.0000572204589844 second'run end'&gt;&gt;&gt; 在闭包函数内最后一行添加一个return func()即可返回func()函数原本的return返回值 若此时要求实现较为复杂的逻辑，装饰器本身也要携带不同的参数以实现多元化的功能。例如在上方例子基础上，要求增加在不同的板块内计时的倍率不同的功能。高级版： 12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; import time&gt;&gt;&gt; def timmer_rate(rate_type):... print('Current rate is %s rate' % rate_type )... def timmer(func):... def warpper(*args,**kwargs):... start_time=time.time()... func(*args,**kwargs)... stop_time=time.time()... run_time=stop_time-start_time... if rate_type == 'normal':... print('the func run time is %s second' % run_time)... elif rate_type == 'double':... print('the func run time is %s second' %(run_time*2))... return func(*args,**kwargs)... return warpper... return timmer...&gt;&gt;&gt;&gt;&gt;&gt; @timmer_rate('normal')... def test1(n):... time.sleep(n)... return 'test1 run end'...Current rate is normal rate&gt;&gt;&gt; @timmer_rate('double')... def test2(n):... time.sleep(n)... return 'test2 run end'...Current rate is double rate&gt;&gt;&gt; test1(4)the func run time is 4.0002288818359375 second'test1 run end'&gt;&gt;&gt; test2(2)the func run time is 4.0002288818359375 second'test2 run end'&gt;&gt;&gt; 相比上方版本，只是在原本两层的装饰器外再添加了一层，可以用来接收装饰器本身的参数，在装饰器内层加以判断，以实现差异化的功能，最终在test1内执行4s和test2内执行2s，计时结果时间接近一致。]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 手记（一）：生成器与迭代器]]></title>
    <url>%2F2017%2F09%2F05%2FPython%20%E6%89%8B%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%94%9F%E6%88%90%E5%99%A8%E4%B8%8E%E8%BF%AD%E4%BB%A3%E5%99%A8.html</url>
    <content type="text"><![CDATA[Python 手记（一）：生成器与迭代器一、列表生成式 在引入生成器和迭代器之前，首先还得提起列表生成式。List Comprehensions，是Python内置的非常简洁强大的用作创建list列表的生成式。 假设：list1=[1,2,3,4,5,6,7,8,9,10]，要求将列表内每个元素的值乘以2，怎么做呢？ Way1：新建一个list 12345678910&gt;&gt;&gt;&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt;&gt;&gt;&gt; list2=[]&gt;&gt;&gt; for i in list1:... list2.append(i*2)...&gt;&gt;&gt; print(list2)[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]&gt;&gt;&gt; way2：原list修改值 1234567&gt;&gt;&gt;&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt; for i,e in enumerate(list1): //取下标... list1[i]*=2...&gt;&gt;&gt; print(list1)[2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 有没有更简洁的办法呢？ of course，List Comprehensions！ 12345&gt;&gt;&gt;&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt; list2=[i*2 for i in list1] //列表生成式：将循环主体、条件、范围写在一起并用[]修饰生成list&gt;&gt;&gt;print(list2)[2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 还可以加上条件判断，例如取出list1中能被2整除的元素，乘以2123456&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt;&gt;&gt;&gt; list2=[i*2 for i in list1 if i % 2 ==0]&gt;&gt;&gt;&gt;&gt;&gt; print(list2)[4, 8, 12, 16, 20] 双重循环加条件判断： 12345&gt;&gt;&gt; a=[1,2,3]&gt;&gt;&gt; b=['a','b','c']&gt;&gt;&gt; c=[str(i)+j for i in a for j in b if i &gt;= 2]&gt;&gt;&gt; print(c)['2a', '2b', '2c', '3a', '3b', '3c'] 不仅是list，for循环甚至可以用多个变量来迭代dict的key和value： 123456789&gt;&gt;&gt; # _*_conding:utf-8 _*_&gt;&gt;&gt; dict=&#123;'sz':'深圳','gz':'广州','fs':'佛山'&#125;&gt;&gt;&gt;&gt;&gt;&gt; for k,v in dict.items():... print(k,'=',v)...sz = 深圳gz = 广州fs = 佛山 那么同样，可以用生成式来将dict生成list 12345&gt;&gt;&gt; # _*_conding:utf-8 _*_&gt;&gt;&gt; dict=&#123;'sz':'深圳','gz':'广州','fs':'佛山'&#125;&gt;&gt;&gt;list=[k+'='+v for k,v in dict.items()]&gt;&gt;&gt;print(list)['sz=深圳', 'gz=广州', 'fs=佛山'] 二、生成器Generator 上方描述了列表生成式是一次性将所有元素都装入生成的list中，读取list时也是一次性全部读取，list是python最常用的数据类型之一，当数据规模非常大时（百万千万级别），如果继续将如此大规模的数据一次性读取进内存中再进行下一步操作，会带来很长的等待时间和资源消耗，而通常我们所要读取的是列表中的某一部分元素而不是全部，由此引入了generator的概念。 Generator的工作方式是：惰性运算，只保存算法不保存值，边循环边计算，仅当需要进入下一次循环时，才计算出list中的下一个元素，而不必创建完整的list. Generator在python里的创建方式中最简单的方法为：将生成式的[]修改为()即可生成一个generator，它的调用方法为内置的next()方法(或.next)，每next()一次则运算一次,且顺序执行指针方向不可逆，list遍历结束后，会报错StopIteration： 1234567891011121314151617181920&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt; a=(i*2 for i in list1 if i % 2 ==0)&gt;&gt;&gt; a&lt;generator object &lt;genexpr&gt; at 0x0000000000D380F8&gt;&gt;&gt;&gt; print(a)&lt;generator object &lt;genexpr&gt; at 0x0000000000D380F8&gt;&gt;&gt;&gt; print(a.__next__())4&gt;&gt;&gt; print(next(a))8&gt;&gt;&gt; print(next(a))12&gt;&gt;&gt; print(next(a))16&gt;&gt;&gt; print(next(a))20&gt;&gt;&gt; print(next(a))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 通常调用生成器，也不会用next()方法一次次地来调用，而是使用for循环,因为生成器也是可迭代对象：123456789101112&gt;&gt;&gt; list1=[1,2,3,4,5,6,7,8,9,10]&gt;&gt;&gt; a=(i*2 for i in list1 if i % 2 ==0)&gt;&gt;&gt; a&lt;generator object &lt;genexpr&gt; at 0x0000000000D380F8&gt;&gt;&gt;&gt; for i in a : print(i)...48121620&gt;&gt;&gt; 当generator的使用环境比较复杂，无法通过修改for循环的生成式来创建生成器满足需求时，可以引入函数。 创建一个普通的计算二进制计算循环函数： 1234567891011121314151617181920&gt;&gt;&gt; def binary(maxnum):... n=1... count=0... while count&lt;=maxnum:... print(n)... n*=2... count+=1...&gt;&gt;&gt; binary(8)1248163264128256&gt;&gt;&gt;&gt;&gt;&gt; 不难发现，这个函数计算的结果是有规则性的，可以根据前一个结果推算出后一个结果，那么这个函数距离generator已经非常近了，仅需一个yield即可。将print(n)改成yield(n)即可实现将函数变为生成器： 12345678910111213141516171819202122&gt;&gt;&gt; def binary(maxnum):... n=1... count=0... while count&lt;=maxnum:... yield n... n*=2... count+=1... return ‘func run end’&gt;&gt;&gt; res=binary(8)&gt;&gt;&gt;&gt;&gt;&gt; for i in res:... print(i)...1248163264128256 以上可以看出，函数变成了可以使用for循环的可迭代对象，但是执行结果没有return返回值，为什么呢？因为加入了yield关键字，把function变为了generator。 个人理解，yield在生成器中的起到断点的作用，每次调用next()时，遇到yield语句就中断，下一次再调用next()时再从上次中断的yield处开始下一轮循环。（类似游戏中的存档~） 再来看一个yield最简单的用法： 123456789101112131415161718192021222324&gt;&gt;&gt;&gt;&gt;&gt; def test():... print('first')... yield 'a'... print('second')... yield 'b'... print('third')... yield 'c'...&gt;&gt;&gt; res=test()&gt;&gt;&gt; next(res)first'a'&gt;&gt;&gt; next(res)second'b'&gt;&gt;&gt; next(res)third'c'&gt;&gt;&gt; next(res)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 可以看出，每执行一次next()函数调用，遇到yield后，函数的执行便卡住返回，下一次调用时，再从上一次yield处开始。直至生成器内的元素遍历完成后第四次调用next()返回StopIteration的报错。 再对比一下上方的的for循环的函数式生成器，可以发现，使用for循环时，生成式元素遍历完毕后，不会报错，而使用next()函数调用时，元素遍历完毕后再次next()会返回报错StopIteration，这即是生成式的一种返回值，类似函数里的return code。可以根据返回值来定义一些后续的操作，再次拿上方的二进制计算生成器来举例： 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; def binary(maxnum):... n=1... count=0... while count&lt;=maxnum:... yield n... n*=2... count+=1... return 'func run end '...&gt;&gt;&gt; res=binary(8)&gt;&gt;&gt;&gt;&gt;&gt; while True:... try:... x=next(res)... print('current value:',x)... except StopIteration as signal:... print('Finally:',singal.value)... break...current value: 1current value: 2current value: 4current value: 8current value: 16current value: 32current value: 64current value: 128current value: 256Finally: func run end&gt;&gt;&gt; 解析：try + except 配合使用，将接收到StopIteration作为执行完成的信号，singal.value的值为函数生成式return的值。 三、迭代器 在python中，可以被for循环遍历的对象，被称为可迭代对象，Iterable。其中包括常见的数据类型，包括list、tuple、dict、set、str等，另一种则是generator和带yield的generator function。可以使用isinstance()判断一个对象是否是Iterable对象。 1234567891011&gt;&gt;&gt;&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance([],Iterable)True&gt;&gt;&gt; isinstance((),Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;,Iterable)True&gt;&gt;&gt; isinstance('abcd',Iterable)True&gt;&gt;&gt; 但是，Iterable对象并不代表着它就是迭代器Iterator。只有能调用next()函数不断返回下一个值的对象才能被称为迭代器，因此，生成器绝对是迭代器，但迭代器不一定是生成器，这个概念要理清。可以使用isinstance()判断一个对象是否是Iterator。 1234567891011121314151617181920&gt;&gt;&gt; from collections import Iterator //从collection库导入Iterator模块&gt;&gt;&gt; isinstance(&#123;&#125;,Iterator)False&gt;&gt;&gt; isinstance([],Iterator)False&gt;&gt;&gt; isinstance((),Iterator)False&gt;&gt;&gt; isinstance('abcd',Iterator)False&gt;&gt;&gt;&gt;&gt;&gt; def test():... n=0... while True:... n+=1... yield n... return 'calculate done'...&gt;&gt;&gt; isinstance(test(),Iterator)True&gt;&gt;&gt; 为什么list、tuple、dict、str都是Iterable可迭代对象，却不是Iterator迭代器呢？因为在python中，关于Iterator对象的定义是：可以被next()函数调用并不断返回下一个数据，直到没有数据时StopIteration的一个类似有序序列的数据流，无法提前得知这个数据流的长度，只能不断通过next()函数去惰性计算下一个元素，而不是一次性全部计算出。 有没有办法实现将list、tuple、dict、str转换成成Iterator呢？ 12345678910&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance(iter(&#123;&#125;),Iterator)True&gt;&gt;&gt; isinstance(iter([]),Iterator)True&gt;&gt;&gt; isinstance(iter(()),Iterator)True&gt;&gt;&gt; isinstance(iter('abcd'),Iterator)True&gt;&gt;&gt; 使用Iter()函数，可以将Iterable对象转换成Iterator。 附记：python中的for循环本质上就是通过不断调用next()函数实现的，例如,以下for循环和下方调用迭代器方式的循环，两者本质上是完全一致的： 1234list=[1,2,3]for i in list: print(i) 12345678from collections import Iteratorlist=iter([1,2,3])while True: try: i=next(list) print(i) except StopIteration: break Over，过几天再总结归纳一下python装饰器~]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis集群：基于twemproxy的实现]]></title>
    <url>%2F2017%2F06%2F28%2FRedis%E9%9B%86%E7%BE%A4%EF%BC%9A%E5%9F%BA%E4%BA%8Etwemproxy%E7%9A%84%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[Redis集群：基于twemproxy的实现一、前言 1.Why 集群？ 当前各家互联网公司为了提高站点响应速度，使用缓存工具将热点数据保存在内存中，避免直接从后端数据库读取查询，降低后端压力。其中常见的cache工具有memcache和redis，相较于memcache，redis有着许多优势，这里不再赘述。 在大型站点应用中，热点数据几十上百G也很普遍，而无论是物理机、云主机（虚拟机），内存资源往往是有限的，虽然目前硬件成本降低，几十G几十核的主机也很常见，但是对于redis这种工作进程为单线程模式的工具来说，一台主机只运行一个实例就有些浪费，且出现单点故障时影响范围很大。同时，redis管理大内存时不如管理相对较小的内存高效，据第三方测试，redis单进程内存分配达到20G后性能急剧下降，因此普遍使用的方式为单进程分配8G内存，单主机开启多个redis instance. 2.How 集群？ ⑴.Redis官方集群方案 Redis Cluster（服务端sharding） Redis cluster是一种在服务端sharding（分片）的技术，在redis3.0版本开始正式提供。 Redis cluster的服务端sharding引入了slot（槽）的概念，一共分为16384（2^14）个slot，集群中的每个node节点负责分摊这些slot，每个进入redis的键值对，根据key进行hash运算，除以16384取模，匹配相应的slot，再分配进相应的node中的实例中。在redis cluster方案中，数据储存的粒度由原来的instance再次精细为slot。 Redis cluster提供一种叫做cluster bus（集群总线）的功能特性，采用特殊的二进制协议，通信及响应速度极快。它提供节点故障检测、故障转移、新节点识别等节点管理功能，该功能的进程间通信端口号为服务端口号值+10000，例如redis对外提供服务的端口号为6555，则cluster bus的端口号则为16555。 值得注意的是：redis cluster是官方在3.0以后的版本才正式推出的，虽然目前社区内比较活跃，但是成熟的生产案例还不多，需要时间考验。 ⑵.redis sharding 集群（客户端sharding） Redis 3.0服务端sharding推出之前，采用的较普遍的client端集群方式，工作逻辑为：将键值对中的key使用hash算法散列，特定的key映射至特定的redis 实例上，然后由client端主动向该node set/get数据，server端则工作在被动模式。 目前java redis的客户端驱动jedis已经支持redis sharding的功能。 但是Redis sharding面临的问题：扩容。在client端sharding模式下，数据控制完全由client端掌控，redis集群各node间彼此独立，因此在扩容（或缩容）时，数据无法直接从原node转向新node，这时会造成缓存无法命中的问题，查询/写入请求会透过缓存层直接访问后端DB，给后端带来较大的压力，为了避免这个问题，官方给出了比较讨巧的解决方案：presharding。 ⑶.代理中间件sharding 上面分别介绍了多Redis服务器集群的两种方式，它们是基于客户端sharding的Redis Sharding和基于服务端sharding的Redis Cluster。 客户端sharding技术 其优势在于服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强。 其不足之处在于：由于sharding处理放到客户端，规模进步扩大时给运维带来挑战。服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化。 服务端sharding的Redis Cluster其优势在于服务端Redis集群拓扑结构变化时，客户端不需要感知，客户端像使用单Redis服务器一样使用Redis集群，运维管理也比较方便。 不过Redis Cluster正式版推出时间不长，系统稳定性、性能等都需要时间检验，尤其在大规模使用场合。 能不能结合二者优势？即能使服务端各实例彼此独立，支持线性可伸缩，同时sharding又能集中处理，方便统一管理？本篇主要介绍的Redis代理中间件twemproxy就是这样一种利用中间件做sharding的技术。 twemproxy处于客户端和服务器的中间，将客户端发来的请求，进行一定的处理后(如sharding)，再转发给后端真正的Redis服务器。也就是说，客户端不直接访问Redis服务器，而是通过twemproxy代理中间件间接访问。twemproxy中间件的内部处理是无状态的，它本身可以很轻松地做LB/HA集群，这样可避免单点压力或故障。twemproxy又叫nutcracker，起源于twitter系统中redis/memcached集群开发实践，运行效果良好，后代码奉献给开源社区。其轻量高效，采用C语言开发。 GitHub link：(https://github.com/twitter/twemproxy) twemproxy后端不仅支持redis，同时也支持memcached，这是twitter系统具体环境造成的。由于使用了中间件，twemproxy可以通过共享与后端系统的连接，降低客户端直接连接后端服务器的连接数量。同时，它也提供sharding功能，支持后端服务器集群水平扩展。统一运维管理也带来了方便。当然，也是由于使用了中间件代理，相比客户端直连服务器方式，性能上会有所损耗，Twitter官方实测结果大约降低了20%左右，不过性能的部分损耗可以依靠增加集群节点数量来弥补。 So，开始实验吧~ 二、环境 虚拟机：redis server：192.168.0.168：6666 6667 6668 #三个实例 4G ramtwemproxy-server1：192.168.0.68twemproxy-server2：192.168.0.69 #vip：192.168.0.66 Centos 6.8，关闭iptables、selinux 三、准备工具包1.安装配置redis提前安装gcc，配置好epel源可以yum 直接安装 123456789101112131415161718[root@redis redis]# cd /usr/local/packages/[root@redis redis]# tar -xf redis-3.2.10.tar.gz [root@redis redis]# cd redis-3.2.10[root@redis redis]# make &amp;&amp; make install [root@redis redis]#mkdir /etc/redis[root@redis redis]#mkdir -pv /usr/local/redis/bin[root@redis redis]#cp redis.conf /etc/redis/[root@redis redis]#cp src/&#123;redis-cli,redis-server,redis-benchmark&#125; /usr/local/redis/bin[root@redis redis]#cd /etc/redis/[root@redis redis]#cp redis.conf redis.conf.bak[root@redis redis]#mv redis.conf redis_6666.conf##vim redis_6666.conf,修改工作端口号，pid路径，maxmemory=1G，注意，bind ip一定要修改为当前网卡IP，不能使用127.0.0.1 ##[root@redis redis]#cp redis_6666.conf redis_6667.conf[root@redis redis]#cp redis_6666.conf redis_6668.conf[root@redis redis]# /usr/local/redis/bin/redis-server /etc/redis/redis_6666.conf [root@redis redis]# /usr/local/redis/bin/redis-server /etc/redis/redis_6667.conf [root@redis redis]# /usr/local/redis/bin/redis-server /etc/redis/redis_6668.conf 三个进程全部已启动1234[root@redis redis]# ss -anlp | grep redisLISTEN 0 128 127.0.0.1:6666 *:* users:(("redis-server",13316,4))LISTEN 0 128 127.0.0.1:6667 *:* users:(("redis-server",13339,4))LISTEN 0 128 127.0.0.1:6668 *:* users:(("redis-server",13344,4))** 用自带redis-cli登入正常123[root@redis redis]# /usr/local/redis/bin/redis-cli -p 6666[root@redis redis]# /usr/local/redis/bin/redis-cli -p 6667[root@redis redis]# /usr/local/redis/bin/redis-cli -p 6668 2.安装配置twemproxy编译安装autoconf 1234[root@twemproxy-server ~]# wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz[root@twemproxy-server ~]# tar -zvxf autoconf-2.69.tar.gz[root@twemproxy-server ~]# cd autoconf-2.69[root@twemproxy-server autoconf-2.69]# ./configure &amp;&amp; make &amp;&amp; make install 编译安装automake 1234[root@twemproxy-server ~]# wget http://ftp.gnu.org/gnu/automake/automake-1.15.tar.gz[root@twemproxy-server ~]# tar -zvxf automake-1.15.tar.gz[root@twemproxy-server ~]# cd automake-1.15[root@twemproxy-server automake-1.15]# ./configure &amp;&amp; make &amp;&amp; make install 编译安装libtool 1234[root@twemproxy-server ~]# wget https://ftp.gnu.org/gnu/libtool/libtool-2.4.6.tar.gz[root@twemproxy-server ~]# tar -zvxf libtool-2.4.6.tar.gz[root@twemproxy-server ~]# cd libtool-2.4.6[root@twemproxy-server libtool-2.4.6]# ./configure &amp;&amp; make &amp;&amp; make install 编译安装配置twemproxy 1234567891011121314151617181920212223[root@twemproxy-server ~]# cd /usr/local/packages[root@twemproxy-server ~]# tar -xf twemproxy-0.4.1.tar.gz [root@twemproxy-server ~]# cd cd twemproxy-0.4.1[root@twemproxy-server-2 twemproxy-0.4.1]# aclocal[root@twemproxy-server-2 twemproxy-0.4.1]# autoreconf -f -i -Wall,no-obsolete //执行autoreconf 生成 configure文件等[root@twemproxy-server-2 twemproxy-0.4.1]# ./configure --prefix=/usr/local/twemproxy/[root@twemproxy-server-2 twemproxy-0.4.1]# make &amp;&amp; make install[root@twemproxy-server-2 twemproxy-0.4.1]cp -r conf/ /usr/local/twemproxy/[root@twemproxy-server ~]cd /usr/local/twemproxy/conf[root@twemproxy-server ~]vim nutcracker.ymlalpha: //随意命名 listen: 0.0.0.0:22121 #结合keepalived时必须配置为0.0.0.0 hash: fnv1a_64 distribution: ketama auto_eject_hosts: true redis: true server_retry_timeout: 2000 server_failure_limit: 1 servers: - 192.168.0.168:6666:1 - 192.168.0.168:6667:1 - 192.168.0.168:6668:1 //分片成3部分，格式ip：port:weight 启动查看twemproxy： 123456[root@twemproxy-server ~] /usr/local/twemproxy/sbin/nutcracker -c /usr/local/twemproxy/conf/nutcracker.yml &amp;[root@twemproxy-server ~]ss -tnlp | grep twemproxy[root@haproxy-b conf]# ss -tnlpState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 192.168.0.69:22121 *:* users:(("nutcracker",2581,5))LISTEN 0 128 *:22222 *:* users:(("nutcracker",2581,3)) 在redis端使用redis-cli登陆twemproxy并设置key-value成功： 3.keepalive配置twemproxy-server2配置同server1，这里不贴了。epel源中有keepalived包，直接安装：yum -y install keepalived 主配置： 1234567891011121314151617181920212223242526272829303132333435[root@haproxy-m conf]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 #生产环境设置真实mail server smtp_connect_timeout 10 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state MASTER interface eth1 virtual_router_id 51 priority 101 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.0.66 &#125; track_interface &#123; eth1 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" //通知及响应脚本&#125; 备配置： 1234567891011121314151617181920212223242526272829303132333435[root@haproxy-b twemproxy]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from keepalived@localhost smtp_server 127.0.0.1 smtp_connect_timeout 10 router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth2 virtual_router_id 51 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.0.66 &#125; track_interface &#123; eth2 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; notify.sh：[root@ twemproxy-server]# cat /etc/keepalived/notify.sh 123456789101112131415161718192021222324252627282930#!/bin/bashvip=192.168.0.66reciever='root@localhost' notify() &#123; mailsubject="`hostname`turn to $1,$vip switched" mailbody="`date`,vrrp status changed:`hostname` changed to be $1" echo "$mailbody" | mail -s "$mailsubject" $reciever &#125;case "$1" in backup) notify backup exit 0 ;; master) notify master exit 0 ;; fault) notify fault service keepalived restart exit 0 ;; *) echo "usage:please input $0 with &#123;master|backup|fault&#125;" exit 1 ;;esac 启动keepalived并查看vip位置，vip在master机上： ​4.操作测试通过vip访问，set/get数据成功： 将twemproxy主的网卡shutdown，查看备机，VIP已转移至备机： 客户端访问redis数据正常： 至此，高可用twemproxy代理redis集群部署完成。 扩展思路：1.redis节点配置为主从模式，故障检测，主备切换2.redis主备节点配置为读写分离模式，热点数据通常读写比例为5:1以上，可配置为一主（写）拖N从（读） 参考文档：]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shotcut]]></title>
    <url>%2F2000%2F01%2F01%2F20000101-shotcut.html</url>
    <content type="text"><![CDATA[截图]]></content>
  </entry>
</search>
